# CI Pipeline for FollowWeb
# 
# FAIL-FAST STRATEGY:
# - All pytest runs use -x --maxfail=1 to stop on first test failure
# - Matrix strategy uses fail-fast: true to cancel all jobs on first failure
# - All jobs depend on quality-check AND test-quick passing
# - Each parallel job checks for failures in sibling jobs before proceeding
# - If ANY job fails, ALL remaining jobs detect and cancel themselves immediately
# - Saves CI time and resources by not running unnecessary jobs
# 
# This workflow uses only FREE and OPEN-SOURCE tools with permissive licenses:
# - GitHub Actions (MIT License)
# - Python ecosystem tools (Python Software Foundation License)
# - ruff (MIT License) - Code formatting and linting
# - mypy (MIT License) - Type checking
# - pytest (MIT License) - Testing framework
# - bandit (Apache 2.0) - Security linting
# - pip-audit (Apache 2.0) - Vulnerability scanning
# - NetworkX (BSD License) - Graph analysis
# - All other dependencies use MIT, BSD, or Apache 2.0 licenses
# 
# NO PROPRIETARY SERVICES OR TOKENS REQUIRED
# 
# This workflow has been hardened based on real-world issues encountered:
# 
# 1. Type Checking Issues:
#    - Install missing type stubs (types-python-dateutil, types-PyYAML, types-decorator)
#    - Clean build artifacts before mypy to avoid conflicts
#    - Configure mypy to ignore external library issues via pyproject.toml
#
# 2. Code Quality Issues:
#    - Use ruff for both linting and formatting (replaces black + flake8)
#    - Configure .ipynb exclusion in pyproject.toml rather than CLI flags
#    - Separate formatting check from linting for clearer error reporting
#
# 3. Security Scanning Issues:
#    - Use pip-audit instead of safety (no registration required)
#    - Only fail on medium/high severity bandit issues (low severity acceptable)
#
# 4. Package Building Issues:
#    - Clean build artifacts before building to avoid stale files
#    - Use explicit package list in pyproject.toml to avoid including unwanted packages
#    - Verify package integrity with twine check
#
# 5. Dependency Management:
#    - Install type stubs explicitly to avoid mypy failures
#    - Use separate requirements files for different purposes
#    - Handle conditional dependencies properly (nx-parallel for Python 3.11+)
#
# Common Troubleshooting:
# - If mypy fails with external library errors: Update type stub installations
# - If package build includes wrong files: Check pyproject.toml packages configuration
# - If ruff fails on notebooks: Ensure extend-exclude in pyproject.toml includes "*.ipynb"
# - If security scan fails: Check if new vulnerabilities need dependency updates
# - If tests fail randomly: May be parallel execution issues, check pytest-xdist configuration

name: CI

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run tests weekly on Saturdays at 2 AM UTC to catch dependency issues
    # Changed from Sunday to avoid collision with nightly workflow
    - cron: '0 2 * * 6'
  workflow_dispatch:
    # Allow manual triggering of the workflow

# Cancel in-progress runs for the same workflow on the same branch
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

# Global defaults for all jobs
defaults:
  run:
    shell: bash

jobs:
  # Prebuild environment - runs in parallel with smoke test
  # Creates cached environment for all subsequent jobs
  prebuild:
    name: Prebuild Environment
    uses: ./.github/workflows/codespaces-prebuild.yml
  
  # Fast quality checks - runs in parallel with smoke test to catch issues early
  # This job runs ruff linting and formatting checks before any tests
  # Automatically applies safe fixes and only fails if issues remain
  quality-check:
    name: Quick Quality Check
    runs-on: ubuntu-latest
    timeout-minutes: 5
    defaults:
      run:
        working-directory: FollowWeb
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        ref: ${{ github.head_ref }}
    
    - name: Set up Python 3.12
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'
        cache: 'pip'
    
    - name: Install ruff only (minimal dependencies)
      run: |
        python -m pip install --upgrade pip
        python -m pip install ruff
    
    - name: Apply ruff auto-fixes
      id: ruff-fix
      run: |
        echo "Attempting to auto-fix issues with ruff..."
        
        # Apply safe auto-fixes for linting issues
        ruff check FollowWeb_Visualizor tests --fix --unsafe-fixes || true
        
        # Apply formatting fixes
        ruff format FollowWeb_Visualizor tests || true
        
        # Check if any files were modified
        if git diff --quiet; then
          echo "No auto-fixes applied"
          echo "fixes_applied=false" >> $GITHUB_OUTPUT
        else
          echo "Auto-fixes applied successfully"
          echo "fixes_applied=true" >> $GITHUB_OUTPUT
          
          # Show what was fixed
          echo "### Auto-fixes Applied" >> $GITHUB_STEP_SUMMARY
          echo "\`\`\`diff" >> $GITHUB_STEP_SUMMARY
          git diff >> $GITHUB_STEP_SUMMARY
          echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
        fi
      continue-on-error: true
    
    - name: Commit and push auto-fixes
      if: steps.ruff-fix.outputs.fixes_applied == 'true' && github.event_name == 'pull_request'
      run: |
        git config --local user.email "github-actions[bot]@users.noreply.github.com"
        git config --local user.name "github-actions[bot]"
        git add -A
        git commit -m "style: apply ruff auto-fixes [skip ci]"
        git push
      continue-on-error: true
    
    - name: Run ruff format check (after auto-fix)
      run: |
        echo "Checking if any formatting issues remain after auto-fix..."
        if ! ruff format --check FollowWeb_Visualizor tests --diff; then
          echo "::error::Formatting issues remain after auto-fix. Manual intervention required."
          echo "### [FAIL] Formatting Issues Remain" >> $GITHUB_STEP_SUMMARY
          echo "Ruff was unable to automatically fix all formatting issues." >> $GITHUB_STEP_SUMMARY
          echo "Please run locally: \`ruff format FollowWeb_Visualizor tests\`" >> $GITHUB_STEP_SUMMARY
          exit 1
        else
          echo "[PASS] All formatting issues resolved"
        fi
      continue-on-error: false
    
    - name: Run ruff linting check (after auto-fix)
      run: |
        echo "Checking if any linting issues remain after auto-fix..."
        if ! ruff check FollowWeb_Visualizor tests --output-format=github; then
          echo "::error::Linting issues remain after auto-fix. Manual intervention required."
          echo "### [FAIL] Linting Issues Remain" >> $GITHUB_STEP_SUMMARY
          echo "Ruff was unable to automatically fix all linting issues." >> $GITHUB_STEP_SUMMARY
          echo "Please run locally: \`ruff check FollowWeb_Visualizor tests --fix\`" >> $GITHUB_STEP_SUMMARY
          exit 1
        else
          echo "[PASS] All linting issues resolved"
        fi
      continue-on-error: false
    
    - name: Quality check summary
      if: always()
      run: |
        if [[ "${{ steps.ruff-fix.outputs.fixes_applied }}" == "true" ]]; then
          echo "### [PASS] Quality Check Passed (with auto-fixes)" >> $GITHUB_STEP_SUMMARY
          echo "Ruff automatically fixed code quality issues." >> $GITHUB_STEP_SUMMARY
        else
          echo "### [PASS] Quality Check Passed" >> $GITHUB_STEP_SUMMARY
          echo "No code quality issues found." >> $GITHUB_STEP_SUMMARY
        fi
  
  # Quick smoke test on Ubuntu with latest Python - runs in parallel with performance and benchmarks
  # Runs unit tests only (excludes integration tests) for faster feedback
  # Full test suite including integration tests runs in the 'test' job after this passes
  # If this job fails, all subsequent jobs will be cancelled
  test-quick:
    name: Smoke Test (Python 3.12 on Ubuntu)
    runs-on: ubuntu-latest
    needs: [prebuild, quality-check]  # Wait for prebuild and quality check
    timeout-minutes: 15
    defaults:
      run:
        working-directory: FollowWeb
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
    
    - name: Configure Git
      run: |
        git config --global init.defaultBranch main
        git config --global advice.detachedHead false
    
    - name: Set up Python 3.12
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'
        cache: 'pip'
        cache-dependency-path: 'FollowWeb/requirements-ci.txt'
    
    - name: Use prebuild environment
      run: |
        if [ -d ".venv" ]; then
          echo "✅ Using prebuild cache"
          source .venv/bin/activate
          echo "VIRTUAL_ENV=$VIRTUAL_ENV" >> $GITHUB_ENV
          echo "$VIRTUAL_ENV/bin" >> $GITHUB_PATH
          python -c "import FollowWeb_Visualizor; print('Package ready from cache')"
        else
          echo "❌ Cache miss - installing fresh"
          python -m pip install --upgrade pip setuptools wheel
          python -m pip install -r requirements-ci.txt -e .
        fi
      shell: bash
    
    - name: Run quick smoke tests
      run: |
        set -e
        # Get CPU count for CI environment (use all cores, no safety margin needed in CI)
        cpu_count=$(python -c "import os; print(os.cpu_count() or 1)")
        echo "Running tests with $cpu_count workers (all available cores)"
        # Run unit tests only (fast, isolated tests with maximum parallelization)
        # No coverage requirement for smoke test - just validate code works
        # FAIL FAST: Stop immediately on first failure to save CI time
        python -m pytest -m unit -x --maxfail=1 --tb=short -v -n $cpu_count
      shell: bash
      env:
        MPLBACKEND: Agg
        PYTHONPATH: ${{ github.workspace }}/FollowWeb

  # Full test matrix - tests oldest (3.9) and newest (3.12) Python on all platforms
  # Ubuntu 3.12 already tested in smoke test, so excluded here
  test:
    name: Test Python ${{ matrix.python-version }} on ${{ matrix.os }}
    runs-on: ${{ matrix.os }}
    needs: [prebuild, quality-check, test-quick]  # Wait for prebuild, quality check and smoke test
    timeout-minutes: 30
    defaults:
      run:
        working-directory: FollowWeb
    
    strategy:
      fail-fast: true  # Stop all matrix jobs immediately on first failure
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python-version: ['3.9', '3.12']
    
    continue-on-error: false
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        # Fetch full history for better coverage analysis
        fetch-depth: 0
    
    - name: Configure Git
      run: |
        git config --global init.defaultBranch main
        git config --global advice.detachedHead false
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}
        cache: 'pip'
        cache-dependency-path: 'FollowWeb/requirements-ci.txt'
    
    - name: Restore prebuild cache
      if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.12'
      uses: actions/cache/restore@v4
      with:
        path: |
          ~/.cache/pip
          FollowWeb/.venv
          FollowWeb/tests/test_data
          .pytest_cache
          .mypy_cache
          .ruff_cache
        key: prebuild-${{ runner.os }}-py3.12-${{ hashFiles('FollowWeb/requirements-ci.txt', 'FollowWeb/pyproject.toml') }}
        restore-keys: |
          prebuild-${{ runner.os }}-py3.12-
    
    - name: Display Python version and environment info
      run: |
        python --version
        python -c "import sys; print(f'Python executable: {sys.executable}')"
        python -c "import platform; print(f'Platform: {platform.platform()}')"
        pip --version
    
    - name: Use prebuild environment
      run: |
        if [ -d ".venv" ]; then
          echo "✅ Using prebuild cache"
          source .venv/bin/activate
          echo "VIRTUAL_ENV=$VIRTUAL_ENV" >> $GITHUB_ENV
          echo "$VIRTUAL_ENV/bin" >> $GITHUB_PATH
          python -c "import FollowWeb_Visualizor; print('Package ready from cache')"
        else
          echo "❌ Cache miss - installing fresh"
          python -m pip install --upgrade pip setuptools wheel
          python -m pip install -r requirements-ci.txt
          python -m pip install -e .
        fi
      shell: bash
    
    - name: Verify installation
      run: |
        # Package must import successfully
        python -c "import FollowWeb_Visualizor"
        python ../.github/scripts/ci_helpers.py success "Package imported successfully"
        
        # Test parallel processing availability
        python -c "from FollowWeb_Visualizor.utils.parallel import is_nx_parallel_available; is_nx_parallel_available()"
        python ../.github/scripts/ci_helpers.py success "nx-parallel available"
        
        # Test all submodules import
        python -c "from FollowWeb_Visualizor import analysis, core, data, output, utils, visualization"
        python ../.github/scripts/ci_helpers.py success "All submodules imported successfully"
        
        # Test basic functionality
        python -c "from FollowWeb_Visualizor.core.config import ConfigurationManager; from FollowWeb_Visualizor.data.loaders import DataLoader, InstagramLoader"
        python ../.github/scripts/ci_helpers.py success "Core functionality verified"
        
        # CLI entry point must work
        if followweb --help > /dev/null 2>&1; then
          python ../.github/scripts/ci_helpers.py success "CLI entry point working"
        else
          python ../.github/scripts/ci_helpers.py error "CLI entry point failed"
          followweb --help || true
          exit 1
        fi
      shell: bash
    
    - name: Run type checking
      run: |
        # Clean any previous build artifacts that might interfere with mypy
        if [ -d "build" ]; then rm -rf build; fi
        # Run mypy with proper configuration to ignore external library issues
        mypy FollowWeb_Visualizor
      continue-on-error: false
      shell: bash
    
    - name: Run tests with coverage
      run: |
        set -e  # Exit immediately if any command fails
        threshold=$(python ../.github/scripts/ci_helpers.py coverage-threshold-value)
        # Get CPU count for CI environment (use all cores, no safety margin needed in CI)
        cpu_count=$(python -c "import os; print(os.cpu_count() or 1)")
        echo "Running tests with $cpu_count workers (all available cores)"
        # Run fast tests (unit + integration) with coverage in parallel
        # Exclude slow, performance, and benchmark tests - those run in dedicated performance job
        # FAIL FAST: Stop immediately on first failure to save CI time
        python -m pytest -m "not (benchmark or slow or performance)" -x --maxfail=1 -n $cpu_count --cov=FollowWeb_Visualizor --cov-report=xml --cov-report=term --cov-report=html --cov-fail-under=$threshold || {
          exit_code=$?
          python ../.github/scripts/ci_helpers.py error "Test Execution Failed" --summary-only
          python ../.github/scripts/ci_helpers.py info "**Exit Code:** $exit_code" --summary-only
          exit $exit_code
        }
        
        # Generate test results summary with dynamic counts
        python ../.github/scripts/ci_helpers.py test-summary
      shell: bash
      continue-on-error: false
      env:
        MPLBACKEND: Agg
        PYTHONPATH: ${{ github.workspace }}/FollowWeb
    
    - name: Upload coverage reports
      if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
      uses: actions/upload-artifact@v4
      with:
        name: coverage-reports-${{ matrix.os }}-${{ matrix.python-version }}
        path: |
          FollowWeb/coverage.xml
          FollowWeb/htmlcov/
        retention-days: 7
        if-no-files-found: ignore
    
    - name: Generate coverage report summary
      if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
      run: |
        python ../.github/scripts/ci_helpers.py info "**Coverage Report Summary**" --summary-only
        python ../.github/scripts/ci_helpers.py info "\`\`\`" --summary-only
        python -m coverage report --show-missing >> $GITHUB_STEP_SUMMARY || python ../.github/scripts/ci_helpers.py error "Coverage report generation failed" --summary-only
        python ../.github/scripts/ci_helpers.py info "\`\`\`" --summary-only
        python ../.github/scripts/ci_helpers.py info "Full HTML coverage report available in artifacts." --summary-only

  security:
    name: Security Scan
    runs-on: ubuntu-latest
    needs: [prebuild, quality-check, test-quick]  # Wait for prebuild, quality check and smoke test
    timeout-minutes: 10
    defaults:
      run:
        working-directory: FollowWeb
    
    steps:
    - name: Check for failures in parallel jobs
      uses: actions/github-script@v7
      with:
        script: |
          const jobs = await github.rest.actions.listJobsForWorkflowRun({
            owner: context.repo.owner,
            repo: context.repo.repo,
            run_id: context.runId,
          });
          const failedJobs = jobs.data.jobs.filter(job => 
            job.conclusion === 'failure' && job.status === 'completed'
          );
          if (failedJobs.length > 0) {
            core.setFailed(`Cancelling due to failed jobs: ${failedJobs.map(j => j.name).join(', ')}`);
          }
    
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Configure Git
      run: |
        git config --global init.defaultBranch main
        git config --global advice.detachedHead false
    
    - name: Set up Python 3.12
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'
        cache: 'pip'
        cache-dependency-path: 'requirements-ci.txt'
    
    - name: Restore prebuild cache
      uses: actions/cache/restore@v4
      with:
        path: |
          ~/.cache/pip
          FollowWeb/.venv
          FollowWeb/tests/test_data
          .pytest_cache
          .mypy_cache
          .ruff_cache
        key: prebuild-${{ runner.os }}-py3.12-${{ hashFiles('FollowWeb/requirements-ci.txt', 'FollowWeb/pyproject.toml') }}
        restore-keys: |
          prebuild-${{ runner.os }}-py3.12-
    
    - name: Use prebuild environment
      run: |
        if [ -d ".venv" ]; then
          echo "✅ Using prebuild cache"
          source .venv/bin/activate
          echo "VIRTUAL_ENV=$VIRTUAL_ENV" >> $GITHUB_ENV
          echo "$VIRTUAL_ENV/bin" >> $GITHUB_PATH
          python -c "import FollowWeb_Visualizor; print('Package ready from cache')"
        else
          echo "❌ Cache miss - installing fresh"
          python -m pip install --upgrade pip
          python -m pip install bandit[toml] pip-audit -r requirements-ci.txt
        fi
    
    - name: Run Bandit security linter
      run: |
        # Generate JSON report for artifacts (include all issues for reporting)
        bandit -r FollowWeb_Visualizor -f json -o bandit-report.json || true
        
        python ../.github/scripts/ci_helpers.py info "**Bandit Security Scan Results**" --summary-only
        python ../.github/scripts/ci_helpers.py info "\`\`\`" --summary-only
        
        # First, run bandit for all severity levels to show complete results
        python ../.github/scripts/ci_helpers.py info "**All Security Issues Found:**" --print-only
        bandit -r FollowWeb_Visualizor 2>&1 | tee -a $GITHUB_STEP_SUMMARY || true
        
        python ../.github/scripts/ci_helpers.py info "" --summary-only
        python ../.github/scripts/ci_helpers.py info "**Critical Check (Medium/High Severity Only):**" --summary-only
        
        # Now check for medium/high severity issues - FAIL only on these
        set +e  # Don't exit on error temporarily
        bandit_critical_output=$(bandit -r FollowWeb_Visualizor --severity-level medium 2>&1)
        bandit_critical_exit_code=$?
        
        echo "$bandit_critical_output" >> $GITHUB_STEP_SUMMARY
        
        # Allow bandit to fail as long as failure is non-critical (low severity only)
        # Exit codes: 0 = no issues, 1 = issues found, >1 = scanner error
        if [ $bandit_critical_exit_code -eq 0 ] || echo "$bandit_critical_output" | grep -q "No issues identified"; then
          python ../.github/scripts/ci_helpers.py success "No medium or high severity security issues found (low severity issues are acceptable)" --print-only
          python ../.github/scripts/ci_helpers.py success "No critical security issues found" --summary-only
        elif [ $bandit_critical_exit_code -eq 1 ]; then
          # Exit code 1 with medium+ severity means critical issues found
          python ../.github/scripts/ci_helpers.py error "CRITICAL: Medium or high severity security issues detected"
          exit 1
        else
          # Exit code >1 means scanner error - this is a critical failure
          python ../.github/scripts/ci_helpers.py error "Bandit security scanner failed to run properly (exit code: $bandit_critical_exit_code)" --print-only
          python ../.github/scripts/ci_helpers.py error "Bandit security scanner failed to run" --summary-only
          exit 1
        fi
        
        python ../.github/scripts/ci_helpers.py info "\`\`\`" --summary-only
      continue-on-error: false
    
    - name: Run pip-audit for known vulnerabilities
      run: |
        # Generate JSON report for artifacts
        pip-audit --format=json --output=pip-audit-report.json
        # Run vulnerability scan - FAIL on any vulnerabilities
        python ../.github/scripts/ci_helpers.py info "**Security Scan Results**" --summary-only
        python ../.github/scripts/ci_helpers.py info "\`\`\`" --summary-only
        pip-audit --desc >> $GITHUB_STEP_SUMMARY
        python ../.github/scripts/ci_helpers.py info "\`\`\`" --summary-only
      continue-on-error: false
    
    - name: Upload security reports
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: security-reports-ubuntu-latest
        path: |
          FollowWeb/bandit-report.json
          FollowWeb/pip-audit-report.json
        retention-days: 7
        if-no-files-found: ignore

  format-check:
    name: Code Quality & Format Check
    runs-on: ubuntu-latest
    needs: [prebuild, quality-check, test-quick]  # Wait for prebuild, quality check and smoke test
    timeout-minutes: 10
    defaults:
      run:
        working-directory: FollowWeb
    
    steps:
    - name: Check for failures in parallel jobs
      uses: actions/github-script@v7
      with:
        script: |
          const jobs = await github.rest.actions.listJobsForWorkflowRun({
            owner: context.repo.owner,
            repo: context.repo.repo,
            run_id: context.runId,
          });
          const failedJobs = jobs.data.jobs.filter(job => 
            job.conclusion === 'failure' && job.status === 'completed'
          );
          if (failedJobs.length > 0) {
            core.setFailed(`Cancelling due to failed jobs: ${failedJobs.map(j => j.name).join(', ')}`);
          }
    
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python 3.12
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'
        cache: 'pip'
        cache-dependency-path: 'requirements-ci.txt'
    
    - name: Restore prebuild cache
      uses: actions/cache/restore@v4
      with:
        path: |
          ~/.cache/pip
          FollowWeb/.venv
          FollowWeb/tests/test_data
          .pytest_cache
          .mypy_cache
          .ruff_cache
        key: prebuild-${{ runner.os }}-py3.12-${{ hashFiles('FollowWeb/requirements-ci.txt', 'FollowWeb/pyproject.toml') }}
        restore-keys: |
          prebuild-${{ runner.os }}-py3.12-
    
    - name: Use prebuild environment
      run: |
        if [ -d ".venv" ]; then
          echo "✅ Using prebuild cache"
          source .venv/bin/activate
          echo "VIRTUAL_ENV=$VIRTUAL_ENV" >> $GITHUB_ENV
          echo "$VIRTUAL_ENV/bin" >> $GITHUB_PATH
          python -c "import FollowWeb_Visualizor; print('Package ready from cache')"
        else
          echo "❌ Cache miss - installing fresh"
          python -m pip install --upgrade pip
          python -m pip install -r requirements-minimal.txt
        fi
    
    - name: Check code formatting with ruff
      run: |
        # Note: ruff format doesn't support --extend-exclude, use pyproject.toml config
        ruff format --check FollowWeb_Visualizor tests --diff
    
    - name: Run ruff linting
      run: |
        ruff check FollowWeb_Visualizor tests --output-format=github --extend-exclude="*.ipynb"
    
    - name: Check import sorting
      run: |
        ruff check --select I FollowWeb_Visualizor tests --extend-exclude="*.ipynb"

  build:
    name: Build & Package
    runs-on: ubuntu-latest
    needs: [prebuild, quality-check, test-quick]  # Wait for prebuild, quality check and smoke test
    timeout-minutes: 15
    defaults:
      run:
        working-directory: FollowWeb
    
    steps:
    - name: Check for failures in parallel jobs
      uses: actions/github-script@v7
      with:
        script: |
          const jobs = await github.rest.actions.listJobsForWorkflowRun({
            owner: context.repo.owner,
            repo: context.repo.repo,
            run_id: context.runId,
          });
          const failedJobs = jobs.data.jobs.filter(job => 
            job.conclusion === 'failure' && job.status === 'completed'
          );
          if (failedJobs.length > 0) {
            core.setFailed(`Cancelling due to failed jobs: ${failedJobs.map(j => j.name).join(', ')}`);
          }
    
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for proper versioning
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'
    
    - name: Install build dependencies
      run: |
        python -m pip install --upgrade pip setuptools wheel build twine check-manifest
    
    - name: Clean build artifacts
      run: |
        # Clean any previous build artifacts to avoid package configuration issues
        if [ -d "build" ]; then rm -rf build; fi
        if [ -d "*.egg-info" ]; then rm -rf *.egg-info; fi
      shell: bash
    
    - name: Verify package manifest
      run: |
        check-manifest --verbose
    
    - name: Build package
      run: |
        python -m build --sdist --wheel --outdir dist/
    
    - name: Validate package structure
      run: |
        # Ensure the wheel contains only the expected packages
        python -m zipfile -l dist/*.whl | grep -E "(FollowWeb_Visualizor/|followweb_visualizor.*dist-info/)" || python ../.github/scripts/ci_helpers.py info "Package structure validation" --print-only
        # Verify no unwanted packages are included
        if python -m zipfile -l dist/*.whl | grep -E "(tests/|analysis_tools/|build/)"; then
          python ../.github/scripts/ci_helpers.py error "Unwanted packages found in wheel"
          exit 1
        fi
    
    - name: Verify package integrity
      run: |
        twine check dist/*
        
    - name: Display package info
      run: |
        ls -la dist/
        python -m zipfile -l dist/*.whl || true
      shell: bash
    
    - name: Test package installation in clean environment
      run: |
        # Create a temporary virtual environment to test package installation
        python -m venv test_env
        source test_env/bin/activate || test_env\\Scripts\\activate
        pip install dist/*.whl
        python -c "import FollowWeb_Visualizor"
        python ../.github/scripts/ci_helpers.py success "Package installs and imports correctly" --print-only
        deactivate
      shell: bash
    
    - name: Upload build artifacts
      uses: actions/upload-artifact@v4
      with:
        name: python-package-distributions
        path: FollowWeb/dist/
        retention-days: 30

  # Performance tests - runs in parallel with smoke test and benchmarks
  # If this job fails, all subsequent jobs will be cancelled
  performance:
    name: Performance Tests
    runs-on: ubuntu-latest
    needs: [prebuild, quality-check]  # Wait for prebuild and quality check, run in parallel with smoke test/benchmarks
    timeout-minutes: 20
    defaults:
      run:
        working-directory: FollowWeb
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python 3.12
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'
        cache: 'pip'
        cache-dependency-path: 'requirements-ci.txt'
    
    - name: Restore prebuild cache
      uses: actions/cache/restore@v4
      with:
        path: |
          ~/.cache/pip
          FollowWeb/.venv
          FollowWeb/tests/test_data
          .pytest_cache
          .mypy_cache
          .ruff_cache
        key: prebuild-${{ runner.os }}-py3.12-${{ hashFiles('FollowWeb/requirements-ci.txt', 'FollowWeb/pyproject.toml') }}
        restore-keys: |
          prebuild-${{ runner.os }}-py3.12-
    
    - name: Use prebuild environment
      run: |
        if [ -d ".venv" ]; then
          echo "✅ Using prebuild cache"
          source .venv/bin/activate
          echo "VIRTUAL_ENV=$VIRTUAL_ENV" >> $GITHUB_ENV
          echo "$VIRTUAL_ENV/bin" >> $GITHUB_PATH
          python -c "import FollowWeb_Visualizor; print('Package ready from cache')"
        else
          echo "❌ Cache miss - installing fresh"
          python -m pip install --upgrade pip
          python -m pip install -r requirements-ci.txt -e .
        fi
    
    - name: Run performance tests
      run: |
        # Run slow and performance tests sequentially (no parallelization)
        # Exclude benchmark tests - those run in separate benchmarks job
        # FAIL FAST: Stop immediately on first failure to save CI time
        python -m pytest -m "slow or performance" -x --maxfail=1 -v --tb=short -n 0
        
        # Generate performance test summary
        python ../.github/scripts/ci_helpers.py success "Performance tests completed - no regressions detected" --print-only
        
        # Write to GitHub step summary
        python ../.github/scripts/ci_helpers.py info "**Performance Test Results**" --summary-only
        python ../.github/scripts/ci_helpers.py success "Performance tests completed successfully - no regressions detected" --summary-only
      env:
        MPLBACKEND: Agg
        PYTEST_PARALLEL_DISABLE: "1"
      continue-on-error: false

  # Benchmark tests - runs in parallel with smoke test and performance tests
  # If this job fails, all subsequent jobs will be cancelled
  benchmarks:
    name: Benchmark Tests
    runs-on: ubuntu-latest
    needs: [prebuild, quality-check]  # Wait for prebuild and quality check, run in parallel with smoke test/performance
    timeout-minutes: 20
    defaults:
      run:
        working-directory: FollowWeb
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python 3.12
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'
        cache: 'pip'
        cache-dependency-path: 'requirements-ci.txt'
    
    - name: Restore prebuild cache
      uses: actions/cache/restore@v4
      with:
        path: |
          ~/.cache/pip
          FollowWeb/.venv
          FollowWeb/tests/test_data
          .pytest_cache
          .mypy_cache
          .ruff_cache
        key: prebuild-${{ runner.os }}-py3.12-${{ hashFiles('FollowWeb/requirements-ci.txt', 'FollowWeb/pyproject.toml') }}
        restore-keys: |
          prebuild-${{ runner.os }}-py3.12-
    
    - name: Use prebuild environment
      run: |
        if [ -d ".venv" ]; then
          echo "✅ Using prebuild cache"
          source .venv/bin/activate
          echo "VIRTUAL_ENV=$VIRTUAL_ENV" >> $GITHUB_ENV
          echo "$VIRTUAL_ENV/bin" >> $GITHUB_PATH
          python -c "import FollowWeb_Visualizor; print('Package ready from cache')"
        else
          echo "❌ Cache miss - installing fresh"
          python -m pip install --upgrade pip
          python -m pip install -r requirements-ci.txt -e .
        fi
    
    - name: Run benchmark tests
      run: |
        # Create benchmarks directory
        mkdir -p .benchmarks
        # Run benchmark tests sequentially (no parallelization)
        # FAIL FAST: Stop immediately on first failure to save CI time
        python -m pytest -m "benchmark" -x --maxfail=1 -v --tb=short -n 0 --benchmark-save=ci_run --benchmark-save-data --benchmark-storage=.benchmarks
        
        # Generate benchmark test summary
        python ../.github/scripts/ci_helpers.py success "Benchmark tests completed - no regressions detected" --print-only
      env:
        MPLBACKEND: Agg
        PYTEST_PARALLEL_DISABLE: "1"
      continue-on-error: false
    
    - name: Generate benchmark report
      if: always()
      run: |
        # Generate structured benchmark summary
        python ../.github/scripts/ci_helpers.py benchmark-summary
        
        # Also show detailed pytest-benchmark output
        python ../.github/scripts/ci_helpers.py info "" --summary-only
        python ../.github/scripts/ci_helpers.py info "**Detailed Benchmark Output**" --summary-only
        python ../.github/scripts/ci_helpers.py info "\`\`\`" --summary-only
        
        # Run benchmark comparison if we have previous data
        if [ -f ".benchmarks/ci_run/0001_*.json" ]; then
          # Display current benchmark results with comparison
          python -m pytest -m benchmark --benchmark-only --benchmark-compare=ci_run --benchmark-storage=.benchmarks --tb=no -v 2>&1 | tee -a $GITHUB_STEP_SUMMARY || true
        else
          echo "No previous benchmark data found - this is the baseline run" | tee -a $GITHUB_STEP_SUMMARY
          # Just show the current results
          python -m pytest -m benchmark --benchmark-only --benchmark-storage=.benchmarks --tb=no -v 2>&1 | tee -a $GITHUB_STEP_SUMMARY || true
        fi
        
        python ../.github/scripts/ci_helpers.py info "\`\`\`" --summary-only
      shell: bash
      continue-on-error: true
    
    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: benchmark-results
        path: FollowWeb/.benchmarks/
        retention-days: 30
        if-no-files-found: ignore
  
  # Documentation pipeline - runs after quality check and smoke test pass
  documentation:
    name: Documentation
    needs: [prebuild, quality-check, test-quick]
    uses: ./.github/workflows/docs.yml
  
  # All checks must pass - no exceptions
  ci-success:
    name: CI Success
    runs-on: ubuntu-latest
    needs: [prebuild, quality-check, test-quick, test, security, format-check, build, performance, benchmarks, documentation]
    if: always()
    
    steps:
    - name: Check all jobs status
      run: |
        # All jobs must succeed - no failures allowed
        # This check runs BEFORE any installs to fail fast if any job failed
        failed_jobs=""
        skipped_jobs=""
        
        # Check each job result - fail on anything other than "success"
        if [[ "${{ needs.quality-check.result }}" != "success" ]]; then
          if [[ "${{ needs.quality-check.result }}" == "skipped" ]]; then
            skipped_jobs="$skipped_jobs QualityCheck(${{ needs.quality-check.result }})"
          else
            failed_jobs="$failed_jobs QualityCheck(${{ needs.quality-check.result }})"
          fi
        fi
        
        if [[ "${{ needs.test-quick.result }}" != "success" ]]; then
          if [[ "${{ needs.test-quick.result }}" == "skipped" ]]; then
            skipped_jobs="$skipped_jobs QuickTest(${{ needs.test-quick.result }})"
          else
            failed_jobs="$failed_jobs QuickTest(${{ needs.test-quick.result }})"
          fi
        fi
        
        if [[ "${{ needs.test.result }}" != "success" ]]; then
          if [[ "${{ needs.test.result }}" == "skipped" ]]; then
            skipped_jobs="$skipped_jobs FullTests(${{ needs.test.result }})"
          else
            failed_jobs="$failed_jobs FullTests(${{ needs.test.result }})"
          fi
        fi
        
        if [[ "${{ needs.security.result }}" != "success" ]]; then
          if [[ "${{ needs.security.result }}" == "skipped" ]]; then
            skipped_jobs="$skipped_jobs Security(${{ needs.security.result }})"
          else
            failed_jobs="$failed_jobs Security(${{ needs.security.result }})"
          fi
        fi
        
        if [[ "${{ needs.format-check.result }}" != "success" ]]; then
          if [[ "${{ needs.format-check.result }}" == "skipped" ]]; then
            skipped_jobs="$skipped_jobs CodeQuality(${{ needs.format-check.result }})"
          else
            failed_jobs="$failed_jobs CodeQuality(${{ needs.format-check.result }})"
          fi
        fi
        
        if [[ "${{ needs.build.result }}" != "success" ]]; then
          if [[ "${{ needs.build.result }}" == "skipped" ]]; then
            skipped_jobs="$skipped_jobs Build(${{ needs.build.result }})"
          else
            failed_jobs="$failed_jobs Build(${{ needs.build.result }})"
          fi
        fi
        
        if [[ "${{ needs.performance.result }}" != "success" ]]; then
          if [[ "${{ needs.performance.result }}" == "skipped" ]]; then
            skipped_jobs="$skipped_jobs Performance(${{ needs.performance.result }})"
          else
            failed_jobs="$failed_jobs Performance(${{ needs.performance.result }})"
          fi
        fi
        
        if [[ "${{ needs.benchmarks.result }}" != "success" ]]; then
          if [[ "${{ needs.benchmarks.result }}" == "skipped" ]]; then
            skipped_jobs="$skipped_jobs Benchmarks(${{ needs.benchmarks.result }})"
          else
            failed_jobs="$failed_jobs Benchmarks(${{ needs.benchmarks.result }})"
          fi
        fi
        
        if [[ "${{ needs.documentation.result }}" != "success" ]]; then
          if [[ "${{ needs.documentation.result }}" == "skipped" ]]; then
            skipped_jobs="$skipped_jobs Documentation(${{ needs.documentation.result }})"
          else
            failed_jobs="$failed_jobs Documentation(${{ needs.documentation.result }})"
          fi
        fi
        
        # Fail if any jobs failed
        if [[ -n "$failed_jobs" ]]; then
          echo "::error::CI PIPELINE FAILED - Failed Jobs:$failed_jobs"
          echo "**[FAIL] CI Pipeline Failed**" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Failed Jobs:**$failed_jobs" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Job Results:**" >> $GITHUB_STEP_SUMMARY
          echo "- Quality Check: ${{ needs.quality-check.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- Quick Test: ${{ needs.test-quick.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- Full Tests: ${{ needs.test.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- Security: ${{ needs.security.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- Code Quality: ${{ needs.format-check.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- Build: ${{ needs.build.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- Performance: ${{ needs.performance.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- Benchmarks: ${{ needs.benchmarks.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- Documentation: ${{ needs.documentation.result }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Action Required:** Review the failed job logs above for details." >> $GITHUB_STEP_SUMMARY
          if [[ -n "$skipped_jobs" ]]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "**Skipped Jobs:**$skipped_jobs" >> $GITHUB_STEP_SUMMARY
          fi
          exit 1
        fi
        
        # Warn about skipped jobs but don't fail
        if [[ -n "$skipped_jobs" ]]; then
          echo "::warning::Some jobs were skipped:$skipped_jobs"
          echo "**[WARN] CI PIPELINE WARNING**" >> $GITHUB_STEP_SUMMARY
          echo "**Skipped Jobs:**$skipped_jobs" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "::notice::ALL PREREQUISITE JOBS PASSED - PROCEEDING WITH SUCCESS VALIDATION"
    
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
    
    - name: Generate success summary
      run: |
        # Generate success summary
        echo "::notice::ALL CHECKS PASSED!"
        echo "**[PASS] CI Pipeline Success**" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "All quality checks passed:" >> $GITHUB_STEP_SUMMARY
        echo "- [PASS] Quality Check: ${{ needs.quality-check.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- [PASS] Quick Test: ${{ needs.test-quick.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- [PASS] Full Tests: ${{ needs.test.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- [PASS] Security: ${{ needs.security.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- [PASS] Code Quality: ${{ needs.format-check.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- [PASS] Build: ${{ needs.build.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- [PASS] Performance: ${{ needs.performance.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- [PASS] Benchmarks: ${{ needs.benchmarks.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- [PASS] Documentation: ${{ needs.documentation.result }}" >> $GITHUB_STEP_SUMMARY