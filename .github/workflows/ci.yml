# CI Pipeline for FollowWeb
# Optimized 4-phase pipeline: Quality Check → Parallel Init → Matrix Tests → Success Validation
# See .github/workflows/README.md for detailed documentation

name: CI

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run tests weekly on Saturdays at 2 AM UTC to catch dependency issues
    # Changed from Sunday to avoid collision with nightly workflow
    - cron: '0 2 * * 6'
  workflow_dispatch:
    # Allow manual triggering of the workflow

# Cancel in-progress runs for the same workflow on the same branch
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

# Global defaults for all jobs
defaults:
  run:
    shell: bash

jobs:
  # PHASE 1: Quality Check (runs first, independently)
  # Does NOT need any other jobs - only needs minimal tools (ruff, mypy)
  # Combines all code quality checks into one job
  quality-check:
    name: Quality Check
    runs-on: ubuntu-latest
    timeout-minutes: 10
    defaults:
      run:
        working-directory: FollowWeb
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python 3.12
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'
        cache: 'pip'
    
    - name: Install quality check tools (minimal dependencies)
      run: |
        python -m pip install --upgrade pip
        python -m pip install ruff mypy
        # Install type stubs for external libraries
        python -m pip install types-python-dateutil types-PyYAML types-decorator
    
    - name: Check code formatting
      run: |
        echo "Checking code formatting with ruff..."
        if ! ruff format --check FollowWeb_Visualizor tests --diff; then
          echo "::error::Code formatting issues found."
          {
            echo "### ❌ Formatting Check Failed"
            echo ""
            echo "Please run locally to fix:"
            echo "\`\`\`bash"
            echo "cd FollowWeb"
            echo "ruff format FollowWeb_Visualizor tests"
            echo "\`\`\`"
          } >> "$GITHUB_STEP_SUMMARY"
          exit 1
        fi
        echo "✓ Formatting check passed"
    
    - name: Run linting checks
      run: |
        echo "Running ruff linting checks..."
        if ! ruff check FollowWeb_Visualizor tests --output-format=github; then
          echo "::error::Linting issues found."
          {
            echo "### ❌ Linting Check Failed"
            echo ""
            echo "Please run locally to fix:"
            echo "\`\`\`bash"
            echo "cd FollowWeb"
            echo "ruff check FollowWeb_Visualizor tests --fix"
            echo "\`\`\`"
          } >> "$GITHUB_STEP_SUMMARY"
          exit 1
        fi
        echo "✓ Linting check passed"
    
    - name: Check import sorting
      run: |
        echo "Checking import sorting..."
        if ! ruff check --select I FollowWeb_Visualizor tests --extend-exclude="*.ipynb"; then
          echo "::error::Import sorting issues found."
          echo "### ❌ Import Sorting Check Failed" >> "$GITHUB_STEP_SUMMARY"
          exit 1
        fi
        echo "✓ Import sorting check passed"
    
    - name: Run type checking
      run: |
        echo "Running mypy type checking..."
        # Clean any previous build artifacts that might interfere with mypy
        if [ -d "build" ]; then rm -rf build; fi
        if ! mypy FollowWeb_Visualizor --show-error-codes; then
          echo "::error::Type checking failed."
          {
            echo "### ❌ Type Check Failed"
            echo ""
            echo "Please run locally to fix:"
            echo "\`\`\`bash"
            echo "cd FollowWeb"
            echo "mypy FollowWeb_Visualizor"
            echo "\`\`\`"
          } >> "$GITHUB_STEP_SUMMARY"
          exit 1
        fi
        echo "✓ Type checking passed"
    
    - name: Validate GitHub Actions workflows
      run: |
        echo "Validating workflow files..."
        # Install actionlint using bash script installer (more reliable)
        cd ..
        for i in 1 2 3; do
          if bash <(curl -sSL https://raw.githubusercontent.com/rhysd/actionlint/main/scripts/download-actionlint.bash) 2>/dev/null; then
            break
          fi
          echo "Download attempt $i failed, retrying..."
          sleep 2
        done
        
        if [ ! -f ./actionlint ]; then
          echo "::warning::Failed to download actionlint after 3 attempts, skipping workflow validation"
          {
            echo "### ⚠️ Workflow Validation Skipped"
            echo ""
            echo "actionlint download failed - this is a transient CI issue"
          } >> "$GITHUB_STEP_SUMMARY"
          exit 0
        fi
        
        # Validate all workflows using config (excludes large files that cause hangs)
        echo "Validating workflows (excluding large files via config)..."
        if ./actionlint -config-file .github/actionlint.yaml .github/workflows/*.yml; then
          echo "✓ Workflow validation passed"
        else
          echo "::error::Workflow validation failed"
          {
            echo "### ❌ Workflow Validation Failed"
            echo ""
            echo "Please fix workflow syntax errors."
          } >> "$GITHUB_STEP_SUMMARY"
          exit 1
        fi
      shell: bash
    
    - name: Quality check summary
      if: success()
      run: |
        {
          echo "### ✅ Quality Check Passed"
          echo ""
          echo "All code quality checks passed:"
          echo "- ✓ Code formatting (ruff format)"
          echo "- ✓ Linting (ruff check)"
          echo "- ✓ Import sorting (ruff check --select I)"
          echo "- ✓ Type checking (mypy)"
          echo "- ✓ Workflow validation (actionlint)"
        } >> "$GITHUB_STEP_SUMMARY"
  
  # ============================================================================
  # PHASE 2: PARALLEL INITIALIZATION & CHECKS (depends on quality-check)
  # ============================================================================
  # These jobs run in parallel but ALL depend on quality-check passing:
  # - environment-build: Warms pip cache for test jobs
  # - smoke-test: Quick unit tests
  # - security: Bandit + pip-audit security scans
  # - build: Package building and validation
  #
  # Phase 3 jobs (test matrix, performance, benchmarks) depend on these
  # ============================================================================
  
  # Build environment and cache dependencies for test matrix jobs
  # This job uses setup-python's built-in pip caching which is shared across all jobs
  environment-build:
    name: Build Environment
    runs-on: ubuntu-latest
    needs: [quality-check]  # Depends on quality check passing
    timeout-minutes: 10
    defaults:
      run:
        working-directory: FollowWeb
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python 3.12
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'
        cache: 'pip'
        cache-dependency-path: 'FollowWeb/requirements-ci.txt'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip setuptools wheel
        python -m pip install -r requirements-ci.txt -e .
    
    - name: Verify environment
      run: |
        python -c "import FollowWeb_Visualizor; print('✅ Package ready')"
        echo "### ✅ Environment Built" >> "$GITHUB_STEP_SUMMARY"
  
  # Fast smoke test - validates core functionality (depends on quality-check)
  smoke-test:
    name: Smoke Test
    runs-on: ubuntu-latest
    needs: [quality-check]  # Depends on quality check passing
    timeout-minutes: 10
    defaults:
      run:
        working-directory: FollowWeb
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python 3.12
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'
        cache: 'pip'
        cache-dependency-path: 'FollowWeb/requirements-ci.txt'
    
    - name: Install minimal dependencies
      run: |
        python -m pip install --upgrade pip setuptools wheel
        python -m pip install -r requirements-ci.txt -e .
    
    - name: Run quick smoke tests
      run: |
        cpu_count=$(python -c "import os; print(os.cpu_count() or 1)")
        echo "Running smoke tests with $cpu_count workers"
        python -m pytest -p no:benchmark -m unit -x --maxfail=1 --tb=short -v -n "$cpu_count"
      env:
        MPLBACKEND: Agg
        PYTHONPATH: ${{ github.workspace }}/FollowWeb
    
    - name: Smoke test summary
      if: success()
      run: |
        echo "### ✅ Smoke Test Passed" >> "$GITHUB_STEP_SUMMARY"
  
  # Security scanning - depends on quality-check
  # Installs its own minimal dependencies
  security:
    name: Security Scan
    runs-on: ubuntu-latest
    needs: [quality-check]  # Depends on quality check passing
    timeout-minutes: 10
    defaults:
      run:
        working-directory: FollowWeb
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python 3.12
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'
        cache: 'pip'
        cache-dependency-path: 'FollowWeb/requirements-ci.txt'
    
    - name: Install security scanning tools (minimal dependencies)
      run: |
        python -m pip install --upgrade pip
        python -m pip install bandit[toml] pip-audit -r requirements-ci.txt -e .
    
    - name: Run Bandit security linter
      run: |
        # Generate JSON report for artifacts (include all issues for reporting)
        bandit -r FollowWeb_Visualizor -f json -o bandit-report.json || true
        
        python ../.github/scripts/ci_helpers.py info "**Bandit Security Scan Results**" --summary-only
        python ../.github/scripts/ci_helpers.py info "\`\`\`" --summary-only
        
        # First, run bandit for all severity levels to show complete results
        python ../.github/scripts/ci_helpers.py info "**All Security Issues Found:**" --print-only
        bandit -r FollowWeb_Visualizor 2>&1 | tee -a "$GITHUB_STEP_SUMMARY" || true
        
        python ../.github/scripts/ci_helpers.py info "" --summary-only
        python ../.github/scripts/ci_helpers.py info "**Critical Check (Medium/High Severity Only):**" --summary-only
        
        # Now check for medium/high severity issues - FAIL only on these
        set +e  # Don't exit on error temporarily
        bandit_critical_output=$(bandit -r FollowWeb_Visualizor --severity-level medium 2>&1)
        bandit_critical_exit_code=$?
        
        echo "$bandit_critical_output" >> "$GITHUB_STEP_SUMMARY"
        
        # Allow bandit to fail as long as failure is non-critical (low severity only)
        # Exit codes: 0 = no issues, 1 = issues found, >1 = scanner error
        if [ "$bandit_critical_exit_code" -eq 0 ] || echo "$bandit_critical_output" | grep -q "No issues identified"; then
          python ../.github/scripts/ci_helpers.py success "No medium or high severity security issues found (low severity issues are acceptable)" --print-only
          python ../.github/scripts/ci_helpers.py success "No critical security issues found" --summary-only
        elif [ "$bandit_critical_exit_code" -eq 1 ]; then
          # Exit code 1 with medium+ severity means critical issues found
          python ../.github/scripts/ci_helpers.py error "CRITICAL: Medium or high severity security issues detected"
          exit 1
        else
          # Exit code >1 means scanner error - this is a critical failure
          python ../.github/scripts/ci_helpers.py error "Bandit security scanner failed to run properly (exit code: $bandit_critical_exit_code)" --print-only
          python ../.github/scripts/ci_helpers.py error "Bandit security scanner failed to run" --summary-only
          exit 1
        fi
        
        python ../.github/scripts/ci_helpers.py info "\`\`\`" --summary-only
      continue-on-error: false
    
    - name: Run pip-audit for known vulnerabilities
      run: |
        # Generate JSON report for artifacts
        pip-audit --format=json --output=pip-audit-report.json
        # Run vulnerability scan - FAIL on any vulnerabilities
        python ../.github/scripts/ci_helpers.py info "**Security Scan Results**" --summary-only
        python ../.github/scripts/ci_helpers.py info "\`\`\`" --summary-only
        pip-audit --desc >> "$GITHUB_STEP_SUMMARY"
        python ../.github/scripts/ci_helpers.py info "\`\`\`" --summary-only
      continue-on-error: false
    
    - name: Upload security reports
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: security-reports-ubuntu-latest
        path: |
          FollowWeb/bandit-report.json
          FollowWeb/pip-audit-report.json
        retention-days: 7
        if-no-files-found: ignore
  
  # Package building - depends on quality-check
  # Only needs build tools (not environment-build)
  build:
    name: Build & Package
    runs-on: ubuntu-latest
    needs: [quality-check]  # Depends on quality check passing
    timeout-minutes: 15
    defaults:
      run:
        working-directory: FollowWeb
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for proper versioning
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'
    
    - name: Install build dependencies
      run: |
        python -m pip install --upgrade pip setuptools wheel build twine check-manifest
    
    - name: Clean build artifacts
      run: |
        # Clean any previous build artifacts to avoid package configuration issues
        if [ -d "build" ]; then rm -rf build; fi
        if [ -n "$(find . -maxdepth 1 -name '*.egg-info' -type d)" ]; then rm -rf ./*.egg-info; fi
      shell: bash
    
    - name: Verify package manifest
      run: |
        check-manifest --verbose
    
    - name: Build package
      run: |
        python -m build --sdist --wheel --outdir dist/
    
    - name: Validate package structure
      run: |
        # Ensure the wheel contains only the expected packages
        python -m zipfile -l dist/*.whl | grep -E "(FollowWeb_Visualizor/|followweb_visualizor.*dist-info/)" || python ../.github/scripts/ci_helpers.py info "Package structure validation" --print-only
        # Verify no unwanted packages are included
        if python -m zipfile -l dist/*.whl | grep -E "(tests/|analysis_tools/|build/)"; then
          python ../.github/scripts/ci_helpers.py error "Unwanted packages found in wheel"
          exit 1
        fi
    
    - name: Verify package integrity
      run: |
        twine check dist/*
        
    - name: Display package info
      run: |
        find dist/ -type f -exec ls -lh {} \;
        python -m zipfile -l dist/*.whl || true
      shell: bash
    
    - name: Test package installation in clean environment
      run: |
        # Create a temporary virtual environment to test package installation
        python -m venv test_env
        source test_env/bin/activate || test_env\\Scripts\\activate
        pip install dist/*.whl
        python -c "import FollowWeb_Visualizor"
        python ../.github/scripts/ci_helpers.py success "Package installs and imports correctly" --print-only
        deactivate
      shell: bash
    
    - name: Upload build artifacts
      uses: actions/upload-artifact@v4
      with:
        name: python-package-distributions
        path: FollowWeb/dist/
        retention-days: 30
  
  # ============================================================================
  # PHASE 3: MATRIX TESTS & HEAVY JOBS (depends on Phase 2)
  # ============================================================================
  # These jobs run in parallel but depend on Phase 2 completing:
  # - test: Matrix tests (3 OS × Python versions)
  # - performance: Performance tests
  # - benchmarks: Benchmark tests
  # - documentation: Documentation generation
  # ============================================================================
  
  # PHASE 3: MATRIX TESTS (depends on environment-build and quality-check)
  # Optimized matrix: 3 strategic builds covering key platforms and Python versions
  # - macOS latest supported (3.12)
  # - Linux oldest supported (3.9) 
  # - Windows stable version (3.12)
  test:
    name: Test Python ${{ matrix.python-version }} on ${{ matrix.os }}
    runs-on: ${{ matrix.os }}
    needs: [environment-build, quality-check]  # Needs env build for pip cache, quality-check for code validation
    timeout-minutes: 30
    defaults:
      run:
        working-directory: FollowWeb
    
    strategy:
      fail-fast: true
      max-parallel: 3
      matrix:
        include:
          - os: macos-latest
            python-version: '3.12'
          - os: ubuntu-latest
            python-version: '3.9'
          - os: windows-latest
            python-version: '3.12'
    
    continue-on-error: false
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        # Fetch full history for better coverage analysis
        fetch-depth: 0
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}
        cache: 'pip'
        cache-dependency-path: 'FollowWeb/requirements-ci.txt'
    
    - name: Display Python version and environment info
      run: |
        python --version
        python -c "import sys; print(f'Python executable: {sys.executable}')"
        python -c "import platform; print(f'Platform: {platform.platform()}')"
        pip --version
    
    - name: Install dependencies (using pip cache from setup-python)
      run: |
        python -m pip install --upgrade pip setuptools wheel
        python -m pip install -r requirements-ci.txt -e .
      shell: bash
    
    - name: Verify installation
      run: |
        # Package must import successfully
        python -c "import FollowWeb_Visualizor"
        python ../.github/scripts/ci_helpers.py success "Package imported successfully"
        
        # Test parallel processing availability
        python -c "from FollowWeb_Visualizor.utils.parallel import is_nx_parallel_available; is_nx_parallel_available()"
        python ../.github/scripts/ci_helpers.py success "nx-parallel available"
        
        # Test all submodules import
        python -c "from FollowWeb_Visualizor import analysis, core, data, output, utils, visualization"
        python ../.github/scripts/ci_helpers.py success "All submodules imported successfully"
        
        # Test basic functionality
        python -c "from FollowWeb_Visualizor.core.config import ConfigurationManager; from FollowWeb_Visualizor.data.loaders import DataLoader, InstagramLoader"
        python ../.github/scripts/ci_helpers.py success "Core functionality verified"
        
        # CLI entry point must work
        if followweb --help > /dev/null 2>&1; then
          python ../.github/scripts/ci_helpers.py success "CLI entry point working"
        else
          python ../.github/scripts/ci_helpers.py error "CLI entry point failed"
          followweb --help || true
          exit 1
        fi
      shell: bash
    
    # Coverage runs ONLY on ubuntu-latest with Python 3.9 (oldest supported)
    # This ensures coverage is calculated on the most restrictive environment
    - name: Run tests with coverage
      if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.9'
      run: |
        set -e  # Exit immediately if any command fails
        threshold=$(python ../.github/scripts/ci_helpers.py coverage-threshold-value)
        # Get CPU count for CI environment (use all cores, no safety margin needed in CI)
        cpu_count=$(python -c "import os; print(os.cpu_count() or 1)")
        echo "Running tests with $cpu_count workers (all available cores)"
        # Run fast tests (unit + integration) with coverage in parallel
        # Exclude slow, performance, and benchmark tests - those run in dedicated performance job
        # FAIL FAST: Stop immediately on first failure to save CI time
        # 
        # OPTIMIZATION: Benchmark Plugin Isolation (Requirement R5)
        # - Use -p no:benchmark to disable pytest-benchmark plugin
        # - Prevents forced sequential mode in parallel test execution
        # - Benchmark tests run separately in dedicated benchmarks job
        python -m pytest -p no:benchmark -m "not (benchmark or slow or performance)" -x --maxfail=1 -n "$cpu_count" --cov=FollowWeb_Visualizor --cov-report=xml --cov-report=term --cov-report=html --cov-fail-under="$threshold" || {
          exit_code=$?
          python ../.github/scripts/ci_helpers.py error "Test Execution Failed" --summary-only
          python ../.github/scripts/ci_helpers.py info "**Exit Code:** $exit_code" --summary-only
          exit $exit_code
        }
        
        # Generate test results summary with dynamic counts
        python ../.github/scripts/ci_helpers.py test-summary
      shell: bash
      continue-on-error: false
      env:
        MPLBACKEND: Agg
        PYTHONPATH: ${{ github.workspace }}/FollowWeb
    
    # Other matrix jobs skip coverage for faster execution
    - name: Run tests without coverage
      if: matrix.os != 'ubuntu-latest' || matrix.python-version != '3.9'
      run: |
        set -e  # Exit immediately if any command fails
        # Get CPU count for CI environment (use all cores, no safety margin needed in CI)
        cpu_count=$(python -c "import os; print(os.cpu_count() or 1)")
        echo "Running tests with $cpu_count workers (all available cores)"
        # Run fast tests (unit + integration) without coverage in parallel
        # Exclude slow, performance, and benchmark tests - those run in dedicated performance job
        # FAIL FAST: Stop immediately on first failure to save CI time
        # 
        # OPTIMIZATION: Benchmark Plugin Isolation (Requirement R5)
        # - Use -p no:benchmark to disable pytest-benchmark plugin
        # - Prevents forced sequential mode in parallel test execution
        # - Allows full parallelization with pytest-xdist (-n auto)
        python -m pytest -p no:benchmark -m "not (benchmark or slow or performance)" -x --maxfail=1 -n "$cpu_count" || {
          exit_code=$?
          python ../.github/scripts/ci_helpers.py error "Test Execution Failed" --summary-only
          python ../.github/scripts/ci_helpers.py info "**Exit Code:** $exit_code" --summary-only
          exit $exit_code
        }
        
        # Generate test results summary with dynamic counts
        python ../.github/scripts/ci_helpers.py test-summary
      shell: bash
      continue-on-error: false
      env:
        MPLBACKEND: Agg
        PYTHONPATH: ${{ github.workspace }}/FollowWeb
    
    - name: Upload coverage reports
      if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.9'
      uses: actions/upload-artifact@v4
      with:
        name: coverage-reports
        path: |
          FollowWeb/coverage.xml
          FollowWeb/htmlcov/
        retention-days: 30
        if-no-files-found: warn

  # Performance tests - runs in parallel with test matrix
  performance:
    name: Performance Tests
    runs-on: ubuntu-latest
    needs: [environment-build, quality-check]  # Needs env build for pip cache, quality-check for code validation
    timeout-minutes: 20
    defaults:
      run:
        working-directory: FollowWeb
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python 3.12
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'
        cache: 'pip'
        cache-dependency-path: 'FollowWeb/requirements-ci.txt'
    
    - name: Install dependencies (using pip cache from setup-python)
      run: |
        python -m pip install --upgrade pip
        python -m pip install -r requirements-ci.txt -e .
    
    - name: Run performance tests
      run: |
        # Run slow and performance tests sequentially (no parallelization)
        # Exclude benchmark tests - those run in separate benchmarks job
        # FAIL FAST: Stop immediately on first failure to save CI time
        python -m pytest -m "slow or performance" -x --maxfail=1 -v --tb=short -n 0
        
        # Generate performance test summary
        python ../.github/scripts/ci_helpers.py success "Performance tests completed - no regressions detected" --print-only
        
        # Write to GitHub step summary
        python ../.github/scripts/ci_helpers.py info "**Performance Test Results**" --summary-only
        python ../.github/scripts/ci_helpers.py success "Performance tests completed successfully - no regressions detected" --summary-only
      env:
        MPLBACKEND: Agg
        PYTEST_PARALLEL_DISABLE: "1"
      continue-on-error: false

  # Benchmark tests - runs in parallel with test matrix (only needs quality checks)
  benchmarks:
    name: Benchmark Tests
    runs-on: ubuntu-latest
    needs: [environment-build, quality-check]  # Needs env build for pip cache, quality-check for code validation
    timeout-minutes: 20
    defaults:
      run:
        working-directory: FollowWeb
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python 3.12
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'
        cache: 'pip'
        cache-dependency-path: 'FollowWeb/requirements-ci.txt'
    
    - name: Install dependencies (using pip cache from setup-python)
      run: |
        python -m pip install --upgrade pip
        python -m pip install -r requirements-ci.txt -e .
    
    - name: Run benchmark tests
      run: |
        # Create benchmarks directory
        mkdir -p .benchmarks
        # Run benchmark tests sequentially (no parallelization)
        # FAIL FAST: Stop immediately on first failure to save CI time
        python -m pytest -m "benchmark" -x --maxfail=1 -v --tb=short -n 0 --benchmark-save=ci_run --benchmark-save-data --benchmark-storage=.benchmarks
        
        # Generate benchmark test summary
        python ../.github/scripts/ci_helpers.py success "Benchmark tests completed - no regressions detected" --print-only
      env:
        MPLBACKEND: Agg
        PYTEST_PARALLEL_DISABLE: "1"
      continue-on-error: false
    
    - name: Generate benchmark report
      if: always()
      run: |
        # Generate structured benchmark summary
        python ../.github/scripts/ci_helpers.py benchmark-summary
        
        # Also show detailed pytest-benchmark output
        python ../.github/scripts/ci_helpers.py info "" --summary-only
        python ../.github/scripts/ci_helpers.py info "**Detailed Benchmark Output**" --summary-only
        python ../.github/scripts/ci_helpers.py info "\`\`\`" --summary-only
        
        # Run benchmark comparison if we have previous data
        if [ -f ".benchmarks/ci_run/0001_*.json" ]; then
          # Display current benchmark results with comparison
          python -m pytest -m benchmark --benchmark-only --benchmark-compare=ci_run --benchmark-storage=.benchmarks --tb=no -v 2>&1 | tee -a "$GITHUB_STEP_SUMMARY" || true
        else
          echo "No previous benchmark data found - this is the baseline run" | tee -a "$GITHUB_STEP_SUMMARY"
          # Just show the current results
          python -m pytest -m benchmark --benchmark-only --benchmark-storage=.benchmarks --tb=no -v 2>&1 | tee -a "$GITHUB_STEP_SUMMARY" || true
        fi
        
        python ../.github/scripts/ci_helpers.py info "\`\`\`" --summary-only
      shell: bash
      continue-on-error: true
    
    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: benchmark-results
        path: FollowWeb/.benchmarks/
        retention-days: 30
        if-no-files-found: ignore
  
  # Documentation pipeline - runs in parallel with test matrix
  documentation:
    name: Documentation
    needs: [quality-check]  # Only needs quality checks to pass
    uses: ./.github/workflows/docs.yml
  
  # All checks must pass - no exceptions
  ci-success:
    name: CI Success
    runs-on: ubuntu-latest
    needs: [environment-build, smoke-test, quality-check, test, security, build, performance, benchmarks, documentation]
    if: always()
    
    steps:
    - name: Check all jobs status
      run: |
        # All jobs must succeed - no failures allowed
        # This check runs BEFORE any installs to fail fast if any job failed
        failed_jobs=""
        skipped_jobs=""
        
        # Check each job result - fail on anything other than "success"
        if [[ "${{ needs.quality-check.result }}" != "success" ]]; then
          if [[ "${{ needs.quality-check.result }}" == "skipped" ]]; then
            skipped_jobs="$skipped_jobs QualityCheck(${{ needs.quality-check.result }})"
          else
            failed_jobs="$failed_jobs QualityCheck(${{ needs.quality-check.result }})"
          fi
        fi
        
        if [[ "${{ needs.smoke-test.result }}" != "success" ]]; then
          if [[ "${{ needs.smoke-test.result }}" == "skipped" ]]; then
            skipped_jobs="$skipped_jobs SmokeTest(${{ needs.smoke-test.result }})"
          else
            failed_jobs="$failed_jobs SmokeTest(${{ needs.smoke-test.result }})"
          fi
        fi
        
        if [[ "${{ needs.test.result }}" != "success" ]]; then
          if [[ "${{ needs.test.result }}" == "skipped" ]]; then
            skipped_jobs="$skipped_jobs FullTests(${{ needs.test.result }})"
          else
            failed_jobs="$failed_jobs FullTests(${{ needs.test.result }})"
          fi
        fi
        
        if [[ "${{ needs.security.result }}" != "success" ]]; then
          if [[ "${{ needs.security.result }}" == "skipped" ]]; then
            skipped_jobs="$skipped_jobs Security(${{ needs.security.result }})"
          else
            failed_jobs="$failed_jobs Security(${{ needs.security.result }})"
          fi
        fi
        

        
        if [[ "${{ needs.build.result }}" != "success" ]]; then
          if [[ "${{ needs.build.result }}" == "skipped" ]]; then
            skipped_jobs="$skipped_jobs Build(${{ needs.build.result }})"
          else
            failed_jobs="$failed_jobs Build(${{ needs.build.result }})"
          fi
        fi
        
        if [[ "${{ needs.performance.result }}" != "success" ]]; then
          if [[ "${{ needs.performance.result }}" == "skipped" ]]; then
            skipped_jobs="$skipped_jobs Performance(${{ needs.performance.result }})"
          else
            failed_jobs="$failed_jobs Performance(${{ needs.performance.result }})"
          fi
        fi
        
        if [[ "${{ needs.benchmarks.result }}" != "success" ]]; then
          if [[ "${{ needs.benchmarks.result }}" == "skipped" ]]; then
            skipped_jobs="$skipped_jobs Benchmarks(${{ needs.benchmarks.result }})"
          else
            failed_jobs="$failed_jobs Benchmarks(${{ needs.benchmarks.result }})"
          fi
        fi
        
        if [[ "${{ needs.documentation.result }}" != "success" ]]; then
          if [[ "${{ needs.documentation.result }}" == "skipped" ]]; then
            skipped_jobs="$skipped_jobs Documentation(${{ needs.documentation.result }})"
          else
            failed_jobs="$failed_jobs Documentation(${{ needs.documentation.result }})"
          fi
        fi
        
        # Fail if any jobs failed
        if [[ -n "$failed_jobs" ]]; then
          echo "::error::CI PIPELINE FAILED - Failed Jobs:$failed_jobs"
          {
            echo "**[FAIL] CI Pipeline Failed**"
            echo ""
            echo "**Failed Jobs:**$failed_jobs"
            echo ""
            echo "**Job Results:**"
            echo "- Environment Build: ${{ needs.environment-build.result }}"
            echo "- Smoke Test: ${{ needs.smoke-test.result }}"
            echo "- Quality Check: ${{ needs.quality-check.result }}"
            echo "- Matrix Tests: ${{ needs.test.result }}"
            echo "- Security: ${{ needs.security.result }}"
            echo "- Build: ${{ needs.build.result }}"
            echo "- Performance: ${{ needs.performance.result }}"
            echo "- Benchmarks: ${{ needs.benchmarks.result }}"
            echo "- Documentation: ${{ needs.documentation.result }}"
            echo ""
            echo "**Action Required:** Review the failed job logs above for details."
          } >> "$GITHUB_STEP_SUMMARY"
          if [[ -n "$skipped_jobs" ]]; then
            {
              echo ""
              echo "**Skipped Jobs:**$skipped_jobs"
            } >> "$GITHUB_STEP_SUMMARY"
          fi
          exit 1
        fi
        
        # Warn about skipped jobs but don't fail
        if [[ -n "$skipped_jobs" ]]; then
          echo "::warning::Some jobs were skipped:$skipped_jobs"
          {
            echo "**[WARN] CI PIPELINE WARNING**"
            echo "**Skipped Jobs:**$skipped_jobs"
          } >> "$GITHUB_STEP_SUMMARY"
        fi
        
        echo "::notice::ALL PREREQUISITE JOBS PASSED - PROCEEDING WITH SUCCESS VALIDATION"
    
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
    
    - name: Generate success summary
      run: |
        # Generate success summary
        echo "::notice::ALL CHECKS PASSED!"
        {
          echo "**[PASS] CI Pipeline Success**"
          echo ""
          echo "All quality checks passed:"
          echo "- [PASS] Environment Build: ${{ needs.environment-build.result }}"
          echo "- [PASS] Smoke Test: ${{ needs.smoke-test.result }}"
          echo "- [PASS] Quality Check: ${{ needs.quality-check.result }}"
          echo "- [PASS] Matrix Tests: ${{ needs.test.result }}"
          echo "- [PASS] Security: ${{ needs.security.result }}"
          echo "- [PASS] Build: ${{ needs.build.result }}"
          echo "- [PASS] Performance: ${{ needs.performance.result }}"
          echo "- [PASS] Benchmarks: ${{ needs.benchmarks.result }}"
          echo "- [PASS] Documentation: ${{ needs.documentation.result }}"
        } >> "$GITHUB_STEP_SUMMARY"