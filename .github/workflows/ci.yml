# CI Pipeline for FollowWeb
# 
# This workflow uses only FREE and OPEN-SOURCE tools with permissive licenses:
# - GitHub Actions (MIT License)
# - Python ecosystem tools (Python Software Foundation License)
# - ruff (MIT License) - Code formatting and linting
# - mypy (MIT License) - Type checking
# - pytest (MIT License) - Testing framework
# - bandit (Apache 2.0) - Security linting
# - pip-audit (Apache 2.0) - Vulnerability scanning
# - NetworkX (BSD License) - Graph analysis
# - All other dependencies use MIT, BSD, or Apache 2.0 licenses
# 
# NO PROPRIETARY SERVICES OR TOKENS REQUIRED
# 
# This workflow has been hardened based on real-world issues encountered:
# 
# 1. Type Checking Issues:
#    - Install missing type stubs (types-python-dateutil, types-PyYAML, types-decorator)
#    - Clean build artifacts before mypy to avoid conflicts
#    - Configure mypy to ignore external library issues via pyproject.toml
#
# 2. Code Quality Issues:
#    - Use ruff for both linting and formatting (replaces black + flake8)
#    - Configure .ipynb exclusion in pyproject.toml rather than CLI flags
#    - Separate formatting check from linting for clearer error reporting
#
# 3. Security Scanning Issues:
#    - Use pip-audit instead of safety (no registration required)
#    - Only fail on medium/high severity bandit issues (low severity acceptable)
#
# 4. Package Building Issues:
#    - Clean build artifacts before building to avoid stale files
#    - Use explicit package list in pyproject.toml to avoid including unwanted packages
#    - Verify package integrity with twine check
#
# 5. Dependency Management:
#    - Install type stubs explicitly to avoid mypy failures
#    - Use separate requirements files for different purposes
#    - Handle conditional dependencies properly (nx-parallel for Python 3.11+)
#
# Common Troubleshooting:
# - If mypy fails with external library errors: Update type stub installations
# - If package build includes wrong files: Check pyproject.toml packages configuration
# - If ruff fails on notebooks: Ensure extend-exclude in pyproject.toml includes "*.ipynb"
# - If security scan fails: Check if new vulnerabilities need dependency updates
# - If tests fail randomly: May be parallel execution issues, check pytest-xdist configuration

name: CI

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run tests weekly on Saturdays at 2 AM UTC to catch dependency issues
    # Changed from Sunday to avoid collision with nightly workflow
    - cron: '0 2 * * 6'
  workflow_dispatch:
    # Allow manual triggering of the workflow

# Cancel in-progress runs for the same workflow on the same branch
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  # Prebuild environment - runs in parallel with smoke test
  # Creates cached environment for all subsequent jobs
  prebuild:
    name: Prebuild Environment
    uses: ./.github/workflows/codespaces-prebuild.yml
  
  # Quick smoke test on Ubuntu with latest Python - gates the build job
  # Runs unit tests only (excludes integration tests) for faster feedback
  # Full test suite including integration tests runs in the 'test' job after this passes
  test-quick:
    name: Smoke Test (Python 3.12 on Ubuntu)
    runs-on: ubuntu-latest
    timeout-minutes: 15
    defaults:
      run:
        working-directory: FollowWeb
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
    
    - name: Configure Git
      run: |
        git config --global init.defaultBranch main
        git config --global advice.detachedHead false
    
    - name: Set up Python 3.12
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'
        cache: 'pip'
        cache-dependency-path: 'FollowWeb/requirements-ci.txt'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip setuptools wheel
        python -m pip install -r requirements-ci.txt -e .
      shell: bash
    
    - name: Run quick smoke tests
      run: |
        set -e
        # Get CPU count for CI environment (use all cores, no safety margin needed in CI)
        cpu_count=$(python -c "import os; print(os.cpu_count() or 1)")
        echo "Running tests with $cpu_count workers (all available cores)"
        # Run unit tests only (fast, isolated tests with maximum parallelization)
        # No coverage requirement for smoke test - just validate code works
        python -m pytest -m unit --maxfail=5 --tb=short -v -n $cpu_count
      shell: bash
      env:
        MPLBACKEND: Agg
        PYTHONPATH: ${{ github.workspace }}/FollowWeb

  # Full test matrix - tests oldest (3.9) and newest (3.12) Python on all platforms
  # Ubuntu 3.12 already tested in smoke test, so excluded here
  test:
    name: Test Python ${{ matrix.python-version }} on ${{ matrix.os }}
    runs-on: ${{ matrix.os }}
    needs: test-quick  # Wait for smoke test to pass
    timeout-minutes: 15
    defaults:
      run:
        working-directory: FollowWeb
    
    strategy:
      fail-fast: true
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python-version: ['3.9', '3.12']
    
    continue-on-error: false
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        # Fetch full history for better coverage analysis
        fetch-depth: 0
    
    - name: Configure Git
      run: |
        git config --global init.defaultBranch main
        git config --global advice.detachedHead false
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}
        cache: 'pip'
        cache-dependency-path: 'FollowWeb/requirements-ci.txt'
    
    - name: Restore prebuild cache
      if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.12'
      uses: actions/cache/restore@v4
      with:
        path: |
          ~/.cache/pip
          FollowWeb/.venv
          FollowWeb/tests/test_data
          .pytest_cache
          .mypy_cache
          .ruff_cache
        key: prebuild-${{ runner.os }}-py3.12-${{ hashFiles('FollowWeb/requirements-ci.txt', 'FollowWeb/pyproject.toml') }}
        restore-keys: |
          prebuild-${{ runner.os }}-py3.12-
    
    - name: Display Python version and environment info
      run: |
        python --version
        python -c "import sys; print(f'Python executable: {sys.executable}')"
        python -c "import platform; print(f'Platform: {platform.platform()}')"
        pip --version
    
    - name: Install dependencies
      run: |
        # Use cached environment if available, otherwise install fresh
        if [ -d ".venv" ] && [ "${{ matrix.os }}" == "ubuntu-latest" ] && [ "${{ matrix.python-version }}" == "3.12" ]; then
          echo "Using prebuild cache"
          source .venv/bin/activate
        else
          echo "Installing dependencies fresh"
          python -m pip install --upgrade pip setuptools wheel
          python -m pip install -r requirements-ci.txt -e .
        fi
      shell: bash
    
    - name: Verify installation
      run: |
        # Package must import successfully
        python -c "import FollowWeb_Visualizor"
        python ../.github/scripts/ci_helpers.py success "Package imported successfully"
        
        # Test parallel processing availability
        python -c "from FollowWeb_Visualizor.utils.parallel import is_nx_parallel_available; is_nx_parallel_available()"
        python ../.github/scripts/ci_helpers.py success "nx-parallel available"
        
        # Test all submodules import
        python -c "from FollowWeb_Visualizor import analysis, core, data, output, utils, visualization"
        python ../.github/scripts/ci_helpers.py success "All submodules imported successfully"
        
        # Test basic functionality
        python -c "from FollowWeb_Visualizor.core.config import ConfigurationManager; from FollowWeb_Visualizor.data.loaders import DataLoader, InstagramLoader"
        python ../.github/scripts/ci_helpers.py success "Core functionality verified"
        
        # CLI entry point must work
        if followweb --help > /dev/null 2>&1; then
          python ../.github/scripts/ci_helpers.py success "CLI entry point working"
        else
          python ../.github/scripts/ci_helpers.py error "CLI entry point failed"
          followweb --help || true
          exit 1
        fi
      shell: bash
    
    - name: Run code formatting check
      run: |
        ruff format --check FollowWeb_Visualizor tests --diff
      continue-on-error: false
    
    - name: Run linting
      run: |
        ruff check FollowWeb_Visualizor tests --output-format=github
      continue-on-error: false
    
    - name: Run type checking
      run: |
        # Clean any previous build artifacts that might interfere with mypy
        if [ -d "build" ]; then rm -rf build; fi
        # Run mypy with proper configuration to ignore external library issues
        mypy FollowWeb_Visualizor
      continue-on-error: false
      shell: bash
    
    - name: Run tests with coverage
      run: |
        set -e  # Exit immediately if any command fails
        # Only run coverage on Ubuntu with Python 3.11 to save time
        if [ "${{ matrix.os }}" == "ubuntu-latest" ] && [ "${{ matrix.python-version }}" == "3.11" ]; then
          threshold=$(python ../.github/scripts/ci_helpers.py coverage-threshold-value)
          # Run fast tests (unit + integration) with coverage in parallel
          # Exclude slow, performance, and benchmark tests - those run in dedicated performance job
          python -m pytest -m "not (benchmark or slow or performance)" --cov=FollowWeb_Visualizor --cov-report=xml --cov-report=term --cov-report=html --cov-fail-under=$threshold || {
            exit_code=$?
            python ../.github/scripts/ci_helpers.py error "Test Execution Failed" --summary-only
            python ../.github/scripts/ci_helpers.py info "**Exit Code:** $exit_code" --summary-only
            exit $exit_code
          }
        else
          # Run tests without coverage on other platforms for speed
          python -m pytest -m "not (benchmark or slow or performance)" || {
            exit_code=$?
            python ../.github/scripts/ci_helpers.py error "Test Execution Failed" --summary-only
            python ../.github/scripts/ci_helpers.py info "**Exit Code:** $exit_code" --summary-only
            exit $exit_code
          }
        fi
        
        # Generate test results summary with dynamic counts
        python ../.github/scripts/ci_helpers.py test-summary
      shell: bash
      continue-on-error: false
      env:
        MPLBACKEND: Agg
        PYTHONPATH: ${{ github.workspace }}/FollowWeb
    
    - name: Upload coverage reports
      if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
      uses: actions/upload-artifact@v4
      with:
        name: coverage-reports-${{ matrix.os }}-${{ matrix.python-version }}
        path: |
          FollowWeb/coverage.xml
          FollowWeb/htmlcov/
        retention-days: 7
        if-no-files-found: ignore
    
    - name: Generate coverage report summary
      if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
      run: |
        python ../.github/scripts/ci_helpers.py info "**Coverage Report Summary**" --summary-only
        python ../.github/scripts/ci_helpers.py info "\`\`\`" --summary-only
        python -m coverage report --show-missing >> $GITHUB_STEP_SUMMARY || python ../.github/scripts/ci_helpers.py error "Coverage report generation failed" --summary-only
        python ../.github/scripts/ci_helpers.py info "\`\`\`" --summary-only
        python ../.github/scripts/ci_helpers.py info "Full HTML coverage report available in artifacts." --summary-only

  security:
    name: Security Scan
    runs-on: ubuntu-latest
    needs: [test-quick, test]  # Wait for smoke test and full test matrix to pass
    timeout-minutes: 5
    defaults:
      run:
        working-directory: FollowWeb
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Configure Git
      run: |
        git config --global init.defaultBranch main
        git config --global advice.detachedHead false
    
    - name: Set up Python 3.12
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'
        cache: 'pip'
        cache-dependency-path: 'requirements-ci.txt'
    
    - name: Restore prebuild cache
      uses: actions/cache/restore@v4
      with:
        path: |
          ~/.cache/pip
          FollowWeb/.venv
          FollowWeb/tests/test_data
          .pytest_cache
          .mypy_cache
          .ruff_cache
        key: prebuild-${{ runner.os }}-py3.12-${{ hashFiles('FollowWeb/requirements-ci.txt', 'FollowWeb/pyproject.toml') }}
        restore-keys: |
          prebuild-${{ runner.os }}-py3.12-
    
    - name: Install security tools
      run: |
        # Use cached environment if available
        if [ -d ".venv" ]; then
          echo "Using prebuild cache"
          source .venv/bin/activate
        else
          echo "Installing dependencies fresh"
          python -m pip install --upgrade pip
          python -m pip install bandit[toml] pip-audit -r requirements-ci.txt
        fi
    
    - name: Run Bandit security linter
      run: |
        # Activate venv if it exists
        if [ -d ".venv" ]; then source .venv/bin/activate; fi
        # Generate JSON report for artifacts (include all issues for reporting)
        bandit -r FollowWeb_Visualizor -f json -o bandit-report.json || true
        
        python ../.github/scripts/ci_helpers.py info "**Bandit Security Scan Results**" --summary-only
        python ../.github/scripts/ci_helpers.py info "\`\`\`" --summary-only
        
        # First, run bandit for all severity levels to show complete results
        python ../.github/scripts/ci_helpers.py info "**All Security Issues Found:**" --print-only
        bandit -r FollowWeb_Visualizor 2>&1 | tee -a $GITHUB_STEP_SUMMARY || true
        
        python ../.github/scripts/ci_helpers.py info "" --summary-only
        python ../.github/scripts/ci_helpers.py info "**Critical Check (Medium/High Severity Only):**" --summary-only
        
        # Now check for medium/high severity issues - FAIL only on these
        set +e  # Don't exit on error temporarily
        bandit_critical_output=$(bandit -r FollowWeb_Visualizor --severity-level medium 2>&1)
        bandit_critical_exit_code=$?
        
        echo "$bandit_critical_output" >> $GITHUB_STEP_SUMMARY
        
        # Allow bandit to fail as long as failure is non-critical (low severity only)
        # Exit codes: 0 = no issues, 1 = issues found, >1 = scanner error
        if [ $bandit_critical_exit_code -eq 0 ] || echo "$bandit_critical_output" | grep -q "No issues identified"; then
          python ../.github/scripts/ci_helpers.py success "No medium or high severity security issues found (low severity issues are acceptable)" --print-only
          python ../.github/scripts/ci_helpers.py success "No critical security issues found" --summary-only
        elif [ $bandit_critical_exit_code -eq 1 ]; then
          # Exit code 1 with medium+ severity means critical issues found
          python ../.github/scripts/ci_helpers.py error "CRITICAL: Medium or high severity security issues detected"
          exit 1
        else
          # Exit code >1 means scanner error - this is a critical failure
          python ../.github/scripts/ci_helpers.py error "Bandit security scanner failed to run properly (exit code: $bandit_critical_exit_code)" --print-only
          python ../.github/scripts/ci_helpers.py error "Bandit security scanner failed to run" --summary-only
          exit 1
        fi
        
        python ../.github/scripts/ci_helpers.py info "\`\`\`" --summary-only
      continue-on-error: false
    
    - name: Run pip-audit for known vulnerabilities
      run: |
        # Activate venv if it exists
        if [ -d ".venv" ]; then source .venv/bin/activate; fi
        # Generate JSON report for artifacts
        pip-audit --format=json --output=pip-audit-report.json
        # Run vulnerability scan - FAIL on any vulnerabilities
        python ../.github/scripts/ci_helpers.py info "**Security Scan Results**" --summary-only
        python ../.github/scripts/ci_helpers.py info "\`\`\`" --summary-only
        pip-audit --desc >> $GITHUB_STEP_SUMMARY
        python ../.github/scripts/ci_helpers.py info "\`\`\`" --summary-only
      continue-on-error: false
    
    - name: Upload security reports
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: security-reports-ubuntu-latest
        path: |
          FollowWeb/bandit-report.json
          FollowWeb/pip-audit-report.json
        retention-days: 7
        if-no-files-found: ignore

  format-check:
    name: Code Quality & Format Check
    runs-on: ubuntu-latest
    needs: [test-quick, test]  # Wait for smoke test and full test matrix to pass
    timeout-minutes: 5
    defaults:
      run:
        working-directory: FollowWeb
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python 3.12
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'
        cache: 'pip'
        cache-dependency-path: 'requirements-ci.txt'
    
    - name: Restore prebuild cache
      uses: actions/cache/restore@v4
      with:
        path: |
          ~/.cache/pip
          FollowWeb/.venv
          FollowWeb/tests/test_data
          .pytest_cache
          .mypy_cache
          .ruff_cache
        key: prebuild-${{ runner.os }}-py3.12-${{ hashFiles('FollowWeb/requirements-ci.txt', 'FollowWeb/pyproject.toml') }}
        restore-keys: |
          prebuild-${{ runner.os }}-py3.12-
    
    - name: Install dependencies
      run: |
        # Use cached environment if available
        if [ -d ".venv" ]; then
          echo "Using prebuild cache"
          source .venv/bin/activate
        else
          echo "Installing dependencies fresh"
          python -m pip install --upgrade pip
          python -m pip install -r requirements-minimal.txt
        fi
    
    - name: Check code formatting with ruff
      run: |
        # Activate venv if it exists
        if [ -d ".venv" ]; then source .venv/bin/activate; fi
        # Note: ruff format doesn't support --extend-exclude, use pyproject.toml config
        ruff format --check FollowWeb_Visualizor tests --diff
    
    - name: Run ruff linting
      run: |
        # Activate venv if it exists
        if [ -d ".venv" ]; then source .venv/bin/activate; fi
        ruff check FollowWeb_Visualizor tests --output-format=github --extend-exclude="*.ipynb"
    
    - name: Check import sorting
      run: |
        # Activate venv if it exists
        if [ -d ".venv" ]; then source .venv/bin/activate; fi
        ruff check --select I FollowWeb_Visualizor tests --extend-exclude="*.ipynb"

  build:
    name: Build & Package
    runs-on: ubuntu-latest
    needs: [test-quick, test]  # Wait for smoke test and full test matrix to pass
    timeout-minutes: 10
    defaults:
      run:
        working-directory: FollowWeb
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for proper versioning
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'
    
    - name: Install build dependencies
      run: |
        python -m pip install --upgrade pip setuptools wheel build twine check-manifest
    
    - name: Clean build artifacts
      run: |
        # Clean any previous build artifacts to avoid package configuration issues
        if [ -d "build" ]; then rm -rf build; fi
        if [ -d "*.egg-info" ]; then rm -rf *.egg-info; fi
      shell: bash
    
    - name: Verify package manifest
      run: |
        check-manifest --verbose
    
    - name: Build package
      run: |
        python -m build --sdist --wheel --outdir dist/
    
    - name: Validate package structure
      run: |
        # Ensure the wheel contains only the expected packages
        python -m zipfile -l dist/*.whl | grep -E "(FollowWeb_Visualizor/|followweb_visualizor.*dist-info/)" || python ../.github/scripts/ci_helpers.py info "Package structure validation" --print-only
        # Verify no unwanted packages are included
        if python -m zipfile -l dist/*.whl | grep -E "(tests/|analysis_tools/|build/)"; then
          python ../.github/scripts/ci_helpers.py error "Unwanted packages found in wheel"
          exit 1
        fi
    
    - name: Verify package integrity
      run: |
        twine check dist/*
        
    - name: Display package info
      run: |
        ls -la dist/
        python -m zipfile -l dist/*.whl || true
      shell: bash
    
    - name: Test package installation in clean environment
      run: |
        # Create a temporary virtual environment to test package installation
        python -m venv test_env
        source test_env/bin/activate || test_env\\Scripts\\activate
        pip install dist/*.whl
        python -c "import FollowWeb_Visualizor"
        python ../.github/scripts/ci_helpers.py success "Package installs and imports correctly" --print-only
        deactivate
      shell: bash
    
    - name: Upload build artifacts
      uses: actions/upload-artifact@v4
      with:
        name: python-package-distributions
        path: FollowWeb/dist/
        retention-days: 30

  performance:
    name: Performance & Benchmarks
    runs-on: ubuntu-latest
    needs: [test-quick, test]  # Wait for smoke test and full test matrix to pass
    timeout-minutes: 20
    defaults:
      run:
        working-directory: FollowWeb
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python 3.12
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'
        cache: 'pip'
        cache-dependency-path: 'requirements-ci.txt'
    
    - name: Restore prebuild cache
      uses: actions/cache/restore@v4
      with:
        path: |
          ~/.cache/pip
          FollowWeb/.venv
          FollowWeb/tests/test_data
          .pytest_cache
          .mypy_cache
          .ruff_cache
        key: prebuild-${{ runner.os }}-py3.12-${{ hashFiles('FollowWeb/requirements-ci.txt', 'FollowWeb/pyproject.toml') }}
        restore-keys: |
          prebuild-${{ runner.os }}-py3.12-
    
    - name: Install dependencies
      run: |
        # Use cached environment if available
        if [ -d ".venv" ]; then
          echo "Using prebuild cache"
          source .venv/bin/activate
        else
          echo "Installing dependencies fresh"
          python -m pip install --upgrade pip
          python -m pip install -r requirements-ci.txt -e .
        fi
    
    - name: Run performance tests
      run: |
        # Activate venv if it exists
        if [ -d ".venv" ]; then source .venv/bin/activate; fi
        # Create benchmarks directory
        mkdir -p .benchmarks
        # Run slow, performance, and benchmark tests sequentially (no parallelization)
        # These tests are excluded from the main test matrix to prevent timeouts
        python -m pytest -m "slow or performance or benchmark" -v --tb=short -n 0 --benchmark-save=ci_run --benchmark-save-data --benchmark-storage=.benchmarks
        
        # Generate performance test summary with proper emoji handling
        python ../.github/scripts/ci_helpers.py success "Performance tests completed - no regressions detected" --print-only
        
        # Write to GitHub step summary
        python ../.github/scripts/ci_helpers.py info "**Performance Test Results**" --summary-only
        python ../.github/scripts/ci_helpers.py success "Performance tests completed successfully - no regressions detected" --summary-only
      env:
        MPLBACKEND: Agg
        PYTEST_PARALLEL_DISABLE: "1"
      continue-on-error: false
    
    - name: Generate benchmark report
      if: always()
      run: |
        # Generate structured benchmark summary
        python ../.github/scripts/ci_helpers.py benchmark-summary
        
        # Also show detailed pytest-benchmark output
        python ../.github/scripts/ci_helpers.py info "" --summary-only
        python ../.github/scripts/ci_helpers.py info "**Detailed Benchmark Output**" --summary-only
        python ../.github/scripts/ci_helpers.py info "\`\`\`" --summary-only
        
        # Run benchmark comparison if we have previous data
        if [ -f ".benchmarks/ci_run/0001_*.json" ]; then
          # Display current benchmark results with comparison
          python -m pytest -m benchmark --benchmark-only --benchmark-compare=ci_run --benchmark-storage=.benchmarks --tb=no -v 2>&1 | tee -a $GITHUB_STEP_SUMMARY || true
        else
          echo "No previous benchmark data found - this is the baseline run" | tee -a $GITHUB_STEP_SUMMARY
          # Just show the current results
          python -m pytest -m benchmark --benchmark-only --benchmark-storage=.benchmarks --tb=no -v 2>&1 | tee -a $GITHUB_STEP_SUMMARY || true
        fi
        
        python ../.github/scripts/ci_helpers.py info "\`\`\`" --summary-only
      shell: bash
      continue-on-error: true
    
    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: benchmark-results
        path: FollowWeb/.benchmarks/
        retention-days: 30
        if-no-files-found: ignore
  
  # Documentation pipeline - runs after all tests pass
  documentation:
    name: Documentation
    needs: [test-quick, test]
    uses: ./.github/workflows/docs.yml
  
  # All checks must pass - no exceptions
  ci-success:
    name: CI Success
    runs-on: ubuntu-latest
    needs: [test-quick, test, security, format-check, build, performance, documentation]
    if: always()
    
    steps:
    - name: Check all jobs status
      run: |
        # All jobs must succeed - no failures allowed
        # This check runs BEFORE any installs to fail fast if any job failed
        failed_jobs=""
        skipped_jobs=""
        
        # Check each job result - fail on anything other than "success"
        if [[ "${{ needs.test-quick.result }}" != "success" ]]; then
          if [[ "${{ needs.test-quick.result }}" == "skipped" ]]; then
            skipped_jobs="$skipped_jobs QuickTest(${{ needs.test-quick.result }})"
          else
            failed_jobs="$failed_jobs QuickTest(${{ needs.test-quick.result }})"
          fi
        fi
        
        if [[ "${{ needs.test.result }}" != "success" ]]; then
          if [[ "${{ needs.test.result }}" == "skipped" ]]; then
            skipped_jobs="$skipped_jobs FullTests(${{ needs.test.result }})"
          else
            failed_jobs="$failed_jobs FullTests(${{ needs.test.result }})"
          fi
        fi
        
        if [[ "${{ needs.security.result }}" != "success" ]]; then
          if [[ "${{ needs.security.result }}" == "skipped" ]]; then
            skipped_jobs="$skipped_jobs Security(${{ needs.security.result }})"
          else
            failed_jobs="$failed_jobs Security(${{ needs.security.result }})"
          fi
        fi
        
        if [[ "${{ needs.format-check.result }}" != "success" ]]; then
          if [[ "${{ needs.format-check.result }}" == "skipped" ]]; then
            skipped_jobs="$skipped_jobs CodeQuality(${{ needs.format-check.result }})"
          else
            failed_jobs="$failed_jobs CodeQuality(${{ needs.format-check.result }})"
          fi
        fi
        
        if [[ "${{ needs.build.result }}" != "success" ]]; then
          if [[ "${{ needs.build.result }}" == "skipped" ]]; then
            skipped_jobs="$skipped_jobs Build(${{ needs.build.result }})"
          else
            failed_jobs="$failed_jobs Build(${{ needs.build.result }})"
          fi
        fi
        
        if [[ "${{ needs.performance.result }}" != "success" ]]; then
          if [[ "${{ needs.performance.result }}" == "skipped" ]]; then
            skipped_jobs="$skipped_jobs Performance(${{ needs.performance.result }})"
          else
            failed_jobs="$failed_jobs Performance(${{ needs.performance.result }})"
          fi
        fi
        
        if [[ "${{ needs.documentation.result }}" != "success" ]]; then
          if [[ "${{ needs.documentation.result }}" == "skipped" ]]; then
            skipped_jobs="$skipped_jobs Documentation(${{ needs.documentation.result }})"
          else
            failed_jobs="$failed_jobs Documentation(${{ needs.documentation.result }})"
          fi
        fi
        
        # Fail if any jobs failed
        if [[ -n "$failed_jobs" ]]; then
          echo "::error::CI PIPELINE FAILED - Failed Jobs:$failed_jobs"
          echo "**❌ CI Pipeline Failed**" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Failed Jobs:**$failed_jobs" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Job Results:**" >> $GITHUB_STEP_SUMMARY
          echo "- Quick Test: ${{ needs.test-quick.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- Full Tests: ${{ needs.test.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- Security: ${{ needs.security.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- Code Quality: ${{ needs.format-check.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- Build: ${{ needs.build.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- Performance: ${{ needs.performance.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- Documentation: ${{ needs.documentation.result }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Action Required:** Review the failed job logs above for details." >> $GITHUB_STEP_SUMMARY
          if [[ -n "$skipped_jobs" ]]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "**Skipped Jobs:**$skipped_jobs" >> $GITHUB_STEP_SUMMARY
          fi
          exit 1
        fi
        
        # Warn about skipped jobs but don't fail
        if [[ -n "$skipped_jobs" ]]; then
          echo "::warning::Some jobs were skipped:$skipped_jobs"
          echo "**⚠️ CI PIPELINE WARNING**" >> $GITHUB_STEP_SUMMARY
          echo "**Skipped Jobs:**$skipped_jobs" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "::notice::ALL PREREQUISITE JOBS PASSED - PROCEEDING WITH SUCCESS VALIDATION"
    
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
    
    - name: Generate success summary
      run: |
        # Generate success summary
        echo "::notice::ALL CHECKS PASSED!"
        echo "**✅ CI Pipeline Success**" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "All quality checks passed:" >> $GITHUB_STEP_SUMMARY
        echo "- ✅ Quick Test: ${{ needs.test-quick.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- ✅ Full Tests: ${{ needs.test.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- ✅ Security: ${{ needs.security.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- ✅ Code Quality: ${{ needs.format-check.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- ✅ Build: ${{ needs.build.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- ✅ Performance: ${{ needs.performance.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- ✅ Documentation: ${{ needs.documentation.result }}" >> $GITHUB_STEP_SUMMARY