# Freesound Nightly Collection Pipeline
#
# Automated pipeline that runs daily to collect new samples from Freesound API.
# This workflow is part of a four-pipeline architecture:
# 1. Collection Pipeline (this workflow): Collects data only
# 2. Repair Pipeline: Validates and repairs data integrity issues
# 3. Validation Pipeline: Validates data and generates visualizations
# 4. Backup Pipeline: Creates and manages backups
#
# Architecture:
# - Persistent Storage: Private GitHub repository release assets (TOS-compliant)
# - Ephemeral Cache: Workflow cache populated from private repo at start, wiped at end
# - Split Checkpoint: Graph topology (.gpickle) + SQLite metadata (.db) + metadata JSON
# - Pagination-based Collection: Continues from last page across runs
# - Duplicate Detection: Checks metadata cache before API requests
#
# Backup Strategy (Tiered):
# - Frequent backups (every 25 nodes) → v-checkpoint release (14-day retention)
# - Milestone backups (every 100 nodes) → v-permanent release (unlimited retention)
#
# CRITICAL: Backup Failure = Pipeline Failure
# - Primary backup (BACKUP_PAT) is REQUIRED - pipeline will fail fast if backup fails
# - No data collection without successful backup
# - Ensures data integrity and prevents data loss
#
# Features:
# - Scheduled execution at 2 AM UTC Monday-Saturday
# - Manual trigger with configurable parameters
# - Incremental data collection with checkpoint recovery
# - Circuit breaker to limit API requests per run
# - Cache save with failure recovery (runs even on failure)
# - Triggers downstream validation and backup workflows
#
# Workflow Triggers:
# - On success: Triggers validation workflow via workflow_run
# - On completion (success or failure): Triggers backup workflow via workflow_run
# - Cache key (checkpoint-${{ github.run_id }}) is passed to downstream workflows

name: Freesound Nightly Collection

on:
  schedule:
    # Run at 2 AM UTC Monday through Saturday (skip Sunday for validation)
    - cron: '0 2 * * 1-6'
  
  workflow_dispatch:
    # Allow manual triggering with custom parameters
    inputs:
      seed_sample_id:
        description: 'Seed sample ID (leave empty to use most downloaded sample)'
        required: false
        default: ''
        type: string
      max_requests:
        description: 'Maximum API requests (circuit breaker, default: 1950)'
        required: false
        default: '1950'
        type: string
      discovery_mode:
        description: 'Discovery strategy: search, relationships, or mixed'
        required: false
        default: 'search'
        type: choice
        options:
          - search
          - relationships
          - mixed

# Prevent workflow collisions
concurrency:
  group: freesound-pipeline
  cancel-in-progress: false

jobs:
  smoke-test:
    name: Pre-Flight Smoke Test
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'
        cache-dependency-path: 'FollowWeb/requirements.txt'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip setuptools wheel
        pip install -r FollowWeb/requirements.txt
        pip install -e FollowWeb/
    
    - name: Validate environment
      run: |
        {
          echo "##  Pre-Flight Smoke Test"
          echo ""
          echo "###  Package Imports"
        } >> "$GITHUB_STEP_SUMMARY"
        if python -c "import FollowWeb_Visualizor"; then
          echo "-  FollowWeb_Visualizor" >> "$GITHUB_STEP_SUMMARY"
        else
          echo "-  FollowWeb_Visualizor" >> "$GITHUB_STEP_SUMMARY"
          exit 1
        fi
        if python -c "from FollowWeb_Visualizor.data.loaders import IncrementalFreesoundLoader"; then
          echo "-  IncrementalFreesoundLoader" >> "$GITHUB_STEP_SUMMARY"
        else
          echo "-  IncrementalFreesoundLoader" >> "$GITHUB_STEP_SUMMARY"
          exit 1
        fi
        if python -c "from FollowWeb_Visualizor.visualization.renderers import SigmaRenderer"; then
          echo "-  SigmaRenderer" >> "$GITHUB_STEP_SUMMARY"
        else
          echo "-  SigmaRenderer" >> "$GITHUB_STEP_SUMMARY"
          exit 1
        fi
        if python -c "import networkx"; then
          echo "-  NetworkX" >> "$GITHUB_STEP_SUMMARY"
        else
          echo "-  NetworkX" >> "$GITHUB_STEP_SUMMARY"
          exit 1
        fi
        echo "" >> "$GITHUB_STEP_SUMMARY"
    
    - name: Validate secrets
      env:
        FREESOUND_API_KEY: ${{ secrets.FREESOUND_API_KEY }}
        BACKUP_PAT: ${{ secrets.BACKUP_PAT }}
      run: |
        echo "###  Secret Validation" >> "$GITHUB_STEP_SUMMARY"
        
        # Check FREESOUND_API_KEY (required)
        if [ -z "$FREESOUND_API_KEY" ]; then
          echo "-  FREESOUND_API_KEY: ❌ Not configured" >> "$GITHUB_STEP_SUMMARY"
          echo "::error::FREESOUND_API_KEY not configured"
          exit 1
        else
          # Validate API key format (should be 32 hex characters)
          if [[ "$FREESOUND_API_KEY" =~ ^[a-f0-9]{32}$ ]]; then
            echo "-  FREESOUND_API_KEY: ✅ Valid format" >> "$GITHUB_STEP_SUMMARY"
          else
            echo "-  FREESOUND_API_KEY: ⚠️ Configured but format may be invalid" >> "$GITHUB_STEP_SUMMARY"
          fi
        fi
        
        # Check BACKUP_PAT (required)
        if [ -z "$BACKUP_PAT" ]; then
          echo "-  BACKUP_PAT: ❌ Not configured (CRITICAL)" >> "$GITHUB_STEP_SUMMARY"
          echo "::error::BACKUP_PAT not configured - backups are REQUIRED"
          exit 1
        else
          echo "-  BACKUP_PAT: ✅ Configured (primary backup repository)" >> "$GITHUB_STEP_SUMMARY"
        fi
        echo "" >> "$GITHUB_STEP_SUMMARY"
    
    - name: Test loader initialization (no API calls)
      env:
        FREESOUND_API_KEY: ${{ secrets.FREESOUND_API_KEY }}
      run: |
        echo "###  Component Tests" >> "$GITHUB_STEP_SUMMARY"
        
        # Test loader initialization without making API calls
        python -c "
        import os
        import tempfile
        from FollowWeb_Visualizor.data.loaders import IncrementalFreesoundLoader
        
        # Create temporary checkpoint directory
        with tempfile.TemporaryDirectory() as tmpdir:
            # Initialize loader (no API calls)
            # Loader reads FREESOUND_API_KEY from environment variable automatically
            loader = IncrementalFreesoundLoader(
                config={
                    'checkpoint': {
                        'checkpoint_dir': tmpdir,
                        'checkpoint_interval': 1
                    }
                }
            )
            
            # Verify loader attributes
            assert hasattr(loader, 'rate_limiter'), 'Rate limiter not initialized'
            assert hasattr(loader, 'graph'), 'Graph not initialized'
            assert hasattr(loader, 'processed_ids'), 'Processed IDs not initialized'
            
            # Verify initial state
            assert isinstance(loader.processed_ids, set), 'processed_ids should be a set'
            assert len(loader.processed_ids) == 0, 'processed_ids should start empty'
            assert loader.graph.number_of_nodes() == 0, 'Graph should start empty'
            
            print(' Loader initialization successful')
        "
        loader_exit_code=$?
        if [ $loader_exit_code -eq 0 ]; then
          echo "-  IncrementalFreesoundLoader initialization" >> "$GITHUB_STEP_SUMMARY"
        else
          echo "-  IncrementalFreesoundLoader initialization" >> "$GITHUB_STEP_SUMMARY"
          exit 1
        fi
        
        # Test renderer can be imported (initialization tested in actual pipeline)
        python -c "
        from FollowWeb_Visualizor.visualization.renderers import SigmaRenderer
        print(' SigmaRenderer import successful')
        "
        renderer_exit_code=$?
        if [ $renderer_exit_code -eq 0 ]; then
          echo "-  SigmaRenderer import" >> "$GITHUB_STEP_SUMMARY"
        else
          echo "-  SigmaRenderer import" >> "$GITHUB_STEP_SUMMARY"
          exit 1
        fi
        
        echo "" >> "$GITHUB_STEP_SUMMARY"
    
    - name: Test checkpoint operations (no API calls)
      run: |
        echo "###  Checkpoint Tests" >> "$GITHUB_STEP_SUMMARY"
        
        # Test checkpoint module can be imported (actual operations tested in pipeline)
        python -c "
        from FollowWeb_Visualizor.data.checkpoint import GraphCheckpoint
        from pathlib import Path
        
        # Verify checkpoint class can be instantiated
        checkpoint = GraphCheckpoint(Path('test_checkpoint.gpickle'))
        assert hasattr(checkpoint, 'checkpoint_path'), 'Checkpoint path not set'
        assert hasattr(checkpoint, 'save'), 'Save method not available'
        assert hasattr(checkpoint, 'load'), 'Load method not available'
        
        print(' Checkpoint module validated')
        "
        checkpoint_exit_code=$?
        if [ $checkpoint_exit_code -eq 0 ]; then
          echo "-  Checkpoint module validation" >> "$GITHUB_STEP_SUMMARY"
        else
          echo "-  Checkpoint module validation" >> "$GITHUB_STEP_SUMMARY"
          exit 1
        fi
        
        echo "" >> "$GITHUB_STEP_SUMMARY"
    
    - name: Test validation functions
      run: |
        echo "###  Validation Tests" >> "$GITHUB_STEP_SUMMARY"
        
        # Test validation utilities
        python -c "
        from FollowWeb_Visualizor.utils.validation import (
            validate_positive_integer,
            validate_non_negative_integer,
            validate_file_path
        )
        
        # Test positive integer validation
        try:
            validate_positive_integer(5, 'test')
            print(' Positive integer validation works')
        except:
            raise AssertionError('Positive integer validation failed')
        
        # Test that zero is rejected for positive integers
        try:
            validate_positive_integer(0, 'test')
            raise AssertionError('Should have rejected zero')
        except ValueError:
            print(' Zero correctly rejected for positive integers')
        
        # Test non-negative integer validation
        try:
            validate_non_negative_integer(0, 'test')
            print(' Non-negative integer validation works')
        except:
            raise AssertionError('Non-negative integer validation failed')
        
        print(' All validation tests passed')
        "
        validation_exit_code=$?
        if [ $validation_exit_code -eq 0 ]; then
          echo "-  Validation utilities" >> "$GITHUB_STEP_SUMMARY"
        else
          echo "-  Validation utilities" >> "$GITHUB_STEP_SUMMARY"
          exit 1
        fi
        
        echo "" >> "$GITHUB_STEP_SUMMARY"
    
    - name: Smoke test summary
      if: success()
      run: |
        {
          echo "---"
          echo ""
          echo "##  All Pre-Flight Checks Passed"
          echo ""
          echo "The environment is ready for the Freesound Nightly Pipeline."
          echo "Proceeding to data collection phase..."
        } >> "$GITHUB_STEP_SUMMARY"

  freesound-collection:
    name: Freesound Data Collection
    runs-on: ubuntu-latest
    timeout-minutes: 120  # 2-hour timeout
    needs: smoke-test  # Only run if smoke test passes
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for proper Git operations
        token: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Configure Git
      run: |
        git config user.name "GitHub Actions Bot"
        git config user.email "actions@github.com"
        git config --global init.defaultBranch main
        git config --global advice.detachedHead false
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'
        cache-dependency-path: 'FollowWeb/requirements.txt'
    
    - name: Display Python environment info
      run: |
        python --version
        python -c "import sys; print(f'Python executable: {sys.executable}')"
        python -c "import platform; print(f'Platform: {platform.platform()}')"
        pip --version
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip setuptools wheel
        pip install -r FollowWeb/requirements.txt
        pip install -e FollowWeb/
    
    - name: Validate secrets
      env:
        FREESOUND_API_KEY: ${{ secrets.FREESOUND_API_KEY }}
        BACKUP_PAT: ${{ secrets.BACKUP_PAT }}
      run: |
        {
          echo "** Secret Validation**"
          echo ""
        } >> "$GITHUB_STEP_SUMMARY"
        
        # Check FREESOUND_API_KEY (required)
        if [ -z "$FREESOUND_API_KEY" ]; then
          echo "::error::FREESOUND_API_KEY not configured"
          echo "-  FREESOUND_API_KEY: Not configured" >> "$GITHUB_STEP_SUMMARY"
          exit 1
        fi
        API_KEY_STATUS="-  FREESOUND_API_KEY: Configured"
        
        # Check BACKUP_PAT (required)
        if [ -z "$BACKUP_PAT" ]; then
          echo "::error::BACKUP_PAT not configured - backups are REQUIRED"
          echo "-  BACKUP_PAT: ❌ Not configured (CRITICAL)" >> "$GITHUB_STEP_SUMMARY"
          exit 1
        fi
        BACKUP_PAT_STATUS="-  BACKUP_PAT: ✅ Configured (primary backup repository)"
        
        {
          echo "$API_KEY_STATUS"
          echo "$BACKUP_PAT_STATUS"
          echo ""
        } >> "$GITHUB_STEP_SUMMARY"
    
    - name: Verify installation
      run: |
        python -c "import FollowWeb_Visualizor"
        echo " FollowWeb package imported successfully"
        python -c "from FollowWeb_Visualizor.data.loaders import IncrementalFreesoundLoader"
        echo " IncrementalFreesoundLoader available"
        python -c "from FollowWeb_Visualizor.visualization.renderers import SigmaRenderer"
        echo " SigmaRenderer available"
    
    - name: Set pipeline parameters
      id: params
      run: |
        # Use workflow_dispatch inputs if available, otherwise use defaults
        if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
          {
            echo "seed_sample_id=${{ github.event.inputs.seed_sample_id }}"
            echo "max_requests=${{ github.event.inputs.max_requests }}"
            echo "discovery_mode=${{ github.event.inputs.discovery_mode }}"
          } >> "$GITHUB_OUTPUT"
        else
          {
            echo "seed_sample_id="
            echo "max_requests=1950"
            echo "discovery_mode=search"
          } >> "$GITHUB_OUTPUT"
        fi
        
        # Generate execution ID
        echo "execution_id=$(date +'%Y%m%d_%H%M%S')" >> "$GITHUB_OUTPUT"
    
    - name: Download checkpoint from private repository
      id: download_checkpoint
      env:
        BACKUP_PAT: ${{ secrets.BACKUP_PAT }}
      run: |
        {
          echo "** Phase 1: Download Checkpoint from Private Repository**"
          echo ""
        } >> "$GITHUB_STEP_SUMMARY"
        
        # Check if BACKUP_PAT is configured (required)
        if [ -z "$BACKUP_PAT" ]; then
          echo "::error::BACKUP_PAT not configured - cannot download checkpoint"
          {
            echo "- Status: ❌ BACKUP_PAT required"
          } >> "$GITHUB_STEP_SUMMARY"
          echo "checkpoint_restored=false" >> "$GITHUB_OUTPUT"
          exit 1
        fi
        
        # Get repository owner and name from GITHUB_REPOSITORY
        REPO_OWNER="${{ github.repository_owner }}"
        BACKUP_REPO="${REPO_OWNER}/freesound-backup"
        
        # List all assets from v-checkpoint release
        echo "Fetching assets from ${BACKUP_REPO} release v-checkpoint..."
        ASSETS_JSON=$(curl -s -H "Authorization: token $BACKUP_PAT" \
          "https://api.github.com/repos/${BACKUP_REPO}/releases/tags/v-checkpoint")
        
        # Check if release exists
        if echo "$ASSETS_JSON" | grep -q "Not Found"; then
          echo " Release v-checkpoint not found in ${BACKUP_REPO}"
          {
            echo "- Status: No backup available"
          } >> "$GITHUB_STEP_SUMMARY"
          echo "checkpoint_restored=false" >> "$GITHUB_OUTPUT"
          exit 0
        fi
        
        # Find most recent checkpoint backup
        LATEST_ASSET=$(echo "$ASSETS_JSON" | jq -r '.assets | sort_by(.created_at) | reverse | .[0]')
        
        if [ "$LATEST_ASSET" = "null" ] || [ -z "$LATEST_ASSET" ]; then
          echo " No checkpoint backups found in ${BACKUP_REPO}"
          {
            echo "- Status: No backup available"
          } >> "$GITHUB_STEP_SUMMARY"
          echo "checkpoint_restored=false" >> "$GITHUB_OUTPUT"
          exit 0
        fi
        
        # Extract asset details
        ASSET_NAME=$(echo "$LATEST_ASSET" | jq -r '.name')
        ASSET_URL=$(echo "$LATEST_ASSET" | jq -r '.url')
        ASSET_SIZE=$(echo "$LATEST_ASSET" | jq -r '.size')
        ASSET_CREATED=$(echo "$LATEST_ASSET" | jq -r '.created_at')
        
        echo " Found latest backup: $ASSET_NAME"
        echo "- Size: $(numfmt --to=iec-i --suffix=B $ASSET_SIZE)"
        echo "- Created: $ASSET_CREATED"
        
        # Download asset
        echo "Downloading checkpoint backup..."
        curl -L -H "Authorization: token $BACKUP_PAT" \
          -H "Accept: application/octet-stream" \
          "$ASSET_URL" -o checkpoint_backup.tar.gz
        
        # Extract to data/freesound_library/
        echo "Extracting checkpoint to data/freesound_library/..."
        mkdir -p data/freesound_library
        tar -xzf checkpoint_backup.tar.gz -C data/
        
        # Verify extraction
        if [ -f "data/freesound_library/graph_topology.gpickle" ]; then
          echo " Checkpoint restored successfully"
          {
            echo "- Backup: \`$ASSET_NAME\`"
            echo "- Size: \`$(numfmt --to=iec-i --suffix=B $ASSET_SIZE)\`"
            echo "- Created: \`$ASSET_CREATED\`"
          } >> "$GITHUB_STEP_SUMMARY"
          echo "checkpoint_restored=true" >> "$GITHUB_OUTPUT"
        else
          echo " Checkpoint extraction failed"
          echo "- Status: Extraction failed" >> "$GITHUB_STEP_SUMMARY"
          echo "checkpoint_restored=false" >> "$GITHUB_OUTPUT"
        fi
        
        # Cleanup
        rm -f checkpoint_backup.tar.gz
    
    - name: Check for workflow conflicts
      id: orchestration
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        PYTHONPATH: ${{ github.workspace }}
      run: |
        python -c "
        import sys
        sys.path.insert(0, '${{ github.workspace }}')
        from scripts.analysis.workflow_orchestrator import WorkflowOrchestrator
        import os
        
        orchestrator = WorkflowOrchestrator(
            github_token=os.environ['GITHUB_TOKEN'],
            repository='${{ github.repository }}'
        )
        
        can_proceed, reason = orchestrator.check_and_wait_for_conflicts(
            current_workflow='freesound-nightly-pipeline',
            timeout=7200
        )
        
        with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
            f.write(f'can_proceed={str(can_proceed).lower()}\n')
            f.write(f'skip_reason={reason}\n')
        
        if not can_proceed:
            print(f'⚠️ SKIPPING EXECUTION: {reason}')
            with open(os.environ.get('GITHUB_STEP_SUMMARY', '/dev/null'), 'a') as f:
                f.write(f'## ⚠️ Execution Skipped\n\n')
                f.write(f'**Reason:** {reason}\n\n')
                f.write(f'The workflow will retry on the next scheduled run.\n')
        "

    
    - name: Display pipeline configuration
      if: steps.orchestration.outputs.can_proceed == 'true'
      run: |
        {
          echo ""
          echo "** Phase 2: Pipeline Configuration**"
          echo ""
        } >> "$GITHUB_STEP_SUMMARY"
        
        if [ -n "${{ steps.params.outputs.seed_sample_id }}" ]; then
          SEED_INFO="- Seed Sample ID: \`${{ steps.params.outputs.seed_sample_id }}\`"
        else
          SEED_INFO="- Seed Sample: \`Most downloaded sample (auto-detected)\`"
        fi
        
        {
          echo "$SEED_INFO"
          echo "- Max Requests: \`${{ steps.params.outputs.max_requests }}\` (circuit breaker)"
          echo "- Discovery Mode: \`${{ steps.params.outputs.discovery_mode }}\`"
          echo "- Execution ID: \`${{ steps.params.outputs.execution_id }}\`"
          echo "- Trigger: \`${{ github.event_name }}\`"
          echo ""
        } >> "$GITHUB_STEP_SUMMARY"
    
    - name: Run data collection
      if: steps.orchestration.outputs.can_proceed == 'true'
      id: collection
      env:
        FREESOUND_API_KEY: ${{ secrets.FREESOUND_API_KEY }}
      run: |
        {
          echo "** Phase 3: Data Collection**"
          echo ""
        } >> "$GITHUB_STEP_SUMMARY"
        
        # Build command with optional seed sample ID
        cmd="python generate_freesound_visualization.py"
        if [ -n "${{ steps.params.outputs.seed_sample_id }}" ]; then
          cmd="$cmd --seed-sample-id ${{ steps.params.outputs.seed_sample_id }}"
        fi
        cmd="$cmd --max-requests ${{ steps.params.outputs.max_requests }}"
        cmd="$cmd --discovery-mode ${{ steps.params.outputs.discovery_mode }}"
        cmd="$cmd --skip-visualization"  # Skip visualization generation
        
        # Run the collection script
        eval "$cmd" 2>&1 | tee "collection_${{ steps.params.outputs.execution_id }}.log"
        
        # Capture exit code
        collection_exit_code=${PIPESTATUS[0]}
        
        if [ "$collection_exit_code" -eq 0 ]; then
          echo " Data collection completed successfully"
          echo "collection_status=success" >> "$GITHUB_OUTPUT"
        else
          echo " Collection failed with exit code: $collection_exit_code"
          echo "collection_status=failed" >> "$GITHUB_OUTPUT"
          exit $collection_exit_code
        fi
    
    - name: Extract collection statistics
      if: steps.orchestration.outputs.can_proceed == 'true'
      id: stats
      run: |
        # Extract statistics from checkpoint metadata
        if [ -f "data/freesound_library/checkpoint_metadata.json" ]; then
          total_nodes=$(jq -r '.total_nodes // 0' data/freesound_library/checkpoint_metadata.json)
          total_edges=$(jq -r '.total_edges // 0' data/freesound_library/checkpoint_metadata.json)
          current_page=$(jq -r '.pagination.current_page // 1' data/freesound_library/checkpoint_metadata.json)
          search_query=$(jq -r '.pagination.search_query // "N/A"' data/freesound_library/checkpoint_metadata.json)
          
          # Extract collection stats if available
          if jq -e '.collection_stats' data/freesound_library/checkpoint_metadata.json > /dev/null 2>&1; then
            api_requests=$(jq -r '.collection_stats.total_api_requests // 0' data/freesound_library/checkpoint_metadata.json)
            duplicates_skipped=$(jq -r '.collection_stats.duplicates_skipped // 0' data/freesound_library/checkpoint_metadata.json)
            new_samples=$(jq -r '.collection_stats.new_samples_added // 0' data/freesound_library/checkpoint_metadata.json)
          else
            api_requests="N/A"
            duplicates_skipped="N/A"
            new_samples="N/A"
          fi
        else
          total_nodes="0"
          total_edges="0"
          current_page="1"
          search_query="N/A"
          api_requests="N/A"
          duplicates_skipped="N/A"
          new_samples="N/A"
        fi
        
        {
          echo "total_nodes=$total_nodes"
          echo "total_edges=$total_edges"
          echo "current_page=$current_page"
          echo "search_query=$search_query"
          echo "api_requests=$api_requests"
          echo "duplicates_skipped=$duplicates_skipped"
          echo "new_samples=$new_samples"
        } >> "$GITHUB_OUTPUT"
        
        echo " Statistics extracted from checkpoint"
    
    - name: Save checkpoint to cache (failure recovery)
      if: always() && steps.orchestration.outputs.can_proceed == 'true'
      uses: actions/cache/save@v3
      with:
        path: data/freesound_library
        key: checkpoint-${{ github.run_id }}
    
    - name: Upload checkpoint to private repository
      if: steps.orchestration.outputs.can_proceed == 'true' && always()
      id: upload_checkpoint
      env:
        BACKUP_PAT: ${{ secrets.BACKUP_PAT }}
      run: |
        {
          echo ""
          echo "** Phase 4: Upload Checkpoint to Private Repository**"
          echo ""
        } >> "$GITHUB_STEP_SUMMARY"
        
        # Check if checkpoint directory exists
        if [ ! -d "data/freesound_library" ]; then
          echo "::error::Checkpoint directory not found - BACKUP FAILED"
          {
            echo "- Status: ❌ No checkpoint to backup (CRITICAL)"
            echo "- **CRITICAL**: Pipeline must create checkpoint before backup"
          } >> "$GITHUB_STEP_SUMMARY"
          exit 1
        fi
        
        # Check if BACKUP_PAT is configured (required)
        if [ -z "$BACKUP_PAT" ]; then
          echo "::error::BACKUP_PAT not configured - BACKUP FAILED"
          {
            echo "- Status: ❌ PAT not configured (CRITICAL)"
            echo "- **CRITICAL**: Backups are required - pipeline cannot continue"
          } >> "$GITHUB_STEP_SUMMARY"
          exit 1
        fi
        
        # Determine backup tier based on node count
        if [ -f "data/freesound_library/checkpoint_metadata.json" ]; then
          node_count=$(jq -r '.total_nodes // 0' data/freesound_library/checkpoint_metadata.json)
        else
          echo "::warning::Checkpoint metadata not found - data may be incomplete"
          {
            echo "- Status: ⚠️ Metadata missing (data integrity issue)"
            echo "- Action: Creating backup anyway, but remediation recommended"
          } >> "$GITHUB_STEP_SUMMARY"
          # Try to get node count from graph file
          if [ -f "data/freesound_library/graph_topology.gpickle" ]; then
            node_count=$(python3 -c "import pickle; g = pickle.load(open('data/freesound_library/graph_topology.gpickle', 'rb')); print(g.number_of_nodes())" 2>/dev/null || echo "0")
            echo "- Estimated nodes from graph: $node_count" >> "$GITHUB_STEP_SUMMARY"
          else
            node_count=0
            echo "- No graph file found either" >> "$GITHUB_STEP_SUMMARY"
          fi
        fi
        
        # Check if this is a failure recovery backup
        collection_status="${{ steps.collection.outputs.collection_status }}"
        if [ "$collection_status" != "success" ]; then
          echo " Pipeline failed - creating recovery backup"
          echo "- Status: Recovery backup (pipeline failed)" >> "$GITHUB_STEP_SUMMARY"
          # Force backup creation regardless of interval
          tier="recovery"
          release_tag="v-checkpoint"
        else
          echo "Current node count: $node_count"
          
          # Determine tier and release tag using Python
          tier_info=$(python3 -c "import sys; node_count = ${node_count}; backup_interval = 25; tier = 'milestone' if node_count % (backup_interval * 20) == 0 else ('moderate' if node_count % (backup_interval * 4) == 0 else ('frequent' if node_count % backup_interval == 0 else 'none')); release_tag = 'v-permanent' if tier in ['milestone', 'moderate'] else ('v-checkpoint' if tier == 'frequent' else 'none'); print(f'{tier}|{release_tag}')")
          
          tier=$(echo "$tier_info" | cut -d'|' -f1)
          release_tag=$(echo "$tier_info" | cut -d'|' -f2)
          
          # Check if backup should be created
          if [ "$tier" = "none" ]; then
            echo " Node count ($node_count) not at backup interval, skipping backup"
            echo "- Status: Backup skipped (not at interval)" >> "$GITHUB_STEP_SUMMARY"
            exit 0
          fi
        fi
        
        echo "Backup tier: $tier"
        echo "Release tag: $release_tag"
        
        # Create tar.gz archive with tier information in filename
        backup_filename="checkpoint_backup_${node_count}nodes_${tier}_${{ github.run_id }}.tar.gz"
        echo "Creating backup archive: $backup_filename"
        tar -czf "$backup_filename" -C data freesound_library/
        
        # Get file size
        backup_size=$(stat -f%z "$backup_filename" 2>/dev/null || stat -c%s "$backup_filename")
        echo "Backup size: $(numfmt --to=iec-i --suffix=B $backup_size)"
        
        # Get repository details
        REPO_OWNER="${{ github.repository_owner }}"
        BACKUP_REPO="${REPO_OWNER}/freesound-backup"
        
        # Get release ID for the appropriate tag
        RELEASE_JSON=$(curl -s -H "Authorization: token $BACKUP_PAT" \
          "https://api.github.com/repos/${BACKUP_REPO}/releases/tags/${release_tag}")
        
        RELEASE_ID=$(echo "$RELEASE_JSON" | jq -r '.id')
        
        if [ "$RELEASE_ID" = "null" ] || [ -z "$RELEASE_ID" ]; then
          echo "::error::Release ${release_tag} not found in ${BACKUP_REPO} - BACKUP FAILED"
          {
            echo "- Status: ❌ Upload failed (release not found)"
            echo "- Tier: \`$tier\`"
            echo "- Release Tag: \`$release_tag\`"
            echo "- **CRITICAL**: Create release '${release_tag}' in ${BACKUP_REPO} to enable backups"
          } >> "$GITHUB_STEP_SUMMARY"
          rm -f "$backup_filename"
          exit 1
        fi
        
        # Upload asset
        echo "Uploading backup to ${BACKUP_REPO} (release: ${release_tag})..."
        UPLOAD_URL="https://uploads.github.com/repos/${BACKUP_REPO}/releases/${RELEASE_ID}/assets?name=${backup_filename}"
        
        UPLOAD_RESPONSE=$(curl -s -X POST \
          -H "Authorization: token $BACKUP_PAT" \
          -H "Content-Type: application/gzip" \
          --data-binary "@${backup_filename}" \
          "$UPLOAD_URL")
        
        ASSET_ID=$(echo "$UPLOAD_RESPONSE" | jq -r '.id')
        
        if [ "$ASSET_ID" = "null" ] || [ -z "$ASSET_ID" ]; then
          echo "::error::Failed to upload backup to ${BACKUP_REPO} - BACKUP FAILED"
          {
            echo "- Status: ❌ Upload failed"
            echo "- Error: API upload failed (check PAT permissions)"
            echo "- **CRITICAL**: Backup is required - pipeline cannot continue without successful backup"
          } >> "$GITHUB_STEP_SUMMARY"
          rm -f "$backup_filename"
          exit 1
        fi
        
        echo " Backup uploaded successfully"
        {
          echo "- Backup: \`$backup_filename\`"
          echo "- Tier: \`$tier\`"
          echo "- Release Tag: \`$release_tag\`"
          echo "- Size: \`$(numfmt --to=iec-i --suffix=B $backup_size)\`"
          echo "- Node Count: \`$node_count\`"
          echo "- Run ID: \`${{ github.run_id }}\`"
        } >> "$GITHUB_STEP_SUMMARY"
        
        # Store tier info for retention cleanup
        {
          echo "backup_tier=$tier"
          echo "release_tag=$release_tag"
          echo "backup_uploaded=true"
        } >> "$GITHUB_OUTPUT"
        
        # Cleanup local backup file
        rm -f "$backup_filename"
    
    - name: Verify backup integrity
      if: steps.upload_checkpoint.outputs.backup_uploaded == 'true'
      env:
        BACKUP_PAT: ${{ secrets.BACKUP_PAT }}
      run: |
        {
          echo ""
          echo "** Phase 4a: Verify Backup Integrity**"
          echo ""
        } >> "$GITHUB_STEP_SUMMARY"
        
        REPO_OWNER="${{ github.repository_owner }}"
        BACKUP_REPO="${REPO_OWNER}/freesound-backup"
        release_tag="${{ steps.upload_checkpoint.outputs.release_tag }}"
        
        # Get the most recent asset from the release
        RELEASE_JSON=$(curl -s -H "Authorization: token $BACKUP_PAT" \
          "https://api.github.com/repos/${BACKUP_REPO}/releases/tags/${release_tag}")
        
        LATEST_ASSET=$(echo "$RELEASE_JSON" | jq -r '.assets | sort_by(.created_at) | reverse | .[0]')
        
        if [ "$LATEST_ASSET" = "null" ] || [ -z "$LATEST_ASSET" ]; then
          echo " Could not verify backup - asset not found"
          echo "- Status: Verification skipped" >> "$GITHUB_STEP_SUMMARY"
          exit 0
        fi
        
        ASSET_NAME=$(echo "$LATEST_ASSET" | jq -r '.name')
        ASSET_SIZE=$(echo "$LATEST_ASSET" | jq -r '.size')
        ASSET_STATE=$(echo "$LATEST_ASSET" | jq -r '.state')
        
        echo "Verifying backup: $ASSET_NAME"
        echo "- Size: $(numfmt --to=iec-i --suffix=B "$ASSET_SIZE")"
        echo "- State: $ASSET_STATE"
        
        # Verify asset state is 'uploaded'
        if [ "$ASSET_STATE" != "uploaded" ]; then
          echo " Backup verification failed - asset state: $ASSET_STATE"
          {
            echo "- Status:  Verification failed"
            echo "- Asset State: \`$ASSET_STATE\`"
          } >> "$GITHUB_STEP_SUMMARY"
          exit 1
        fi
        
        # Verify asset size is reasonable (> 1KB)
        if [ "$ASSET_SIZE" -lt 1024 ]; then
          echo " Backup verification failed - asset too small: $ASSET_SIZE bytes"
          {
            echo "- Status:  Verification failed"
            echo "- Asset Size: \`$ASSET_SIZE bytes\` (too small)"
          } >> "$GITHUB_STEP_SUMMARY"
          exit 1
        fi
        
        echo " Backup verification passed"
        {
          echo "- Status:  Verified"
          echo "- Asset: \`$ASSET_NAME\`"
          echo "- Size: \`$(numfmt --to=iec-i --suffix=B $ASSET_SIZE)\`"
          echo "- State: \`$ASSET_STATE\`"
        } >> "$GITHUB_STEP_SUMMARY"

    
    - name: Upload checkpoint files as workflow artifacts
      if: always() && steps.orchestration.outputs.can_proceed == 'true'
      uses: actions/upload-artifact@v4
      with:
        name: checkpoint-backup-${{ steps.params.outputs.execution_id }}
        path: |
          data/freesound_library/*.gpickle
          data/freesound_library/*.db
          data/freesound_library/*.json
        retention-days: 7
        if-no-files-found: ignore
    
    - name: Cleanup old backups (retention policy)
      if: steps.orchestration.outputs.can_proceed == 'true' && steps.collection.outputs.collection_status == 'success'
      env:
        BACKUP_PAT: ${{ secrets.BACKUP_PAT }}
      run: |
        {
          echo ""
          echo "** Phase 5: Backup Retention Policy**"
          echo ""
        } >> "$GITHUB_STEP_SUMMARY"
        
        # Check if BACKUP_PAT is configured
        if [ -z "$BACKUP_PAT" ]; then
          echo " BACKUP_PAT not configured, skipping retention cleanup"
          exit 0
        fi
        
        REPO_OWNER="${{ github.repository_owner }}"
        BACKUP_REPO="${REPO_OWNER}/freesound-backup"
        
        # Process v-checkpoint release (frequent tier with 14-day retention)
        {
          echo "### v-checkpoint Release (Frequent Tier)"
          echo ""
        } >> "$GITHUB_STEP_SUMMARY"
        
        CHECKPOINT_ASSETS_JSON=$(curl -s -H "Authorization: token $BACKUP_PAT" \
          "https://api.github.com/repos/${BACKUP_REPO}/releases/tags/v-checkpoint")
        
        CHECKPOINT_TOTAL=$(echo "$CHECKPOINT_ASSETS_JSON" | jq '.assets | length')
        echo "Total v-checkpoint backups: $CHECKPOINT_TOTAL"
        
        # Apply 14-day retention policy for frequent backups
        CUTOFF_DATE=$(date -u -d '14 days ago' +%Y-%m-%dT%H:%M:%SZ 2>/dev/null || date -u -v-14d +%Y-%m-%dT%H:%M:%S 2>/dev/null || echo "")
        
        if [ -n "$CUTOFF_DATE" ]; then
          echo "Cutoff date for frequent backups: $CUTOFF_DATE"
          
          # Get assets older than 14 days
          CHECKPOINT_TO_DELETE=$(echo "$CHECKPOINT_ASSETS_JSON" | jq -r ".assets | map(select(.created_at < \"$CUTOFF_DATE\")) | .[].id")
          
          CHECKPOINT_DELETED=0
          for ASSET_ID in $CHECKPOINT_TO_DELETE; do
            echo "Deleting old v-checkpoint asset ID: $ASSET_ID"
            if curl -s -X DELETE \
              -H "Authorization: token $BACKUP_PAT" \
              "https://api.github.com/repos/${BACKUP_REPO}/releases/assets/${ASSET_ID}"; then
              CHECKPOINT_DELETED=$((CHECKPOINT_DELETED + 1))
            else
              echo " Failed to delete asset $ASSET_ID"
            fi
          done
          
          CHECKPOINT_RETAINED=$((CHECKPOINT_TOTAL - CHECKPOINT_DELETED))
          {
            echo "- Retention: \`14 days\`"
            echo "- Backups retained: \`$CHECKPOINT_RETAINED\`"
            echo "- Backups deleted: \`$CHECKPOINT_DELETED\`"
          } >> "$GITHUB_STEP_SUMMARY"
        else
          echo " Could not calculate cutoff date, skipping time-based cleanup"
          echo "- Status: Cleanup skipped (date calculation failed)" >> "$GITHUB_STEP_SUMMARY"
        fi
        
        # Also enforce max count limit (keep last 10 backups)
        MAX_CHECKPOINT_BACKUPS=10
        if [ "$CHECKPOINT_TOTAL" -gt "$MAX_CHECKPOINT_BACKUPS" ]; then
          TO_DELETE=$((CHECKPOINT_TOTAL - MAX_CHECKPOINT_BACKUPS))
          echo "Enforcing max count limit: deleting $TO_DELETE oldest backups..."
          
          OLDEST_ASSETS=$(echo "$CHECKPOINT_ASSETS_JSON" | jq -r ".assets | sort_by(.created_at) | .[:$TO_DELETE] | .[].id")
          
          for ASSET_ID in $OLDEST_ASSETS; do
            echo "Deleting old v-checkpoint asset ID: $ASSET_ID"
            curl -s -X DELETE \
              -H "Authorization: token $BACKUP_PAT" \
              "https://api.github.com/repos/${BACKUP_REPO}/releases/assets/${ASSET_ID}"
          done
        fi
        
        echo "" >> "$GITHUB_STEP_SUMMARY"
        
        # Process v-permanent release (moderate and milestone tiers - no deletion)
        {
          echo "### v-permanent Release (Moderate & Milestone Tiers)"
          echo ""
        } >> "$GITHUB_STEP_SUMMARY"
        
        PERMANENT_ASSETS_JSON=$(curl -s -H "Authorization: token $BACKUP_PAT" \
          "https://api.github.com/repos/${BACKUP_REPO}/releases/tags/v-permanent")
        
        PERMANENT_TOTAL=$(echo "$PERMANENT_ASSETS_JSON" | jq '.assets | length')
        echo "Total v-permanent backups: $PERMANENT_TOTAL"
        
        # No deletion for permanent backups
        {
          echo "- Retention: \`Permanent (no expiration)\`"
          echo "- Backups retained: \`$PERMANENT_TOTAL\`"
          echo "- Backups deleted: \`0\` (permanent tier never deleted)"
          echo ""
        } >> "$GITHUB_STEP_SUMMARY"
        
        echo " Tier-specific retention cleanup complete"
    

    
    - name: Cleanup ephemeral cache
      if: always()
      run: |
        {
          echo ""
          echo "** Phase 7: Cleanup Ephemeral Cache**"
          echo ""
        } >> "$GITHUB_STEP_SUMMARY"
        
        # Delete checkpoint directory
        if [ -d "data/freesound_library" ]; then
          rm -rf data/freesound_library
          echo " Ephemeral cache wiped"
          echo "- Status: Cache deleted" >> "$GITHUB_STEP_SUMMARY"
        else
          echo " No cache to clean"
          echo "- Status: No cache found" >> "$GITHUB_STEP_SUMMARY"
        fi
    
    - name: Generate execution summary
      if: always()
      run: |
        {
          echo ""
          echo "---"
          echo ""
          echo "##  Execution Summary"
          echo ""
        } >> "$GITHUB_STEP_SUMMARY"
        
        # Check if execution was skipped due to conflicts
        if [ "${{ steps.orchestration.outputs.can_proceed }}" = "false" ]; then
          {
            echo "###  Status: Skipped"
            echo ""
            echo "**Reason:** ${{ steps.orchestration.outputs.skip_reason }}"
            echo ""
            echo "The workflow detected a conflicting workflow and waited for 2 hours, but it did not complete in time."
            echo "The workflow will retry on the next scheduled run."
            echo ""
          } >> "$GITHUB_STEP_SUMMARY"
        elif [ "${{ steps.collection.outputs.collection_status }}" = "success" ]; then
          {
            echo "###  Status: Collection Complete"
            echo ""
            
            # Data collection statistics
            echo "###  Collection Statistics"
            echo ""
            echo "| Metric | Value |"
            echo "|--------|-------|"
            echo "| **Total Nodes** | \`${{ steps.stats.outputs.total_nodes }}\` |"
            echo "| **Total Edges** | \`${{ steps.stats.outputs.total_edges }}\` |"
          } >> "$GITHUB_STEP_SUMMARY"
          if [ "${{ steps.stats.outputs.new_samples }}" != "N/A" ]; then
            echo "| **New Samples** | \`${{ steps.stats.outputs.new_samples }}\` |" >> "$GITHUB_STEP_SUMMARY"
          fi
          if [ "${{ steps.stats.outputs.duplicates_skipped }}" != "N/A" ]; then
            echo "| **Duplicates Skipped** | \`${{ steps.stats.outputs.duplicates_skipped }}\` |" >> "$GITHUB_STEP_SUMMARY"
          fi
          if [ "${{ steps.stats.outputs.api_requests }}" != "N/A" ]; then
            echo "| **API Requests Used** | \`${{ steps.stats.outputs.api_requests }}\` |" >> "$GITHUB_STEP_SUMMARY"
          fi
          {
            echo ""
            
            # Pagination state
            echo "###  Pagination State"
            echo ""
            echo "| Property | Value |"
            echo "|----------|-------|"
            echo "| **Current Page** | \`${{ steps.stats.outputs.current_page }}\` |"
            echo "| **Search Query** | \`${{ steps.stats.outputs.search_query }}\` |"
            echo ""
          } >> "$GITHUB_STEP_SUMMARY"
          
          # Backup tier information
          if [ "${{ steps.upload_checkpoint.outputs.backup_tier }}" != "" ]; then
            {
              echo "###  Backup Information"
              echo ""
              echo "| Property | Value |"
              echo "|----------|-------|"
              echo "| **Backup Tier** | \`${{ steps.upload_checkpoint.outputs.backup_tier }}\` |"
              echo "| **Release Tag** | \`${{ steps.upload_checkpoint.outputs.release_tag }}\` |"
              echo ""
            } >> "$GITHUB_STEP_SUMMARY"
          fi
          
          # Downstream workflows
          {
            echo "###  Downstream Workflows"
            echo ""
            echo "- ✅ **Repair Pipeline**: Will be triggered automatically to validate and repair data"
            echo "- ✅ **Validation Pipeline**: Will be triggered automatically after repair"
            echo "- ✅ **Backup Pipeline**: Will be triggered automatically after validation"
            echo ""
          } >> "$GITHUB_STEP_SUMMARY"
          
        else
          {
            echo "###  Status: Collection Failed"
            echo ""
            echo "The data collection failed. Please check the logs for error details."
            echo ""
            echo "**Common issues:**"
            echo "- API key not configured or invalid"
            echo "- API rate limit exceeded"
            echo "- Network connectivity issues"
            echo "- Checkpoint file corruption"
            echo "- BACKUP_PAT not configured or invalid"
            echo ""
            echo "**Note:** Checkpoint has been saved to cache for failure recovery."
            echo "The backup workflow will create a recovery backup."
            echo ""
          } >> "$GITHUB_STEP_SUMMARY"
        fi
        
        # Execution details (always shown)
        {
          echo "###  Execution Details"
          echo ""
          echo "| Detail | Value |"
          echo "|--------|-------|"
          echo "| **Execution ID** | \`${{ steps.params.outputs.execution_id }}\` |"
          echo "| **Trigger** | \`${{ github.event_name }}\` |"
          echo "| **Workflow Run** | [#${{ github.run_number }}](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}) |"
          echo "| **Cache Key** | \`checkpoint-${{ github.run_id }}\` |"
          echo "| **Commit** | [\`${GITHUB_SHA:0:7}\`](https://github.com/${{ github.repository }}/commit/${{ github.sha }}) |"
          echo ""
          
          # Artifacts
          echo "###  Artifacts"
          echo ""
          echo "- Collection logs are available as workflow artifacts (30-day retention)"
          echo "- Checkpoint saved to GitHub Actions cache (7-day retention)"
          echo "- Download from the [workflow run page](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})"
        } >> "$GITHUB_STEP_SUMMARY"
    
    - name: Upload collection logs
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: collection-logs-${{ steps.params.outputs.execution_id }}
        path: |
          collection_*.log
          freesound_viz_*.log
          fetch_freesound_*.log
        retention-days: 30
        if-no-files-found: ignore
