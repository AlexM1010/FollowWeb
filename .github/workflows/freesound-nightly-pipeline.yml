# Freesound Nightly Collection Pipeline
#
# Automated pipeline that runs daily to collect new samples from Freesound API.
# This workflow is part of a four-pipeline architecture:
# 1. Collection Pipeline (this workflow): Collects data and saves to cache
# 2. Backup Pipeline: Creates and manages backups from cache
# 3. Repair Pipeline: Validates and repairs data integrity issues
# 4. Validation Pipeline: Validates data and generates visualizations
#
# Architecture:
# - Ephemeral Cache: Workflow cache populated from backup repo at start, wiped at end
# - Split Checkpoint: Graph topology (.gpickle) + SQLite metadata (.db) + metadata JSON
# - Pagination-based Collection: Continues from last page across runs
# - Duplicate Detection: Checks metadata cache before API requests
#
# Simplified Backup Strategy:
# - Collection workflow ONLY saves checkpoint to cache
# - Dedicated backup workflow handles ALL backup operations:
#   - Tier determination (frequent, moderate, milestone)
#   - Upload to permanent storage
#   - Retention policy enforcement
#
# Features:
# - Scheduled execution at 2 AM UTC Monday-Saturday
# - Manual trigger with configurable parameters
# - Incremental data collection with checkpoint recovery
# - Circuit breaker to limit API requests per run
# - Cache save with failure recovery (runs even on failure)
# - Triggers downstream backup workflow
#
# Workflow Triggers:
# - On completion (success or failure): Triggers backup workflow via workflow_run
# - Cache key (checkpoint-${{ github.run_id }}) is passed to downstream workflows

name: Freesound Nightly Collection

on:
  schedule:
    # Run at 2 AM UTC Monday through Saturday (skip Sunday for validation)
    - cron: '0 2 * * 1-6'
  
  workflow_dispatch:
    # Allow manual triggering with custom parameters
    inputs:
      seed_sample_id:
        description: 'Seed sample ID (leave empty to use most downloaded sample)'
        required: false
        default: ''
        type: string
      max_requests:
        description: 'Maximum API requests (circuit breaker, default: 10 for testing, 100 for production)'
        required: false
        default: '100'
        type: string
      discovery_mode:
        description: 'Discovery strategy: search, relationships, or mixed'
        required: false
        default: 'search'
        type: choice
        options:
          - search
          - relationships
          - mixed

# Prevent workflow collisions
concurrency:
  group: freesound-pipeline
  cancel-in-progress: false

jobs:
  smoke-test:
    name: Pre-Flight Smoke Test
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'
        cache-dependency-path: 'FollowWeb/requirements.txt'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip setuptools wheel
        pip install -r FollowWeb/requirements.txt
        pip install -e FollowWeb/
    
    - name: Validate environment
      run: |
        {
          echo "##  Pre-Flight Smoke Test"
          echo ""
          echo "###  Package Imports"
        } >> "$GITHUB_STEP_SUMMARY"
        if python -c "import FollowWeb_Visualizor"; then
          echo "-  FollowWeb_Visualizor" >> "$GITHUB_STEP_SUMMARY"
        else
          echo "-  FollowWeb_Visualizor" >> "$GITHUB_STEP_SUMMARY"
          exit 1
        fi
        if python -c "from FollowWeb_Visualizor.data.loaders import IncrementalFreesoundLoader"; then
          echo "-  IncrementalFreesoundLoader" >> "$GITHUB_STEP_SUMMARY"
        else
          echo "-  IncrementalFreesoundLoader" >> "$GITHUB_STEP_SUMMARY"
          exit 1
        fi
        if python -c "from FollowWeb_Visualizor.visualization.renderers import SigmaRenderer"; then
          echo "-  SigmaRenderer" >> "$GITHUB_STEP_SUMMARY"
        else
          echo "-  SigmaRenderer" >> "$GITHUB_STEP_SUMMARY"
          exit 1
        fi
        if python -c "import networkx"; then
          echo "-  NetworkX" >> "$GITHUB_STEP_SUMMARY"
        else
          echo "-  NetworkX" >> "$GITHUB_STEP_SUMMARY"
          exit 1
        fi
        echo "" >> "$GITHUB_STEP_SUMMARY"
    
    - name: Validate secrets
      env:
        FREESOUND_API_KEY: ${{ secrets.FREESOUND_API_KEY }}
      run: |
        echo "###  Secret Validation" >> "$GITHUB_STEP_SUMMARY"
        
        # Check FREESOUND_API_KEY (required)
        if [ -z "$FREESOUND_API_KEY" ]; then
          echo "-  FREESOUND_API_KEY: ❌ Not configured" >> "$GITHUB_STEP_SUMMARY"
          echo "::error::FREESOUND_API_KEY not configured"
          exit 1
        else
          # Validate API key format (should be 32 hex characters)
          if [[ "$FREESOUND_API_KEY" =~ ^[a-f0-9]{32}$ ]]; then
            echo "-  FREESOUND_API_KEY: ✅ Valid format" >> "$GITHUB_STEP_SUMMARY"
          else
            echo "-  FREESOUND_API_KEY: ⚠️ Configured but format may be invalid" >> "$GITHUB_STEP_SUMMARY"
          fi
        fi
        echo "" >> "$GITHUB_STEP_SUMMARY"
    
    - name: Test loader initialization (no API calls)
      env:
        FREESOUND_API_KEY: ${{ secrets.FREESOUND_API_KEY }}
      run: |
        echo "###  Component Tests" >> "$GITHUB_STEP_SUMMARY"
        
        # Test loader initialization without making API calls
        python -c "
        import os
        import tempfile
        from FollowWeb_Visualizor.data.loaders import IncrementalFreesoundLoader
        
        # Create temporary checkpoint directory
        with tempfile.TemporaryDirectory() as tmpdir:
            # Initialize loader (no API calls)
            # Loader reads FREESOUND_API_KEY from environment variable automatically
            loader = IncrementalFreesoundLoader(
                config={
                    'checkpoint': {
                        'checkpoint_dir': tmpdir,
                        'checkpoint_interval': 1
                    }
                }
            )
            
            # Verify loader attributes
            assert hasattr(loader, 'rate_limiter'), 'Rate limiter not initialized'
            assert hasattr(loader, 'graph'), 'Graph not initialized'
            assert hasattr(loader, 'processed_ids'), 'Processed IDs not initialized'
            
            # Verify initial state
            assert isinstance(loader.processed_ids, set), 'processed_ids should be a set'
            assert len(loader.processed_ids) == 0, 'processed_ids should start empty'
            assert loader.graph.number_of_nodes() == 0, 'Graph should start empty'
            
            print(' Loader initialization successful')
        "
        loader_exit_code=$?
        if [ $loader_exit_code -eq 0 ]; then
          echo "-  IncrementalFreesoundLoader initialization" >> "$GITHUB_STEP_SUMMARY"
        else
          echo "-  IncrementalFreesoundLoader initialization" >> "$GITHUB_STEP_SUMMARY"
          exit 1
        fi
        
        # Test renderer can be imported (initialization tested in actual pipeline)
        python -c "
        from FollowWeb_Visualizor.visualization.renderers import SigmaRenderer
        print(' SigmaRenderer import successful')
        "
        renderer_exit_code=$?
        if [ $renderer_exit_code -eq 0 ]; then
          echo "-  SigmaRenderer import" >> "$GITHUB_STEP_SUMMARY"
        else
          echo "-  SigmaRenderer import" >> "$GITHUB_STEP_SUMMARY"
          exit 1
        fi
        
        echo "" >> "$GITHUB_STEP_SUMMARY"
    
    - name: Test checkpoint operations (no API calls)
      run: |
        echo "###  Checkpoint Tests" >> "$GITHUB_STEP_SUMMARY"
        
        # Test checkpoint module can be imported (actual operations tested in pipeline)
        python -c "
        from FollowWeb_Visualizor.data.checkpoint import GraphCheckpoint
        from pathlib import Path
        
        # Verify checkpoint class can be instantiated
        checkpoint = GraphCheckpoint(Path('test_checkpoint.gpickle'))
        assert hasattr(checkpoint, 'checkpoint_path'), 'Checkpoint path not set'
        assert hasattr(checkpoint, 'save'), 'Save method not available'
        assert hasattr(checkpoint, 'load'), 'Load method not available'
        
        print(' Checkpoint module validated')
        "
        checkpoint_exit_code=$?
        if [ $checkpoint_exit_code -eq 0 ]; then
          echo "-  Checkpoint module validation" >> "$GITHUB_STEP_SUMMARY"
        else
          echo "-  Checkpoint module validation" >> "$GITHUB_STEP_SUMMARY"
          exit 1
        fi
        
        echo "" >> "$GITHUB_STEP_SUMMARY"
    
    - name: Test validation functions
      run: |
        echo "###  Validation Tests" >> "$GITHUB_STEP_SUMMARY"
        
        # Test validation utilities
        python -c "
        from FollowWeb_Visualizor.utils.validation import (
            validate_positive_integer,
            validate_non_negative_integer,
            validate_file_path
        )
        
        # Test positive integer validation
        try:
            validate_positive_integer(5, 'test')
            print(' Positive integer validation works')
        except:
            raise AssertionError('Positive integer validation failed')
        
        # Test that zero is rejected for positive integers
        try:
            validate_positive_integer(0, 'test')
            raise AssertionError('Should have rejected zero')
        except ValueError:
            print(' Zero correctly rejected for positive integers')
        
        # Test non-negative integer validation
        try:
            validate_non_negative_integer(0, 'test')
            print(' Non-negative integer validation works')
        except:
            raise AssertionError('Non-negative integer validation failed')
        
        print(' All validation tests passed')
        "
        validation_exit_code=$?
        if [ $validation_exit_code -eq 0 ]; then
          echo "-  Validation utilities" >> "$GITHUB_STEP_SUMMARY"
        else
          echo "-  Validation utilities" >> "$GITHUB_STEP_SUMMARY"
          exit 1
        fi
        
        echo "" >> "$GITHUB_STEP_SUMMARY"
    
    - name: Smoke test summary
      if: success()
      run: |
        {
          echo "---"
          echo ""
          echo "##  All Pre-Flight Checks Passed"
          echo ""
          echo "The environment is ready for the Freesound Nightly Pipeline."
          echo "Proceeding to data collection phase..."
        } >> "$GITHUB_STEP_SUMMARY"

  freesound-collection:
    name: Freesound Data Collection
    runs-on: ubuntu-latest
    timeout-minutes: 120  # 2-hour timeout
    needs: smoke-test  # Only run if smoke test passes
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for proper Git operations
        token: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Configure Git
      run: |
        git config user.name "GitHub Actions Bot"
        git config user.email "actions@github.com"
        git config --global init.defaultBranch main
        git config --global advice.detachedHead false
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'
        cache-dependency-path: 'FollowWeb/requirements.txt'
    
    - name: Display Python environment info
      run: |
        python --version
        python -c "import sys; print(f'Python executable: {sys.executable}')"
        python -c "import platform; print(f'Platform: {platform.platform()}')"
        pip --version
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip setuptools wheel
        pip install -r FollowWeb/requirements.txt
        pip install -e FollowWeb/
    
    - name: Validate secrets
      env:
        FREESOUND_API_KEY: ${{ secrets.FREESOUND_API_KEY }}
      run: |
        {
          echo "** Secret Validation**"
          echo ""
        } >> "$GITHUB_STEP_SUMMARY"
        
        # Check FREESOUND_API_KEY (required)
        if [ -z "$FREESOUND_API_KEY" ]; then
          echo "::error::FREESOUND_API_KEY not configured"
          echo "-  FREESOUND_API_KEY: Not configured" >> "$GITHUB_STEP_SUMMARY"
          exit 1
        fi
        API_KEY_STATUS="-  FREESOUND_API_KEY: Configured"
        
        {
          echo "$API_KEY_STATUS"
          echo ""
        } >> "$GITHUB_STEP_SUMMARY"
    
    - name: Verify installation
      run: |
        python -c "import FollowWeb_Visualizor"
        echo " FollowWeb package imported successfully"
        python -c "from FollowWeb_Visualizor.data.loaders import IncrementalFreesoundLoader"
        echo " IncrementalFreesoundLoader available"
        python -c "from FollowWeb_Visualizor.visualization.renderers import SigmaRenderer"
        echo " SigmaRenderer available"
    
    - name: Set pipeline parameters
      id: params
      run: |
        # Use workflow_dispatch inputs if available, otherwise use defaults
        if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
          {
            echo "seed_sample_id=${{ github.event.inputs.seed_sample_id }}"
            echo "max_requests=${{ github.event.inputs.max_requests }}"
            echo "discovery_mode=${{ github.event.inputs.discovery_mode }}"
          } >> "$GITHUB_OUTPUT"
        else
          {
            echo "seed_sample_id="
            echo "max_requests=100"
            echo "discovery_mode=search"
          } >> "$GITHUB_OUTPUT"
        fi
        
        # Generate execution ID
        echo "execution_id=$(date +'%Y%m%d_%H%M%S')" >> "$GITHUB_OUTPUT"
    
    - name: Download checkpoint from private repository
      id: download_checkpoint
      env:
        BACKUP_PAT: ${{ secrets.BACKUP_PAT }}
      run: |
        {
          echo "** Phase 1: Download Checkpoint from Private Repository**"
          echo ""
        } >> "$GITHUB_STEP_SUMMARY"
        
        # Check if BACKUP_PAT is configured (optional - for restoring from backup)
        if [ -z "$BACKUP_PAT" ]; then
          echo "::warning::BACKUP_PAT not configured - starting fresh collection"
          {
            echo "- Status: ⚠️  Starting fresh (no backup to restore)"
          } >> "$GITHUB_STEP_SUMMARY"
          echo "checkpoint_restored=false" >> "$GITHUB_OUTPUT"
          exit 0
        fi
        
        # Get repository owner and name from GITHUB_REPOSITORY
        REPO_OWNER="${{ github.repository_owner }}"
        BACKUP_REPO="${REPO_OWNER}/freesound-backup"
        
        # List all assets from v-checkpoint release
        echo "Fetching assets from ${BACKUP_REPO} release v-checkpoint..."
        ASSETS_JSON=$(curl -s -H "Authorization: token $BACKUP_PAT" \
          "https://api.github.com/repos/${BACKUP_REPO}/releases/tags/v-checkpoint")
        
        # Check if release exists
        if echo "$ASSETS_JSON" | grep -q "Not Found"; then
          echo " Release v-checkpoint not found in ${BACKUP_REPO}"
          {
            echo "- Status: No backup available"
          } >> "$GITHUB_STEP_SUMMARY"
          echo "checkpoint_restored=false" >> "$GITHUB_OUTPUT"
          exit 0
        fi
        
        # Find best checkpoint backup (prefer non-zero nodes, then most recent)
        # Extract node count from filename pattern: checkpoint_backup_XXXnodes_*.tar.gz
        LATEST_ASSET=$(echo "$ASSETS_JSON" | jq -r '
          .assets 
          | map(select(.name | test("checkpoint_backup_[0-9]+nodes")))
          | map(. + {node_count: (.name | capture("(?<nodes>[0-9]+)nodes") | .nodes | tonumber)})
          | map(select(.node_count > 0))
          | sort_by([.node_count, .created_at])
          | reverse
          | .[0]
        ')
        
        if [ "$LATEST_ASSET" = "null" ] || [ -z "$LATEST_ASSET" ]; then
          echo " No checkpoint backups found in ${BACKUP_REPO}"
          {
            echo "- Status: No backup available"
          } >> "$GITHUB_STEP_SUMMARY"
          echo "checkpoint_restored=false" >> "$GITHUB_OUTPUT"
          exit 0
        fi
        
        # Extract asset details
        ASSET_NAME=$(echo "$LATEST_ASSET" | jq -r '.name')
        ASSET_URL=$(echo "$LATEST_ASSET" | jq -r '.url')
        ASSET_SIZE=$(echo "$LATEST_ASSET" | jq -r '.size')
        ASSET_CREATED=$(echo "$LATEST_ASSET" | jq -r '.created_at')
        
        echo " Found latest backup: $ASSET_NAME"
        echo "- Size: $(numfmt --to=iec-i --suffix=B "$ASSET_SIZE")"
        echo "- Created: $ASSET_CREATED"
        
        # Download asset
        echo "Downloading checkpoint backup..."
        curl -L -H "Authorization: token $BACKUP_PAT" \
          -H "Accept: application/octet-stream" \
          "$ASSET_URL" -o checkpoint_backup.tar.gz
        
        # Extract to data/freesound_library/
        echo "Extracting checkpoint to data/freesound_library/..."
        mkdir -p data/freesound_library
        tar -xzf checkpoint_backup.tar.gz -C data/
        
        # Verify extraction
        if [ -f "data/freesound_library/graph_topology.gpickle" ]; then
          echo " Checkpoint restored successfully"
          {
            echo "- Backup: \`$ASSET_NAME\`"
            echo "- Size: \`$(numfmt --to=iec-i --suffix=B "$ASSET_SIZE")\`"
            echo "- Created: \`$ASSET_CREATED\`"
          } >> "$GITHUB_STEP_SUMMARY"
          echo "checkpoint_restored=true" >> "$GITHUB_OUTPUT"
        else
          echo " Checkpoint extraction failed"
          echo "- Status: Extraction failed" >> "$GITHUB_STEP_SUMMARY"
          echo "checkpoint_restored=false" >> "$GITHUB_OUTPUT"
        fi
        
        # Cleanup
        rm -f checkpoint_backup.tar.gz
    
    - name: Check for workflow conflicts
      id: orchestration
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        PYTHONPATH: ${{ github.workspace }}
      run: |
        python -c "
        import sys
        sys.path.insert(0, '${{ github.workspace }}')
        from scripts.analysis.workflow_orchestrator import WorkflowOrchestrator
        import os
        
        orchestrator = WorkflowOrchestrator(
            github_token=os.environ['GITHUB_TOKEN'],
            repository='${{ github.repository }}'
        )
        
        can_proceed, reason = orchestrator.check_and_wait_for_conflicts(
            current_workflow='freesound-nightly-pipeline',
            timeout=7200
        )
        
        with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
            f.write(f'can_proceed={str(can_proceed).lower()}\n')
            f.write(f'skip_reason={reason}\n')
        
        if not can_proceed:
            print(f'⚠️ SKIPPING EXECUTION: {reason}')
            with open(os.environ.get('GITHUB_STEP_SUMMARY', '/dev/null'), 'a') as f:
                f.write(f'## ⚠️ Execution Skipped\n\n')
                f.write(f'**Reason:** {reason}\n\n')
                f.write(f'The workflow will retry on the next scheduled run.\n')
        "

    
    - name: Display pipeline configuration
      if: steps.orchestration.outputs.can_proceed == 'true'
      run: |
        {
          echo ""
          echo "** Phase 2: Pipeline Configuration**"
          echo ""
        } >> "$GITHUB_STEP_SUMMARY"
        
        if [ -n "${{ steps.params.outputs.seed_sample_id }}" ]; then
          SEED_INFO="- Seed Sample ID: \`${{ steps.params.outputs.seed_sample_id }}\`"
        else
          SEED_INFO="- Seed Sample: \`Most downloaded sample (auto-detected)\`"
        fi
        
        {
          echo "$SEED_INFO"
          echo "- Max Requests: \`${{ steps.params.outputs.max_requests }}\` (circuit breaker)"
          echo "- Discovery Mode: \`${{ steps.params.outputs.discovery_mode }}\`"
          echo "- Execution ID: \`${{ steps.params.outputs.execution_id }}\`"
          echo "- Trigger: \`${{ github.event_name }}\`"
          echo ""
        } >> "$GITHUB_STEP_SUMMARY"
    
    - name: Run data collection
      if: steps.orchestration.outputs.can_proceed == 'true'
      id: collection
      env:
        FREESOUND_API_KEY: ${{ secrets.FREESOUND_API_KEY }}
      run: |
        {
          echo "** Phase 3: Data Collection**"
          echo ""
        } >> "$GITHUB_STEP_SUMMARY"
        
        # Build command with optional seed sample ID
        cmd="python generate_freesound_visualization.py"
        if [ -n "${{ steps.params.outputs.seed_sample_id }}" ]; then
          cmd="$cmd --seed-sample-id ${{ steps.params.outputs.seed_sample_id }}"
        fi
        cmd="$cmd --max-requests ${{ steps.params.outputs.max_requests }}"
        cmd="$cmd --discovery-mode ${{ steps.params.outputs.discovery_mode }}"
        cmd="$cmd --skip-visualization"  # Skip visualization generation
        
        # Run the collection script
        eval "$cmd" 2>&1 | tee "collection_${{ steps.params.outputs.execution_id }}.log"
        
        # Capture exit code
        collection_exit_code=${PIPESTATUS[0]}
        
        if [ "$collection_exit_code" -eq 0 ]; then
          echo " Data collection completed successfully"
          echo "collection_status=success" >> "$GITHUB_OUTPUT"
        else
          echo " Collection failed with exit code: $collection_exit_code"
          echo "collection_status=failed" >> "$GITHUB_OUTPUT"
          exit "$collection_exit_code"
        fi
    
    - name: Extract collection statistics
      if: steps.orchestration.outputs.can_proceed == 'true'
      id: stats
      run: |
        # Extract statistics from checkpoint metadata
        if [ -f "data/freesound_library/checkpoint_metadata.json" ]; then
          total_nodes=$(jq -r '.nodes // 0' data/freesound_library/checkpoint_metadata.json)
          total_edges=$(jq -r '.edges // 0' data/freesound_library/checkpoint_metadata.json)
          current_page=$(jq -r '.pagination.current_page // 1' data/freesound_library/checkpoint_metadata.json)
          search_query=$(jq -r '.pagination.search_query // "N/A"' data/freesound_library/checkpoint_metadata.json)
          
          # Extract collection stats if available
          if jq -e '.collection_stats' data/freesound_library/checkpoint_metadata.json > /dev/null 2>&1; then
            api_requests=$(jq -r '.collection_stats.total_api_requests // 0' data/freesound_library/checkpoint_metadata.json)
            duplicates_skipped=$(jq -r '.collection_stats.duplicates_skipped // 0' data/freesound_library/checkpoint_metadata.json)
            new_samples=$(jq -r '.collection_stats.new_samples_added // 0' data/freesound_library/checkpoint_metadata.json)
          else
            api_requests="N/A"
            duplicates_skipped="N/A"
            new_samples="N/A"
          fi
        else
          total_nodes="0"
          total_edges="0"
          current_page="1"
          search_query="N/A"
          api_requests="N/A"
          duplicates_skipped="N/A"
          new_samples="N/A"
        fi
        
        {
          echo "total_nodes=$total_nodes"
          echo "total_edges=$total_edges"
          echo "current_page=$current_page"
          echo "search_query=$search_query"
          echo "api_requests=$api_requests"
          echo "duplicates_skipped=$duplicates_skipped"
          echo "new_samples=$new_samples"
        } >> "$GITHUB_OUTPUT"
        
        echo " Statistics extracted from checkpoint"
    
    # Monitor cache size and trigger early backup if needed
    - name: Monitor cache size
      if: always() && steps.orchestration.outputs.can_proceed == 'true'
      id: cache_monitor
      run: |
        python scripts/cache_monitor.py --checkpoint-dir data/freesound_library
        cache_exit_code=$?
        
        # Store exit code for later steps
        echo "cache_check_exit_code=$cache_exit_code" >> "$GITHUB_OUTPUT"
        
        # Exit code 0 = OK, 1 = Warning (trigger backup), 2 = Critical (trigger backup)
        if [ "$cache_exit_code" -eq 2 ]; then
          echo "::error::Cache size critical - immediate backup required"
          echo "trigger_early_backup=true" >> "$GITHUB_OUTPUT"
        elif [ "$cache_exit_code" -eq 1 ]; then
          echo "::warning::Cache size warning - early backup recommended"
          echo "trigger_early_backup=true" >> "$GITHUB_OUTPUT"
        else
          echo "trigger_early_backup=false" >> "$GITHUB_OUTPUT"
        fi
    
    # Check total repository cache usage
    - name: Check total repository cache usage
      if: always() && steps.orchestration.outputs.can_proceed == 'true'
      id: total_cache_check
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        echo "Checking total repository cache usage..."
        
        # Get all caches for this repository
        CACHES_JSON=$(curl -s -H "Authorization: token $GITHUB_TOKEN" \
          "https://api.github.com/repos/${{ github.repository }}/actions/caches")
        
        TOTAL_CACHES=$(echo "$CACHES_JSON" | jq '.total_count // 0')
        TOTAL_SIZE=$(echo "$CACHES_JSON" | jq '[.actions_caches[].size_in_bytes] | add // 0')
        TOTAL_SIZE_MB=$((TOTAL_SIZE / 1024 / 1024))
        TOTAL_SIZE_GB=$(echo "scale=2; $TOTAL_SIZE_MB / 1024" | bc)
        
        echo "Total caches: $TOTAL_CACHES"
        echo "Total size: ${TOTAL_SIZE_MB}MB (${TOTAL_SIZE_GB}GB)"
        
        # Store for summary
        {
          echo "total_caches=$TOTAL_CACHES"
          echo "total_cache_size_mb=$TOTAL_SIZE_MB"
          echo "total_cache_size_gb=$TOTAL_SIZE_GB"
        } >> "$GITHUB_OUTPUT"
        
        # Warn if approaching 10GB limit
        if [ "$TOTAL_SIZE_MB" -gt 9500 ]; then
          echo "::error::Total repository cache size (${TOTAL_SIZE_GB}GB) exceeds 9.5GB - critical!"
        elif [ "$TOTAL_SIZE_MB" -gt 8000 ]; then
          echo "::warning::Total repository cache size (${TOTAL_SIZE_GB}GB) exceeds 8GB"
        fi
    
    # Debug: List checkpoint files before caching
    - name: Debug - List checkpoint files
      if: always() && steps.orchestration.outputs.can_proceed == 'true'
      run: |
        echo "Listing checkpoint directory contents:"
        ls -laR data/freesound_library/ || echo "Directory not found"
        echo ""
        echo "File sizes:"
        du -h data/freesound_library/* || echo "No files found"
        echo ""
        echo "Total directory size:"
        du -sh data/freesound_library/ || echo "Directory not found"
    
    # CRITICAL: Verify checkpoint has data before caching
    - name: Verify checkpoint has data
      if: always() && steps.orchestration.outputs.can_proceed == 'true'
      id: verify_checkpoint
      run: |
        # Extract node and edge counts from checkpoint metadata (support both field names)
        if [ -f "data/freesound_library/checkpoint_metadata.json" ]; then
          node_count=$(jq -r '.total_nodes // .nodes // 0' data/freesound_library/checkpoint_metadata.json)
          edge_count=$(jq -r '.edges // 0' data/freesound_library/checkpoint_metadata.json)
          
          # Always reject 0 nodes
          if [ "$node_count" -eq 0 ]; then
            echo "::warning::Checkpoint has 0 nodes - skipping cache save to prevent corruption"
            {
              echo ""
              echo "⚠️ **Warning: Empty Checkpoint Detected**"
              echo ""
              echo "The checkpoint has 0 nodes. This checkpoint will NOT be cached to prevent"
              echo "corrupting downstream workflows with empty data."
              echo ""
              echo "**Possible Causes:**"
              echo "- Circuit breaker triggered before any data collected"
              echo "- API returned no results"
              echo "- Checkpoint initialization failed"
            } >> "$GITHUB_STEP_SUMMARY"
            echo "has_data=false" >> "$GITHUB_OUTPUT"
          # Only reject 0 edges if we have many nodes (edges should have been generated)
          # Allow 0 edges for small checkpoints (< 10 nodes) as edges may not be generated yet
          elif [ "$edge_count" -eq 0 ] && [ "$node_count" -ge 10 ]; then
            echo "::warning::Checkpoint has $node_count nodes but 0 edges - likely edge generation failed"
            {
              echo ""
              echo "⚠️ **Warning: Checkpoint Missing Edges**"
              echo ""
              echo "The checkpoint has $node_count nodes but 0 edges. This checkpoint will NOT be cached"
              echo "to prevent corrupting downstream workflows with incomplete graph data."
              echo ""
              echo "**Note:** Small checkpoints (< 10 nodes) are allowed to have 0 edges during collection."
              echo ""
              echo "**Possible Causes:**"
              echo "- Edge generation failed"
              echo "- Graph topology not properly saved"
              echo "- Checkpoint corruption during save"
            } >> "$GITHUB_STEP_SUMMARY"
            echo "has_data=false" >> "$GITHUB_OUTPUT"
          else
            if [ "$edge_count" -eq 0 ]; then
              echo "✅ Checkpoint has $node_count nodes and $edge_count edges - allowing (small checkpoint)"
            else
              echo "✅ Checkpoint has $node_count nodes and $edge_count edges - safe to cache"
            fi
            echo "has_data=true" >> "$GITHUB_OUTPUT"
          fi
        else
          echo "::warning::Checkpoint metadata not found - skipping cache save"
          echo "has_data=false" >> "$GITHUB_OUTPUT"
        fi
    
    # Save checkpoint to cache - backup workflow handles permanent storage
    - name: Save checkpoint to cache
      if: always() && steps.orchestration.outputs.can_proceed == 'true' && steps.verify_checkpoint.outputs.has_data == 'true'
      id: cache_save
      uses: actions/cache/save@v3
      with:
        path: data/freesound_library
        key: checkpoint-${{ github.run_id }}
        # CRITICAL: Include files that are in .gitignore
        # The checkpoint files (*.gpickle, *.db, *.json) are gitignored but must be cached
        enableCrossOsArchive: false
        # Note: GitHub Actions cache respects .gitignore by default, but we need these files
        # The cache action will include them because we're explicitly specifying the path
    
    - name: Verify cache save
      if: always() && steps.orchestration.outputs.can_proceed == 'true'
      run: |
        {
          echo ""
          echo "** Phase 4: Save Checkpoint to Cache**"
          echo ""
          echo "- Cache Key: \`checkpoint-${{ github.run_id }}\`"
          echo "- Retention: 7 days"
          echo "- Size Limit: 10GB"
        } >> "$GITHUB_STEP_SUMMARY"
        
        if [ "${{ steps.verify_checkpoint.outputs.has_data }}" = "true" ]; then
          echo "- Status: ✅ Saved" >> "$GITHUB_STEP_SUMMARY"
          
          # Add cache size warning if needed
          if [ "${{ steps.cache_monitor.outputs.trigger_early_backup }}" = "true" ]; then
            {
              echo ""
              echo "⚠️ **Cache Size Alert**"
              echo ""
              echo "Cache size: \`${{ steps.cache_monitor.outputs.cache_size_human }}\` (\`${{ steps.cache_monitor.outputs.cache_percent }}%\` of 10GB limit)"
              echo ""
              echo "**Action:** Backup workflow will be triggered to prevent cache eviction"
            } >> "$GITHUB_STEP_SUMMARY"
          fi
        else
          echo "- Status: ⏭️ Skipped (empty checkpoint)" >> "$GITHUB_STEP_SUMMARY"
        fi
        
        {
          echo ""
          echo "**Note:** Backup workflow will handle permanent storage upload"
          echo ""
        } >> "$GITHUB_STEP_SUMMARY"
    
    - name: Upload checkpoint files as workflow artifacts
      if: always() && steps.orchestration.outputs.can_proceed == 'true'
      uses: actions/upload-artifact@v4
      with:
        name: checkpoint-backup-${{ steps.params.outputs.execution_id }}
        path: |
          data/freesound_library/*.gpickle
          data/freesound_library/*.db
          data/freesound_library/*.json
        retention-days: 7
        if-no-files-found: ignore
    
    - name: Cleanup ephemeral cache
      if: always()
      run: |
        {
          echo ""
          echo "** Phase 5: Cleanup Ephemeral Cache**"
          echo ""
        } >> "$GITHUB_STEP_SUMMARY"
        
        # Delete checkpoint directory
        if [ -d "data/freesound_library" ]; then
          rm -rf data/freesound_library
          echo " Ephemeral cache wiped"
          echo "- Status: Cache deleted" >> "$GITHUB_STEP_SUMMARY"
        else
          echo " No cache to clean"
          echo "- Status: No cache found" >> "$GITHUB_STEP_SUMMARY"
        fi
    
    - name: Generate execution summary
      if: always()
      run: |
        {
          echo ""
          echo "---"
          echo ""
          echo "##  Execution Summary"
          echo ""
        } >> "$GITHUB_STEP_SUMMARY"
        
        # Check if execution was skipped due to conflicts
        if [ "${{ steps.orchestration.outputs.can_proceed }}" = "false" ]; then
          {
            echo "###  Status: Skipped"
            echo ""
            echo "**Reason:** ${{ steps.orchestration.outputs.skip_reason }}"
            echo ""
            echo "The workflow detected a conflicting workflow and waited for 2 hours, but it did not complete in time."
            echo "The workflow will retry on the next scheduled run."
            echo ""
          } >> "$GITHUB_STEP_SUMMARY"
        elif [ "${{ steps.collection.outputs.collection_status }}" = "success" ]; then
          {
            echo "###  Status: Collection Complete"
            echo ""
            
            # Data collection statistics
            echo "###  Collection Statistics"
            echo ""
            echo "| Metric | Value |"
            echo "|--------|-------|"
            echo "| **Total Nodes** | \`${{ steps.stats.outputs.total_nodes }}\` |"
            echo "| **Total Edges** | \`${{ steps.stats.outputs.total_edges }}\` |"
          } >> "$GITHUB_STEP_SUMMARY"
          if [ "${{ steps.stats.outputs.new_samples }}" != "N/A" ]; then
            echo "| **New Samples** | \`${{ steps.stats.outputs.new_samples }}\` |" >> "$GITHUB_STEP_SUMMARY"
          fi
          if [ "${{ steps.stats.outputs.duplicates_skipped }}" != "N/A" ]; then
            echo "| **Duplicates Skipped** | \`${{ steps.stats.outputs.duplicates_skipped }}\` |" >> "$GITHUB_STEP_SUMMARY"
          fi
          if [ "${{ steps.stats.outputs.api_requests }}" != "N/A" ]; then
            echo "| **API Requests Used** | \`${{ steps.stats.outputs.api_requests }}\` |" >> "$GITHUB_STEP_SUMMARY"
          fi
          {
            echo ""
            
            # Pagination state
            echo "###  Pagination State"
            echo ""
            echo "| Property | Value |"
            echo "|----------|-------|"
            echo "| **Current Page** | \`${{ steps.stats.outputs.current_page }}\` |"
            echo "| **Search Query** | \`${{ steps.stats.outputs.search_query }}\` |"
            echo ""
          } >> "$GITHUB_STEP_SUMMARY"
          
          # Cache size status
          if [ -n "${{ steps.cache_monitor.outputs.cache_size_human }}" ]; then
            {
              echo "###  Cache Status"
              echo ""
              echo "| Metric | Value |"
              echo "|--------|-------|"
              echo "| **Checkpoint Size** | \`${{ steps.cache_monitor.outputs.cache_size_human }}\` |"
              echo "| **Checkpoint Usage** | \`${{ steps.cache_monitor.outputs.cache_percent }}%\` of 10GB limit |"
            } >> "$GITHUB_STEP_SUMMARY"
            
            # Add total repository cache info if available
            if [ -n "${{ steps.total_cache_check.outputs.total_caches }}" ]; then
              {
                echo "| **Total Caches** | \`${{ steps.total_cache_check.outputs.total_caches }}\` |"
                echo "| **Total Cache Size** | \`${{ steps.total_cache_check.outputs.total_cache_size_gb }}GB\` |"
              } >> "$GITHUB_STEP_SUMMARY"
            fi
            
            echo "| **Status** | \`${{ steps.cache_monitor.outputs.cache_status }}\` |" >> "$GITHUB_STEP_SUMMARY"
            
            if [ "${{ steps.cache_monitor.outputs.trigger_early_backup }}" = "true" ]; then
              echo "| **Action** | ⚠️ Early backup triggered |" >> "$GITHUB_STEP_SUMMARY"
            fi
            
            {
              echo ""
              echo "**Note:** GitHub's 10GB cache limit is shared across ALL caches in the repository."
              echo "Backup workflow will clear all caches after successful backup."
              echo ""
            } >> "$GITHUB_STEP_SUMMARY"
          fi
          
          # Downstream workflows
          {
            echo "###  Downstream Workflows"
            echo ""
            echo "- ✅ **Backup Pipeline**: Will be triggered automatically to create backups"
            echo "- ✅ **Repair Pipeline**: Will be triggered automatically to validate and repair data"
            echo "- ✅ **Validation Pipeline**: Will be triggered automatically after repair"
            echo ""
          } >> "$GITHUB_STEP_SUMMARY"
          
        else
          {
            echo "###  Status: Collection Failed"
            echo ""
            echo "The data collection failed. Please check the logs for error details."
            echo ""
            echo "**Common issues:**"
            echo "- API key not configured or invalid"
            echo "- API rate limit exceeded"
            echo "- Network connectivity issues"
            echo "- Checkpoint file corruption"
            echo ""
            echo "**Note:** Checkpoint has been saved to cache."
            echo "The backup workflow will handle backup creation."
            echo ""
          } >> "$GITHUB_STEP_SUMMARY"
        fi
        
        # Execution details (always shown)
        {
          echo "###  Execution Details"
          echo ""
          echo "| Detail | Value |"
          echo "|--------|-------|"
          echo "| **Execution ID** | \`${{ steps.params.outputs.execution_id }}\` |"
          echo "| **Trigger** | \`${{ github.event_name }}\` |"
          echo "| **Workflow Run** | [#${{ github.run_number }}](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}) |"
          echo "| **Cache Key** | \`checkpoint-${{ github.run_id }}\` |"
          echo "| **Commit** | [\`${GITHUB_SHA:0:7}\`](https://github.com/${{ github.repository }}/commit/${{ github.sha }}) |"
          echo ""
          
          # Artifacts
          echo "###  Artifacts"
          echo ""
          echo "- Collection logs are available as workflow artifacts (30-day retention)"
          echo "- Checkpoint saved to GitHub Actions cache (7-day retention)"
          echo "- Download from the [workflow run page](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})"
        } >> "$GITHUB_STEP_SUMMARY"
    
    - name: Upload collection logs
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: collection-logs-${{ steps.params.outputs.execution_id }}
        path: |
          collection_*.log
          freesound_viz_*.log
          fetch_freesound_*.log
        retention-days: 30
        if-no-files-found: ignore
