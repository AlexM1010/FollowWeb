"""
Incremental Freesound data loader with checkpoint support and queue-based recursive fetching.

This module extends FreesoundLoader with incremental graph building capabilities,
checkpoint management, and time-limited execution for long-running operations.

Key Features:
    - Queue-based breadth-first search (BFS) for recursive similar sounds discovery
    - Two-pass processing strategy for correct edge preservation
    - Checkpoint support for crash recovery and resumable operations
    - Time-limited execution with graceful stopping
    - Progress tracking and statistics
    - Retry logic with exponential backoff using tenacity library

Queue-Based BFS:
    Uses a FIFO queue (collections.deque) to process samples breadth-first during
    recursive discovery. Each queue item contains a (sample, depth) tuple, ensuring
    all samples at depth N are processed before moving to depth N+1.

Two-Pass Processing:
    Pass 1: Discovers nodes and stores relationships in a pending_edges dictionary
    Pass 2: Adds edges between discovered nodes using stored relationships

    This approach ensures edges are not lost when target nodes haven't been
    discovered yet during the first pass.
"""

import heapq
import time
from datetime import datetime, timezone
from typing import Any, Callable, Optional, Union, cast

import networkx as nx

from ...core.exceptions import DataProcessingError
from ...output.formatters import EmojiFormatter
from ...utils import ProgressTracker
from ...utils.math import format_time_duration
from ...utils.validation import validate_choice
from ..backup_manager import BackupManager
from ..checkpoint import GraphCheckpoint
from .base import DataLoader


class IncrementalFreesoundLoader(DataLoader):
    """
    Incremental Freesound loader with checkpoint support and queue-based recursive fetching.

    Extends FreesoundLoader to support:
    - Checkpoint loading and saving for crash recovery
    - Incremental processing (skip already-processed samples)
    - Time-limited execution with graceful stopping
    - Progress tracking and statistics
    - Deleted sample cleanup
    - adaptable metadata updates
    - Queue-based BFS for recursive similar sounds discovery
    - Two-pass processing for correct edge preservation
    """

    # ============================================================================
    # DEFAULT CONFIGURATION CONSTANTS - Single Source of Truth
    # ============================================================================
    # All default values are defined here to avoid duplication across the codebase.
    # Override these via config dictionary when initializing the loader.

    # Edge Generation
    DEFAULT_TAG_SIMILARITY_THRESHOLD = 0.15
    """Minimum Jaccard similarity (15%) to create tag-based edges.
    Lower values = larger, more connected communities.
    Higher values = smaller, tighter communities.
    Range: 0.0 (all nodes connect) to 1.0 (only identical tags connect)"""

    # Checkpoint & Persistence
    DEFAULT_CHECKPOINT_DIR = "data/freesound_library"
    """Directory for storing checkpoint files (graph topology, metadata DB, state)"""

    DEFAULT_CHECKPOINT_INTERVAL = 50
    """Number of samples processed between checkpoint saves.
    Lower = more frequent saves (safer but slower).
    Higher = less frequent saves (faster but more data loss risk)"""

    # API Rate Limiting
    DEFAULT_MAX_REQUESTS = 1950
    """Maximum API requests per session (Freesound rate limit: 2000/day).
    Set to 1950 to leave buffer for other operations"""

    DEFAULT_PAGE_SIZE = 150
    """Freesound API maximum page size for search results"""

    # Backup Management
    DEFAULT_BACKUP_INTERVAL_NODES = 25
    """Create backup every N nodes added to graph"""

    DEFAULT_BACKUP_RETENTION_COUNT = 10
    """Number of backup files to retain before cleanup"""

    DEFAULT_COMPRESSION_AGE_DAYS = 7
    """Compress backups older than N days to save disk space"""

    # Discovery & Priority
    DEFAULT_RELATIONSHIP_PRIORITY = 0.7
    """For mixed discovery mode: ratio of relationship-based vs search-based discovery.
    0.0 = all search, 1.0 = all relationships, 0.7 = 70% relationships"""

    DEFAULT_PRIORITY_WEIGHT_DOWNLOADS = 1.0
    """Weight for download count in node priority calculation (most important)"""

    DEFAULT_PRIORITY_WEIGHT_DEGREE = 0.5
    """Weight for node degree (connections) in priority calculation"""

    DEFAULT_PRIORITY_WEIGHT_AGE = 0.1
    """Weight for sample age in priority calculation (least important)"""

    DEFAULT_DORMANT_PENALTY_MULTIPLIER = 0.01
    """Penalty multiplier for nodes that don't discover new samples (dormant nodes)"""

    # Error Handling
    DEFAULT_MAX_THROTTLE_ATTEMPTS = 3
    """Maximum retry attempts when API returns 429 (rate limit) errors"""

    """
    The loader maintains a checkpoint file that tracks:
    - Current graph state
    - Set of processed sample IDs
    - Processing metadata (timestamp, progress, etc.)

    Two-Pass Processing Strategy:
        When using recursive fetching (depth > 0), the loader uses a two-pass approach
        to ensure all edges are preserved correctly:

        Pass 1 (Node Discovery):
            - Uses a FIFO queue to process samples breadth-first
            - Adds nodes to the graph as they are discovered
            - Stores similarity relationships in a pending_edges dictionary
            - Does NOT add edges during this pass (target nodes may not exist yet)
            - Continues until depth limit or max_total_samples is reached

        Pass 2 (Edge Creation):
            - Iterates through stored relationships from Pass 1
            - Adds edges only between nodes that both exist in the graph
            - Makes NO additional API calls (uses stored relationships)
            - Filters out self-loops and duplicate edges

        This approach solves the edge preservation problem where edges were lost
        because target nodes had not been discovered yet when the source node was
        processed.

    Attributes:
        checkpoint: GraphCheckpoint instance for state management
        processed_ids: Set of already-processed sample IDs
        graph: Current graph state (loaded from checkpoint or new)
        start_time: Processing start time for time limit tracking
        checkpoint_interval: Number of samples between checkpoint saves
        max_runtime_hours: Maximum runtime before graceful stop
        verify_existing_sounds: Whether to verify existing samples via API

    Example:
        # Basic usage with recursive fetching
        loader = IncrementalFreesoundLoader(
            config={
                'api_key': 'your_key',
                'checkpoint_dir': 'checkpoints',
                'checkpoint_interval': 50,
                'max_runtime_hours': 2.0
            }
        )

        # Fetch with 2 levels of recursion
        data = loader.fetch_data(
            query='drum',
            max_samples=100,
            recursive_depth=2,
            max_total_samples=100
        )

        # Build the final graph
        graph = loader.build_graph(data)
    """

    def __init__(self, config: Optional[dict[str, Any]] = None):
        """
        Initialize incremental loader with checkpoint support.

        Args:
            config: Configuration dictionary with optional keys:
                   - checkpoint_dir: Directory for checkpoint files (default: 'checkpoints')
                   - checkpoint_interval: Samples between saves (default: 50)
                   - max_runtime_hours: Max runtime before stop (default: None)
                   - verify_existing_sounds: Verify samples via API (default: False)
                   - max_samples_mode: Collection mode - 'limit' or 'queue-empty' (default: 'limit')
                   - max_requests: Maximum API requests per session (default: 1950)
                   Plus all FreesoundLoader config options
        """
        super().__init__(config)

        # Initialize Freesound API client and rate limiter (from FreesoundLoader)
        import os
        import freesound
        from ...utils.rate_limiter import RateLimiter

        api_key = self.config.get("api_key") or os.getenv("FREESOUND_API_KEY")
        if not api_key:
            from ...core.exceptions import DataProcessingError

            raise DataProcessingError(
                "Freesound API key required. Provide via config['api_key'] "
                "or FREESOUND_API_KEY environment variable"
            )

        self.client = freesound.FreesoundClient()
        self.client.set_token(api_key)

        requests_per_minute = self.config.get("requests_per_minute", 60)
        self.rate_limiter = RateLimiter(requests_per_minute=requests_per_minute)

        # Sound cache for efficiency
        self._sound_cache: dict[int, Any] = {}
        self._cache_hits = 0
        self._cache_misses = 0

        # Checkpoint configuration
        checkpoint_dir = self.config.get("checkpoint_dir", self.DEFAULT_CHECKPOINT_DIR)
        self.checkpoint_interval = self.config.get(
            "checkpoint_interval", self.DEFAULT_CHECKPOINT_INTERVAL
        )
        self.max_runtime_hours = self.config.get("max_runtime_hours")
        self.verify_existing_sounds = self.config.get("verify_existing_sounds", False)
        self.max_samples_mode = self.config.get("max_samples_mode", "limit")

        # API quota circuit breaker
        self.session_request_count = 0
        self.max_requests = self.config.get("max_requests", self.DEFAULT_MAX_REQUESTS)

        # Search pagination state (restored from checkpoint if exists)
        self.pagination_state = {"page": 1, "query": "", "sort": "downloads_desc"}

        # Statistics tracking
        self.stats: dict[str, Union[int, list[int]]] = {
            "api_requests_saved": 0,
            "samples_skipped": 0,
            "new_samples_per_expansion": [],
            "dormant_nodes_identified": 0,
            "throttle_retries": 0,
            "failed_requests": 0,
            "user_edges_created": 0,
            "pack_edges_created": 0,
            "tag_edges_created": 0,
        }

        # Validate max_samples_mode
        validate_choice(
            self.max_samples_mode, "max_samples_mode", ["limit", "queue-empty"]
        )

        # Use persistent library file (committed to Git for crash recovery)
        # This file grows over time and persists across runs
        checkpoint_filename = "freesound_library.pkl"
        checkpoint_path = f"{checkpoint_dir}/{checkpoint_filename}"
        self.checkpoint = GraphCheckpoint(checkpoint_path)

        # Initialize backup manager with configurable intervals
        self.backup_manager = BackupManager(
            backup_dir=checkpoint_dir,
            config={
                "backup_interval_nodes": self.config.get(
                    "backup_interval_nodes", self.DEFAULT_BACKUP_INTERVAL_NODES
                ),
                "backup_retention_count": self.config.get(
                    "backup_retention_count", self.DEFAULT_BACKUP_RETENTION_COUNT
                ),
                "enable_compression": self.config.get("backup_compression", True),
                "enable_tiered_backups": self.config.get("tiered_backups", True),
                "compression_age_days": self.config.get(
                    "compression_age_days", self.DEFAULT_COMPRESSION_AGE_DAYS
                ),
            },
            logger=self.logger,
        )

        # Backup path for safety
        self.backup_dir = checkpoint_dir

        # State tracking
        self.processed_ids: set[str] = set()
        self.graph: nx.DiGraph = nx.DiGraph()
        self.start_time: Optional[float] = None

        # Try to load existing checkpoint
        self._load_checkpoint()

        # Track initial state for calculating additions during this session
        self._initial_node_count = self.graph.number_of_nodes()
        self._initial_edge_count = self.graph.number_of_edges()

        self.logger.info(
            f"IncrementalFreesoundLoader initialized: "
            f"checkpoint_interval={self.checkpoint_interval}, "
            f"max_runtime_hours={self.max_runtime_hours}, "
            f"max_requests={self.max_requests}, "
            f"initial_nodes={self._initial_node_count}, "
            f"initial_edges={self._initial_edge_count}"
        )

    def _extract_uploader_id(self, previews: dict[str, str]) -> Optional[int]:
        """
        Extract uploader_id from preview URLs for space-efficient storage.

        Instead of storing full preview URLs (~200 bytes), we store uploader_id (~7 bytes).
        Frontend reconstructs: https://freesound.org/data/previews/{folder}/{id}_{uploader_id}-{quality}.mp3

        Args:
            previews: Dictionary of preview URLs from Freesound API

        Returns:
            uploader_id as integer, or None if not found
        """
        import re

        if not previews or not isinstance(previews, dict):
            return None

        # Try to extract from any available preview URL
        # URL format: https://freesound.org/data/previews/{folder}/{id}_{uploader_id}-{quality}.mp3
        for key in [
            "preview-hq-mp3",
            "preview-lq-mp3",
            "preview-hq-ogg",
            "preview-lq-ogg",
        ]:
            if key in previews and previews[key]:
                url = previews[key]
                match = re.search(r"_(\d+)-", url)
                if match:
                    return int(match.group(1))

        return None

    @staticmethod
    def _is_retryable_error(exception: Exception) -> bool:
        """
        Determine if an exception should trigger a retry.

        Retries on:
        - ConnectionError: Network connectivity issues
        - TimeoutError: Request timeouts
        - freesound.FreesoundException with code 429: Rate limit errors only
        """
        import freesound

        if isinstance(exception, (ConnectionError, TimeoutError)):
            return True
        if isinstance(exception, freesound.FreesoundException):
            return hasattr(exception, "code") and exception.code == 429
        return False

    def _retry_with_backoff(
        self,
        func: Callable,
        *args,
        max_retries: int = 3,
        initial_wait: float = 2.0,
        **kwargs,
    ) -> Any:
        """
        Retry a function on rate limit and network errors using tenacity library.

        Uses exponential backoff with configurable retry attempts.
        """
        from tenacity import (
            before_sleep_log,
            retry,
            retry_if_exception,
            stop_after_attempt,
            wait_exponential,
        )
        import logging

        retry_decorator = retry(
            stop=stop_after_attempt(max_retries),
            wait=wait_exponential(multiplier=1, min=initial_wait, max=10),
            retry=retry_if_exception(self._is_retryable_error),
            before_sleep=before_sleep_log(self.logger, logging.WARNING),
            reraise=True,
        )

        @retry_decorator
        def _wrapped_func():
            return func(*args, **kwargs)

        return _wrapped_func()

    def _extract_sample_metadata(self, sound) -> Optional[dict[str, Any]]:
        """
        Extract metadata from Freesound sound object.

        Args:
            sound: Freesound sound object from API

        Returns:
            Dictionary with sample metadata, or None if invalid
        """
        sound_dict = sound.as_dict()

        # Validate filesize
        if sound_dict.get("filesize", 0) == 0:
            self.logger.warning(
                f"Skipping sample {sound.id} with invalid filesize (0 bytes)"
            )
            return None

        metadata = sound_dict.copy()

        # Extract uploader_id from preview URL
        if "previews" in sound_dict:
            uploader_id = self._extract_uploader_id(sound_dict["previews"])
            if uploader_id:
                metadata["uploader_id"] = uploader_id

        # Remove bulky fields
        for field in ["description", "previews", "images"]:
            if field in metadata:
                del metadata[field]

        # Ensure critical fields
        metadata.setdefault("id", sound.id)
        metadata.setdefault("name", sound.name)
        metadata.setdefault("tags", sound.tags if hasattr(sound, "tags") else [])
        metadata.setdefault(
            "duration", sound.duration if hasattr(sound, "duration") else 0
        )
        metadata.setdefault(
            "username", sound.username if hasattr(sound, "username") else ""
        )

        return metadata

    def close(self) -> None:
        """Close resources and cleanup."""
        if hasattr(self, "metadata_cache") and self.metadata_cache:
            try:
                self.metadata_cache.close()
                self.logger.debug("Metadata cache closed")
            except Exception as e:
                self.logger.error(f"Error closing metadata cache: {e}")

    def __enter__(self):
        """Context manager entry."""
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit - ensures cleanup."""
        self.close()
        return False  # Don't suppress exceptions

    def __del__(self):
        """Destructor to ensure cleanup."""
        self.close()

    def _load_checkpoint(self) -> None:
        """Load checkpoint using split architecture (graph topology + SQLite metadata)."""
        import json
        from pathlib import Path

        from ..storage import MetadataCache

        checkpoint_dir = Path(
            self.config.get("checkpoint_dir", "data/freesound_library")
        )

        # Check for split checkpoint files
        topology_path = checkpoint_dir / "graph_topology.gpickle"
        metadata_db_path = checkpoint_dir / "metadata_cache.db"
        checkpoint_meta_path = checkpoint_dir / "checkpoint_metadata.json"

        # Try loading split checkpoint first (new architecture)
        if topology_path.exists() and metadata_db_path.exists():
            try:
                # Load graph topology (edges only, no attributes)
                import pickle

                with open(topology_path, "rb") as f:
                    # nosec B301 - Loading our own checkpoint data, not untrusted input
                    self.graph = pickle.load(f)  # nosec

                # Connect to metadata cache
                self.metadata_cache = MetadataCache(str(metadata_db_path), self.logger)

                # Restore nodes from metadata cache
                # The graph topology file only contains edges, nodes must be restored from SQLite
                all_metadata = self.metadata_cache.get_all_metadata()
                for sample_id, metadata in all_metadata.items():
                    node_id = str(sample_id)
                    if node_id not in self.graph:
                        self.graph.add_node(node_id, **metadata)

                # Load checkpoint metadata
                if checkpoint_meta_path.exists():
                    with open(checkpoint_meta_path) as f:
                        checkpoint_metadata = json.load(f)

                    self.processed_ids = set(
                        checkpoint_metadata.get("processed_ids", [])
                    )
                    self.pagination_state = checkpoint_metadata.get(
                        "pagination_state",
                        {"page": 1, "query": "", "sort": "downloads_desc"},
                    )
                    last_update = checkpoint_metadata.get("timestamp", "unknown")

                    # Restore edge generation metadata
                    edge_gen_metadata = checkpoint_metadata.get("edge_generation", {})
                    self._last_tag_threshold = edge_gen_metadata.get(
                        "tag_similarity_threshold"
                    )

                    self.logger.info(
                        f"Resumed from split checkpoint: {self.graph.number_of_nodes()} nodes, "
                        f"{len(self.processed_ids)} processed samples, "
                        f"pagination page: {self.pagination_state.get('page', 1)}, "
                        f"last update: {last_update}"
                    )
                else:
                    # No metadata file, reconstruct from graph
                    self.processed_ids = {str(node) for node in self.graph.nodes()}
                    self.logger.warning(
                        "No checkpoint metadata found, reconstructed from graph"
                    )

                return

            except Exception as e:
                self.logger.error(f"Failed to load split checkpoint: {e}")
                # Fall through to try legacy checkpoint

        # Fall back to legacy checkpoint (old architecture)
        checkpoint_data = self.checkpoint.load()

        if checkpoint_data:
            self.graph = checkpoint_data["graph"]
            self.processed_ids = checkpoint_data["processed_ids"]

            # Load sound cache for efficiency (avoids re-fetching known samples)
            self._sound_cache = checkpoint_data.get("sound_cache", {})
            if self._sound_cache:
                self.logger.info(
                    f"Loaded sound cache with {len(self._sound_cache)} samples"
                )

            metadata = checkpoint_data.get("metadata", {})
            last_update = metadata.get("timestamp", "unknown")

            self.logger.info(
                f"Resumed from legacy checkpoint: {self.graph.number_of_nodes()} nodes, "
                f"{len(self.processed_ids)} processed samples, "
                f"last update: {last_update}"
            )

            # Migrate to split architecture
            self.logger.info("Migrating legacy checkpoint to split architecture...")
            self._migrate_to_split_checkpoint()
        else:
            # No checkpoint exists, initialize metadata cache and pagination state
            from ..storage import MetadataCache

            checkpoint_dir = Path(
                self.config.get("checkpoint_dir", "data/freesound_library")
            )
            metadata_db_path = checkpoint_dir / "metadata_cache.db"
            self.metadata_cache = MetadataCache(str(metadata_db_path), self.logger)
            self.pagination_state = {"page": 1, "query": "", "sort": "downloads_desc"}

    def _migrate_to_split_checkpoint(self) -> None:
        """Migrate legacy checkpoint to split architecture."""
        from pathlib import Path

        from ..storage import MetadataCache

        checkpoint_dir = Path(
            self.config.get("checkpoint_dir", "data/freesound_library")
        )
        checkpoint_dir.mkdir(parents=True, exist_ok=True)

        # Create metadata cache
        metadata_db_path = checkpoint_dir / "metadata_cache.db"
        self.metadata_cache = MetadataCache(str(metadata_db_path), self.logger)

        # Extract metadata from graph nodes and store in SQLite
        metadata_dict = {}
        for node_id in self.graph.nodes():
            node_data = dict(self.graph.nodes[node_id])
            metadata_dict[int(node_id)] = node_data

        # Bulk insert metadata
        if metadata_dict:
            self.metadata_cache.bulk_insert(metadata_dict)
            self.logger.info(
                f"Migrated {len(metadata_dict)} node metadata entries to SQLite"
            )

        # Save split checkpoint
        self._save_checkpoint({"migration": "completed"})

        self.logger.info("Migration to split checkpoint architecture completed")

    def _save_checkpoint(self, metadata: Optional[dict[str, Any]] = None) -> None:
        """
        Save current state using split checkpoint architecture.

        Saves:
        1. Graph topology (edges only, no attributes) to .gpickle
        2. Sample metadata to SQLite database
        3. Checkpoint metadata to JSON

        Creates backups every 100 nodes to prevent data loss.

        Implements fail-fast verification: verifies all files exist after save.
        If verification fails, saves to permanent storage and raises exception.

        Args:
            metadata: Optional metadata to include in checkpoint

        Raises:
            RuntimeError: If checkpoint verification fails after save
        """
        import json
        from pathlib import Path

        checkpoint_dir = Path(
            self.config.get("checkpoint_dir", "data/freesound_library")
        )
        checkpoint_dir.mkdir(parents=True, exist_ok=True)

        # Define checkpoint file paths
        topology_path = checkpoint_dir / "graph_topology.gpickle"
        metadata_db_path = checkpoint_dir / "metadata_cache.db"

        # Calculate nodes and edges added during this session
        current_nodes = self.graph.number_of_nodes()
        current_edges = self.graph.number_of_edges()
        nodes_added = current_nodes - getattr(self, "_initial_node_count", 0)
        edges_added = current_edges - getattr(self, "_initial_edge_count", 0)

        # Prepare checkpoint metadata
        checkpoint_metadata = {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "nodes": current_nodes,
            "edges": current_edges,
            "processed_ids": list(self.processed_ids),
            "pagination_state": getattr(
                self,
                "pagination_state",
                {"page": 1, "query": "", "sort": "downloads_desc"},
            ),
            "edge_generation": {
                "last_tag_edge_check": time.time(),
                "processed_node_count": current_nodes,
                "tag_similarity_threshold": getattr(self, "_last_tag_threshold", None),
            },
            "collection_stats": {
                "nodes_added": nodes_added,
                "edges_added": edges_added,
                "total_api_requests": self.session_request_count,
                "new_samples_added": nodes_added,  # Same as nodes_added for consistency
                "duplicates_skipped": self.stats.get("samples_skipped", 0),
                "max_requests": self.max_requests,  # Save for repair workflow
            },
        }

        # Add validation history to metadata if not already present
        if "validation_history" not in checkpoint_metadata:
            # Try to get existing validation history from previous checkpoint
            checkpoint_meta_path = checkpoint_dir / "checkpoint_metadata.json"
            if checkpoint_meta_path.exists():
                try:
                    with open(checkpoint_meta_path) as f:
                        existing_metadata = json.load(f)
                    existing_validation_history = existing_metadata.get(
                        "validation_history", {}
                    )
                    checkpoint_metadata["validation_history"] = (
                        existing_validation_history
                    )
                except Exception:
                    checkpoint_metadata["validation_history"] = {
                        "last_full_existence_check": None,
                        "last_partial_existence_check": None,
                        "last_metadata_refresh": None,
                    }
            else:
                checkpoint_metadata["validation_history"] = {
                    "last_full_existence_check": None,
                    "last_partial_existence_check": None,
                    "last_metadata_refresh": None,
                }

        if metadata:
            # Preserve validation_history if it exists in metadata
            if "validation_history" in metadata:
                checkpoint_metadata["validation_history"] = metadata[
                    "validation_history"
                ]
            checkpoint_metadata.update(metadata)

        # 1. Save graph topology (clean graph without attributes)
        # Create a new graph with only edges (more memory fast than copy + clear)
        import networkx as nx

        graph_clean: nx.DiGraph = nx.DiGraph()
        # Add nodes first (without attributes) to preserve isolated nodes
        graph_clean.add_nodes_from(self.graph.nodes())
        # Then add edges
        graph_clean.add_edges_from(self.graph.edges())

        import pickle

        with open(topology_path, "wb") as f:
            pickle.dump(graph_clean, f, pickle.HIGHEST_PROTOCOL)

        # Explicitly delete to free memory immediately
        del graph_clean

        # 2. Save metadata to SQLite (only new/updated samples)
        if hasattr(self, "metadata_cache"):
            # Extract metadata from graph nodes
            metadata_dict = {}
            for node_id in self.graph.nodes():
                node_data = dict(self.graph.nodes[node_id])
                # Only save if metadata exists
                if node_data:
                    metadata_dict[int(node_id)] = node_data

            # Bulk insert/update metadata
            if metadata_dict:
                self.metadata_cache.bulk_insert(metadata_dict)
                # bulk_insert already logs this

        # 3. Save checkpoint metadata JSON
        checkpoint_meta_path = checkpoint_dir / "checkpoint_metadata.json"
        with open(checkpoint_meta_path, "w") as f:
            json.dump(checkpoint_metadata, f, indent=2)

        # 4. Also call the GraphCheckpoint.save() method for compatibility with tests
        # and to maintain the abstraction layer
        try:
            self.checkpoint.save(
                graph=self.graph,
                processed_ids=self.processed_ids,
                metadata=checkpoint_metadata,
                sound_cache=getattr(self, "_sound_cache", {}),
            )
        except Exception as e:
            self.logger.warning(f"GraphCheckpoint.save() failed: {e}")

        self.logger.debug(
            f"Checkpoint saved: {checkpoint_metadata['nodes']} nodes, "
            f"{checkpoint_metadata['edges']} edges"
        )

        # Use BackupManager for intelligent tiered backups
        if self.backup_manager.should_create_backup(self.graph.number_of_nodes()):
            metadata_db_path = checkpoint_dir / "metadata_cache.db"
            self.backup_manager.create_backup(
                topology_path=topology_path,
                metadata_db_path=metadata_db_path,
                checkpoint_metadata=checkpoint_metadata,
            )

        # FAIL-FAST VERIFICATION: Verify checkpoint files exist and are valid
        # This implements Requirements 11.5, 11.6, 13.1, 13.7
        # Skip verification in test mode to allow mocked data
        import os

        skip_verification = os.getenv("FOLLOWWEB_SKIP_CHECKPOINT_VERIFICATION") == "1"

        if not skip_verification:
            from ..checkpoint_verifier import CheckpointVerifier

            verifier = CheckpointVerifier(checkpoint_dir, self.logger)
            success, message = verifier.verify_checkpoint_files()
        else:
            success = True
            message = "Verification skipped (test mode)"

        if not success:
            self.logger.error(f"ðŸ”´ CRITICAL: Checkpoint verification failed: {message}")
            self.logger.error(
                "ðŸ”´ Attempting to save to permanent storage before failing..."
            )

            # Try to save to permanent storage before failing
            try:
                if hasattr(self, "backup_manager"):
                    self.backup_manager.create_backup(
                        topology_path=topology_path,
                        metadata_db_path=metadata_db_path,
                        checkpoint_metadata=checkpoint_metadata,
                    )
                    self.logger.info("âœ… Data saved to permanent storage")
            except Exception as backup_error:
                self.logger.error(
                    f"âŒ Failed to save to permanent storage: {backup_error}"
                )

            # Close metadata cache before raising exception to prevent file locks
            if hasattr(self, "metadata_cache") and self.metadata_cache:
                try:
                    self.metadata_cache.close()
                except Exception as close_error:
                    self.logger.error(f"Failed to close metadata cache: {close_error}")

            # Raise exception to trigger fail-fast behavior
            raise RuntimeError(
                f"Checkpoint verification failed: {message}. "
                "Data may have been saved to permanent storage."
            )

    def _create_backup(self) -> None:
        """
        Create timestamped backups of split checkpoint files.

        Backs up both graph topology and metadata database.
        """
        try:
            import shutil
            from pathlib import Path

            checkpoint_dir = Path(
                self.config.get("checkpoint_dir", "data/freesound_library")
            )

            # Create backup filenames with timestamp and node count
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            nodes = self.graph.number_of_nodes()

            # Backup graph topology
            topology_path = checkpoint_dir / "graph_topology.gpickle"
            if topology_path.exists():
                topology_backup = (
                    checkpoint_dir
                    / f"graph_topology_backup_{nodes}nodes_{timestamp}.gpickle"
                )
                shutil.copy2(topology_path, topology_backup)
                self.logger.info(f"ðŸ“¦ Topology backup created: {topology_backup.name}")

            # Backup metadata database
            metadata_db_path = checkpoint_dir / "metadata_cache.db"
            if metadata_db_path.exists():
                metadata_backup = (
                    checkpoint_dir
                    / f"metadata_cache_backup_{nodes}nodes_{timestamp}.db"
                )
                shutil.copy2(metadata_db_path, metadata_backup)
                self.logger.info(f"ðŸ“¦ Metadata backup created: {metadata_backup.name}")

        except Exception as e:
            self.logger.warning(f"Failed to create backup: {e}")

    def _check_time_limit(self) -> bool:
        """
        Check if time limit has been reached.

        Returns:
            True if time limit reached, False otherwise
        """
        if not self.max_runtime_hours or not self.start_time:
            return False

        elapsed_hours = (time.time() - self.start_time) / 3600
        return elapsed_hours >= self.max_runtime_hours

    def _calculate_progress_stats(
        self, current: int, total: int, elapsed_seconds: float
    ) -> dict[str, Any]:
        """
        Calculate progress statistics.

        Args:
            current: Current sample count
            total: Total sample count
            elapsed_seconds: Elapsed time in seconds

        Returns:
            Dictionary with progress statistics
        """
        percentage = (current / total * 100) if total > 0 else 0
        remaining = total - current

        # Estimate time remaining
        if current > 0 and elapsed_seconds > 0:
            rate = current / elapsed_seconds
            eta_seconds = remaining / rate if rate > 0 else 0
            eta_minutes = eta_seconds / 60
        else:
            eta_minutes = 0

        return {
            "percentage": percentage,
            "current": current,
            "total": total,
            "remaining": remaining,
            "elapsed_minutes": elapsed_seconds / 60,
            "eta_minutes": eta_minutes,
        }

    def _search_with_pagination(
        self,
        query: Optional[str] = None,
        tags: Optional[list[str]] = None,
        licenses: Optional[list[str]] = None,
        sort_order: str = "downloads_desc",
    ) -> list[dict[str, Any]]:
        """
        Search for samples using pagination, continuing from last checkpoint.

        This method implements page-by-page search using Freesound API pagination.
        It restores pagination state from checkpoint and continues from the last
        processed page. The circuit breaker stops collection when max_requests is hit.

        Args:
            query: Text search query (empty string matches all samples)
            tags: List of tags to filter by
            sort_order: Sort order for results (default: "downloads_desc")

        Returns:
            List of sample dictionaries fetched in this session

        Notes:
            - Checks circuit breaker before each page request
            - Updates pagination_state.current_page after each successful page
            - Saves checkpoint with pagination state after each page
            - Resets pagination when search query or sort order changes
        """
        # Check if search parameters changed - reset pagination if so
        if (
            self.pagination_state.get("query") != query
            or self.pagination_state.get("sort") != sort_order
        ):
            self.logger.info(
                f"Search parameters changed, resetting pagination: "
                f"query '{self.pagination_state.get('query')}' -> '{query}', "
                f"sort '{self.pagination_state.get('sort')}' -> '{sort_order}'"
            )
            self.pagination_state = {
                "page": 1,
                "query": query or "",
                "sort": sort_order,
            }

        samples: list[dict[str, Any]] = []
        current_page: int = self.pagination_state.get("page", 1)

        self.logger.info(
            f"Starting pagination search from page {current_page}: "
            f"query='{query}', sort={sort_order}"
        )

        try:
            # Build search filter
            search_filter = ""
            filters = []

            if tags:
                filters.append(" ".join(f"tag:{tag}" for tag in tags))

            if licenses:
                # Construct OR filter for licenses: (license:"A" OR license:"B")
                license_filter = " OR ".join(f'license:"{lic}"' for lic in licenses)
                filters.append(f"({license_filter})")

            if filters:
                search_filter = " ".join(filters)

            if licenses:
                # Construct OR filter for licenses: (license:"A" OR license:"B")
                license_filter = " OR ".join(f'license:"{lic}"' for lic in licenses)
                filters.append(f"({license_filter})")

            if filters:
                search_filter = " ".join(filters)

            page_size = min(
                self.DEFAULT_PAGE_SIZE,
                self.config.get("page_size", self.DEFAULT_PAGE_SIZE),
            )  # API max

            while True:
                # Check circuit breaker BEFORE making API request
                if self._check_circuit_breaker():
                    self.logger.info(
                        f"Circuit breaker triggered at page {current_page}, "
                        f"collected {len(samples)} samples this session"
                    )
                    break

                # Rate limit the request
                self.rate_limiter.acquire()

                # Fetch page with retry logic
                def _do_search(page=current_page):
                    return self.client.text_search(
                        query=query or "",
                        filter=search_filter if search_filter else None,
                        page=page,
                        page_size=page_size,
                        sort=sort_order,
                        # Collect comprehensive metadata in single API request
                        # Legal: url, license (attribution/compliance)
                        # Taxonomy: category, category_code, category_is_user_provided
                        # Temporal: created (time-series analysis)
                        # Quality: num_ratings, num_comments (confidence metrics)
                        # Technical: type, samplerate, channels (audio properties)
                        # Visual: images (waveforms/spectrograms for tooltips)
                        # Geographic: geotag (location-based features)
                        # Note: download and analysis_files can be fetched on-demand later
                        fields="id,name,tags,description,duration,username,pack,license,created,type,channels,filesize,samplerate,category,category_code,category_is_user_provided,previews,images,num_downloads,num_ratings,avg_rating,num_comments,geotag,url",
                    )

                results = self._retry_with_backoff(_do_search)
                self._increment_request_count()

                # Process results from this page
                page_samples = []
                for sound in results:
                    # Check if sample already exists (duplicate detection)
                    sample_id = sound.id
                    if hasattr(self, "metadata_cache") and self.metadata_cache.exists(
                        sample_id
                    ):
                        self.stats["samples_skipped"] = (
                            cast(int, self.stats["samples_skipped"]) + 1
                        )
                        self.stats["api_requests_saved"] = (
                            cast(int, self.stats["api_requests_saved"]) + 1
                        )
                        continue

                    # Check circuit breaker before fetching full metadata
                    if self._check_circuit_breaker():
                        break

                    # Fetch full metadata for new sample
                    try:
                        self.rate_limiter.acquire()
                        full_sound = self._retry_with_backoff(
                            self.client.get_sound, sample_id
                        )
                        self._increment_request_count()

                        sample_data = self._extract_sample_metadata(full_sound)
                        if sample_data is not None:
                            page_samples.append(sample_data)
                            samples.append(sample_data)
                        else:
                            # Mark as processed so we don't try to fetch it again
                            self.processed_ids.add(str(sample_id))
                    except Exception as e:
                        self.logger.warning(f"Skipping invalid sample {sample_id}: {e}")
                        # Mark as processed so we don't try to fetch it again
                        self.processed_ids.add(str(sample_id))
                        self.stats["samples_skipped"] = (
                            cast(int, self.stats["samples_skipped"]) + 1
                        )

                self.logger.info(
                    f"Page {current_page}: fetched {len(page_samples)} new samples "
                    f"(skipped {self.stats['samples_skipped']} duplicates)"
                )

                # Update pagination state after successful page
                current_page += 1
                self.pagination_state["page"] = current_page

                # Save checkpoint with updated pagination state
                self._save_checkpoint(
                    {
                        "pagination_progress": {
                            "current_page": current_page,
                            "samples_this_session": len(samples),
                        }
                    }
                )

                # Check if there are more pages
                if not results.next:
                    self.logger.info(
                        f"Reached end of results at page {current_page - 1}, "
                        f"resetting pagination to page 1"
                    )
                    self.pagination_state["page"] = 1
                    break

        except Exception as e:
            self.logger.error(f"Pagination search failed at page {current_page}: {e}")
            # Save checkpoint with current state before raising
            self._save_checkpoint(
                {
                    "pagination_error": str(e),
                    "last_successful_page": current_page - 1,
                }
            )
            raise

        self.logger.info(
            f"Pagination search complete: {len(samples)} samples collected, "
            f"next run will start from page {self.pagination_state['page']}"
        )

        return samples

    def fetch_data(  # type: ignore[override]
        self,
        query: Optional[str] = None,
        tags: Optional[list[str]] = None,
        licenses: Optional[list[str]] = None,
        max_samples: int = 1000,
        discovery_mode: str = "search",
        relationship_priority: Optional[float] = None,
        include_user_edges: bool = True,
        include_pack_edges: bool = True,
        include_tag_edges: bool = True,
        tag_similarity_threshold: Optional[float] = None,
        use_pagination: bool = False,
        sort_order: str = "downloads_desc",
    ) -> dict[str, Any]:
        """
        Fetch sample data incrementally with checkpoint support and edge generation.

        Overrides parent method to support incremental processing:
        - Skips already-processed samples
        - Saves checkpoints periodically
        - Respects time limits
        - Tracks progress statistics
        - Generates edges based on user, pack, and tag relationships
        - Supports pagination-based collection mode

        Args:
            query: Text search query
            tags: List of tags to filter by
            max_samples: Maximum number of samples to fetch
            discovery_mode: Sample discovery strategy - "search", "relationships", or "mixed"
            relationship_priority: For mixed mode, ratio of pending vs search (0.0-1.0)
            include_user_edges: Create edges for same-user samples
            include_pack_edges: Create edges for same-pack samples
            include_tag_edges: Create edges for tag similarity
            tag_similarity_threshold: Minimum Jaccard similarity for tag edges (default: from config or DEFAULT_TAG_SIMILARITY_THRESHOLD)
            use_pagination: Use pagination-based collection (continues from last page)
            sort_order: Sort order for pagination search (default: "downloads_desc")

        Returns:
            Dictionary with 'samples' and edge statistics
        """
        # Apply defaults for optional parameters
        if relationship_priority is None:
            relationship_priority = self.DEFAULT_RELATIONSHIP_PRIORITY
        if tag_similarity_threshold is None:
            tag_similarity_threshold = self.config.get(
                "tag_similarity_threshold", self.DEFAULT_TAG_SIMILARITY_THRESHOLD
            )

        # Note: Empty query string is valid per Freesound API docs (returns all sounds)
        if query is None and not tags:
            raise DataProcessingError(
                "Must provide either query or tags for Freesound search"
            )

        self.start_time = time.time()

        # Choose collection mode: pagination or standard search
        if use_pagination:
            self.logger.info(
                f"Using pagination mode: query='{query}', tags={tags}, licenses={licenses}, "
                f"sort={sort_order}, current_page={self.pagination_state.get('page', 1)}, "
                f"already_processed={len(self.processed_ids)}"
            )
            all_samples = self._search_with_pagination(
                query, tags, licenses, sort_order
            )
        else:
            # Standard search mode (legacy behavior)
            self.logger.info(
                f"Searching Freesound (incremental): query='{query}', tags={tags}, "
                f"max_samples={max_samples}, already_processed={len(self.processed_ids)}"
            )
            all_samples = self._search_samples(query, tags, max_samples)

        # Filter out already-processed samples
        new_samples = [s for s in all_samples if str(s["id"]) not in self.processed_ids]

        self.logger.info(
            f"Found {len(all_samples)} total samples, "
            f"{len(new_samples)} new samples to process"
        )

        if not new_samples:
            self.logger.info("No new samples to process")
            return {"samples": [], "relationships": {"similar": {}}}

        # Track which nodes are new in this session
        initial_node_ids = set(self.graph.nodes())

        # Process samples incrementally with progress tracking
        processed_samples = []

        # Simplified processing - add all samples as nodes
        with ProgressTracker(
            total=len(new_samples),
            title="Processing new samples incrementally",
            logger=self.logger,
        ) as tracker:
            for i, sample in enumerate(new_samples):
                # Check time limit
                if self._check_time_limit():
                    elapsed = time.time() - self.start_time
                    stats = self._calculate_progress_stats(i, len(new_samples), elapsed)

                    self.logger.warning(
                        f"Time limit reached ({self.max_runtime_hours}h). "
                        f"Processed {stats['current']}/{stats['total']} samples "
                        f"({stats['percentage']:.1f}%). Saving checkpoint..."
                    )

                    self._save_checkpoint(
                        {"stopped_reason": "time_limit", "progress": stats}
                    )
                    break

                # Add sample to graph (simplified - no similar sounds)
                self._add_sample_to_graph(sample, include_similar=False)
                processed_samples.append(sample)

                # Mark as processed
                self.processed_ids.add(str(sample["id"]))

                # Periodic checkpoint save
                if (i + 1) % self.checkpoint_interval == 0:
                    elapsed = time.time() - self.start_time
                    stats = self._calculate_progress_stats(
                        i + 1, len(new_samples), elapsed
                    )

                    # Progress already shown by ProgressTracker - no need to log
                    self._save_checkpoint({"progress": stats})

                tracker.update(i + 1)

        # Determine new nodes added in this session
        final_node_ids = set(self.graph.nodes())
        new_node_ids = final_node_ids - initial_node_ids

        # Generate edges if requested (NO API REQUESTS - uses existing graph data)
        edge_stats = {}

        if processed_samples and (
            include_user_edges or include_pack_edges or include_tag_edges
        ):
            self.logger.info(
                "Generating edges from existing graph data (0 API requests)..."
            )
            # Note: Edge generation methods should be implemented in earlier tasks
            # For now, we'll call the existing batch edge generation if available
            try:
                if hasattr(self, "_generate_all_edges"):
                    edge_stats = self._generate_all_edges(
                        include_user=include_user_edges,
                        include_pack=include_pack_edges,
                        include_tag=include_tag_edges,
                        tag_threshold=tag_similarity_threshold,
                    )
                else:
                    # Fallback to existing methods
                    if include_user_edges or include_pack_edges:
                        # Use existing batch edge generation
                        usernames = set()
                        pack_names = set()

                        for node_id in self.graph.nodes():
                            node_data = self.graph.nodes[node_id]
                            if include_user_edges:
                                username = node_data.get("user") or node_data.get(
                                    "username"
                                )
                                if username:
                                    usernames.add(username)
                            if include_pack_edges:
                                pack = node_data.get("pack")
                                if pack:
                                    pack_names.add(pack)

                        if include_user_edges and usernames:
                            user_edges = self._add_user_edges_batch()
                            edge_stats["user_edges_added"] = user_edges

                        if include_pack_edges and pack_names:
                            pack_edges = self._add_pack_edges_batch()
                            edge_stats["pack_edges_added"] = pack_edges

                    if include_tag_edges:
                        # Check if threshold changed - if so, regenerate all edges
                        threshold_changed = (
                            hasattr(self, "_last_tag_threshold")
                            and self._last_tag_threshold is not None
                            and self._last_tag_threshold != tag_similarity_threshold
                        )

                        if threshold_changed:
                            self.logger.info(
                                f"Tag similarity threshold changed "
                                f"({self._last_tag_threshold} â†’ {tag_similarity_threshold}), "
                                f"regenerating all tag edges"
                            )
                            tag_edges = self._add_tag_edges_batch(
                                tag_similarity_threshold
                            )
                        else:
                            # Use incremental generation for new nodes
                            tag_edges = self._add_tag_edges_incremental(
                                tag_similarity_threshold, new_node_ids
                            )

                        edge_stats["tag_edges_added"] = tag_edges
                        self.stats["tag_edges_created"] = tag_edges
                        self._last_tag_threshold = tag_similarity_threshold
            except Exception as e:
                self.logger.warning(f"Edge generation failed: {e}")

        # Final checkpoint save
        elapsed = time.time() - self.start_time
        final_stats = self._calculate_progress_stats(
            len(processed_samples), len(new_samples), elapsed
        )

        self._save_checkpoint(
            {"completed": True, "final_stats": final_stats, "edge_stats": edge_stats}
        )

        success_msg = EmojiFormatter.format(
            "success",
            f"Processed {len(processed_samples)} new samples in "
            f"{final_stats['elapsed_minutes']:.1f} minutes",
        )
        self.logger.info(success_msg)

        return {"samples": processed_samples, "edge_stats": edge_stats}

    def _process_samples_recursive(
        self,
        seed_samples: list[dict[str, Any]],
        depth: int,
        max_total_samples: int,
        include_similar: bool,
    ) -> None:
        """
        Process samples recursively using queue-based BFS with two-pass processing.

        This method implements a two-pass strategy to ensure all edges are preserved
        correctly during recursive discovery:

        Pass 1 (Node Discovery):
            Uses a FIFO queue to process samples breadth-first. Each queue item is a
            (sample, depth) tuple. As samples are processed:
            - Nodes are added to the graph immediately
            - Similar sounds are fetched via API
            - Relationships are stored in pending_edges dict (NOT added as edges yet)
            - Unprocessed similar samples are enqueued at depth+1
            - Processing continues until queue is empty or limits are reached

        Pass 2 (Edge Creation):
            After all nodes are discovered, iterates through pending_edges to add edges:
            - Only adds edges where both source and target nodes exist
            - Makes NO additional API calls (uses stored relationships)
            - Filters out self-loops and duplicate edges
            - Logs the number of edges added

        This approach solves the problem where edges were lost because target nodes
        did not exist yet when the source node was processed.

        Args:
            seed_samples: Initial samples to start recursive discovery from
            depth: Maximum depth to recurse (0 = no recursion, 1 = one level, etc.)
            max_total_samples: Maximum total samples to discover across all depths
            include_similar: Whether to fetch similar sounds relationships

        Example:
            # Fetch drum samples with 2 levels of recursion
            seed_samples = [{'id': 12345, 'name': 'kick.wav', ...}]
            loader._process_samples_recursive(
                seed_samples=seed_samples,
                depth=2,
                max_total_samples=100,
                include_similar=True
            )
            # Result: Graph with up to 100 nodes connected by similarity edges
        """
        # Quality thresholds for expansion
        fetch_similar_threshold_downloads = self.config.get(
            "fetch_similar_threshold_downloads", 100
        )
        fetch_similar_threshold_rating = self.config.get(
            "fetch_similar_threshold_rating", 3.5
        )

        # Initialize priority queue with (priority, counter, sample, depth) tuples
        # Priority queue processes highest priority samples first (lower value = higher priority)
        # Counter ensures stable ordering for samples with same priority
        # Priority = negative of calculated score (for max-heap behavior)
        # Tuple format: (priority, counter, sample_dict, depth)
        priority_queue: list[tuple[float, int, dict[str, Any], int]] = []
        counter = 0
        for sample in seed_samples:
            # Calculate intelligent priority score
            priority_score = self.calculate_node_priority(sample)
            priority = -priority_score  # Negative for max-heap behavior
            heapq.heappush(priority_queue, (priority, counter, sample, 0))
            counter += 1

        # Dictionary to store relationships discovered during Pass 1
        # Maps source_id -> [(target_id, similarity_score), ...]
        pending_edges: dict[str, list[tuple[str, float]]] = {}

        # Dormant node tracking
        # Track number of NEW samples discovered during each node expansion
        self.config.get(
            "dormant_penalty_multiplier", self.DEFAULT_DORMANT_PENALTY_MULTIPLIER
        )
        dormant_nodes_count = 0

        # Counter for periodic checkpoint saves
        checkpoint_counter = 0
        start_time = time.time()

        self.logger.info(
            f"Starting recursive processing: depth={depth}, max_total={max_total_samples}, mode={self.max_samples_mode}"
        )
        self.logger.info(
            "Using priority-based processing: most popular samples (by downloads) first"
        )

        # Log mode-specific behavior
        if self.max_samples_mode == "limit":
            self.logger.info(
                EmojiFormatter.format(
                    "progress", f"Limit mode: Will stop at {max_total_samples} samples"
                )
            )
        else:
            self.logger.info(
                EmojiFormatter.format(
                    "progress",
                    "Queue-empty mode: Will continue until priority queue is empty (safety limit: 10000)",
                )
            )

        # ============================================================================
        # PASS 1: NODE DISCOVERY WITH PRIORITY QUEUE
        # ============================================================================
        # Process samples in priority order (most popular first). Each iteration:
        # 1. Dequeues highest priority (sample, depth) tuple
        # 2. Adds the sample as a node to the graph
        # 3. Fetches its similar sounds via API
        # 4. Stores relationships in pending_edges (does NOT add edges yet)
        # 5. Enqueues unprocessed similar samples with their priority
        # This ensures we build a connected graph starting from most useful samples
        # ============================================================================

        while priority_queue:
            # Dequeue next sample to process (highest priority = most popular)
            priority, _, sample, current_depth = heapq.heappop(priority_queue)
            sample_id = str(sample["id"])

            # Skip if already processed (handles duplicate queue entries)
            if sample_id in self.processed_ids:
                continue

            # Check circuit breaker BEFORE making any API calls
            if self._check_circuit_breaker():
                elapsed = time.time() - start_time
                self.logger.info(
                    f"Gracefully stopping after {format_time_duration(elapsed)}, "
                    f"saving checkpoint..."
                )
                self._save_checkpoint(
                    {
                        "stopped_reason": "circuit_breaker",
                        "api_requests_used": self.session_request_count,
                    }
                )
                return

            # Check if time limit has been reached
            if self._check_time_limit():
                elapsed = time.time() - start_time
                self.logger.warning(
                    f"Time limit reached after {format_time_duration(elapsed)}, "
                    f"saving checkpoint..."
                )
                self._save_checkpoint({"stopped_reason": "time_limit"})
                return

            # Check if we've reached the maximum number of samples (mode-dependent)
            if self.max_samples_mode == "limit":
                # Limit mode: Stop at max_total_samples
                if self.graph.number_of_nodes() >= max_total_samples:
                    elapsed = time.time() - start_time
                    self.logger.info(
                        EmojiFormatter.format(
                            "success",
                            f"Reached max_total_samples limit ({max_total_samples}) "
                            f"after {format_time_duration(elapsed)}",
                        )
                    )
                    break
            else:
                # Queue-empty mode: Continue until queue is empty, but enforce safety limit
                safety_limit = 10000
                if self.graph.number_of_nodes() >= safety_limit:
                    elapsed = time.time() - start_time
                    self.logger.warning(
                        EmojiFormatter.format(
                            "warning",
                            f"Reached safety limit ({safety_limit}) in queue-empty mode "
                            f"after {format_time_duration(elapsed)}",
                        )
                    )
                    break

            # Add node to graph (without edges - edges come in Pass 2)
            self._add_node_to_graph(sample)
            self.processed_ids.add(sample_id)
            checkpoint_counter += 1

            # Fetch similar sounds if we haven't reached the depth limit
            # Note: current_depth < depth (not <=) because we want to fetch
            # similar sounds for samples at depth N to discover samples at depth N+1
            if current_depth < depth and include_similar:
                # Apply quality thresholds - only expand from high-quality nodes
                downloads = sample.get("num_downloads", 0)
                avg_rating = sample.get("avg_rating", 0.0)

                # Skip expansion if sample doesn't meet minimum quality thresholds
                if (
                    downloads < fetch_similar_threshold_downloads
                    or avg_rating < fetch_similar_threshold_rating
                ):
                    self.stats["samples_skipped"] = (
                        cast(int, self.stats["samples_skipped"]) + 1
                    )
                    self.logger.debug(
                        f"Skipping expansion for {sample_id}: "
                        f"downloads={downloads} (threshold={fetch_similar_threshold_downloads}), "
                        f"rating={avg_rating:.1f} (threshold={fetch_similar_threshold_rating})"
                    )
                    continue

                self.logger.debug(
                    f"Fetching similar sounds for {sample_id} at depth {current_depth}"
                )
                try:
                    # Track new samples discovered before expansion
                    len(self.processed_ids)

                    # Fetch similar sounds via API
                    similar_list = self._fetch_similar_sounds_for_sample(int(sample_id))

                    # Store relationships for Pass 2 (edge creation)
                    # This is critical: we do NOT add edges here because target
                    # nodes may not exist yet. We'll add edges in Pass 2 after
                    # all nodes have been discovered.
                    if similar_list:
                        pending_edges[sample_id] = similar_list
                        self.logger.debug(
                            f"Stored {len(similar_list)} relationships for {sample_id}"
                        )
                    else:
                        self.logger.debug(f"No similar sounds found for {sample_id}")

                    # Track new samples discovered during expansion
                    new_samples_discovered = 0

                    # Enqueue unprocessed similar samples for next depth level
                    # Each similar sample will be processed at depth = current_depth + 1
                    # Priority by downloads ensures we explore most popular samples first
                    for similar_id, _score in similar_list:
                        similar_id_str = str(similar_id)

                        # Only enqueue if:
                        # 1. Not already processed (avoid duplicate work)
                        # 2. Within sample limit (respect max_total_samples)
                        if (
                            similar_id_str not in self.processed_ids
                            and self.graph.number_of_nodes() < max_total_samples
                        ):
                            try:
                                # Fetch metadata for the similar sample (return_sound=False by default)
                                similar_sample = cast(
                                    dict[str, Any],
                                    self._fetch_sample_metadata(similar_id),
                                )

                                # Count as new sample discovered
                                new_samples_discovered += 1

                                # Calculate intelligent priority score
                                similar_priority_score = self.calculate_node_priority(
                                    similar_sample
                                )
                                similar_priority = (
                                    -similar_priority_score
                                )  # Negative for max-heap

                                # Enqueue at next depth level with priority
                                heapq.heappush(
                                    priority_queue,
                                    (
                                        similar_priority,
                                        counter,
                                        similar_sample,
                                        current_depth + 1,
                                    ),
                                )
                                counter += 1
                            except Exception as e:
                                # Log but don't fail - continue with other samples
                                self.logger.debug(
                                    f"Could not fetch metadata for {similar_id}: {e}"
                                )

                    # Track expansion statistics
                    cast(list, self.stats["new_samples_per_expansion"]).append(
                        new_samples_discovered
                    )

                    # Dormant node detection: mark node as dormant if it yielded 0 new samples
                    if new_samples_discovered == 0:
                        # Mark node as dormant in graph
                        if sample_id in self.graph:
                            now = datetime.now(timezone.utc).isoformat()
                            self.graph.nodes[sample_id]["is_dormant"] = True
                            self.graph.nodes[sample_id]["dormant_since"] = now

                            # Recalculate priority score with dormant penalty
                            # This will apply the dormant_penalty_multiplier (default: 0.01)
                            updated_priority = self.calculate_node_priority(sample)
                            self.graph.nodes[sample_id]["priority_score"] = (
                                updated_priority
                            )

                            dormant_nodes_count += 1
                            self.stats["dormant_nodes_identified"] = (
                                cast(int, self.stats["dormant_nodes_identified"]) + 1
                            )

                            self.logger.debug(
                                f"Node {sample_id} marked as dormant (0 new samples discovered), "
                                f"priority score updated to {updated_priority:.2f}"
                            )

                except Exception as e:
                    # Log warning but continue processing other samples
                    self.logger.warning(
                        f"Failed to fetch similar sounds for {sample_id}: {e}"
                    )

            # Save checkpoint periodically to enable recovery from interruptions
            if checkpoint_counter >= self.checkpoint_interval:
                elapsed = time.time() - start_time
                self._save_checkpoint(
                    {
                        "depth": current_depth,
                        "pending_edges_count": len(pending_edges),
                        "elapsed": format_time_duration(elapsed),
                    }
                )
                checkpoint_counter = 0

        # ============================================================================
        # PASS 2: EDGE CREATION FROM STORED RELATIONSHIPS
        # ============================================================================
        # Now that all nodes have been discovered, we can safely add edges between
        # them. This pass:
        # 1. Iterates through pending_edges dictionary (from Pass 1)
        # 2. Adds edges only where both source and target nodes exist
        # 3. Makes NO additional API calls (uses stored relationships)
        # 4. Filters out self-loops and duplicate edges
        #
        # This solves the edge preservation problem: in Pass 1, when we discovered
        # that sample A is similar to sample B, sample B might not have been
        # discovered yet. Now that all nodes exist, we can add all the edges.
        # ============================================================================

        elapsed = time.time() - start_time
        self.logger.info(
            f"Node discovery complete in {format_time_duration(elapsed)}. "
            f"Adding edges between {len(self.graph.nodes())} discovered nodes..."
        )
        self.logger.info(f"Pending edges dictionary has {len(pending_edges)} entries")

        edge_count = 0
        edge_checkpoint_counter = 0
        edge_checkpoint_interval = max(
            10, len(pending_edges) // 10
        )  # Save every 10% or every 10 sources

        self.logger.info(
            f"Pass 2: Will save checkpoints every {edge_checkpoint_interval} sources "
            f"(~{100 / max(1, len(pending_edges) // edge_checkpoint_interval):.0f}% intervals)"
        )

        # Skip Pass 2 if no pending edges
        if len(pending_edges) == 0:
            self.logger.info("No pending edges to process, skipping Pass 2")
            return

        # Use progress tracker for Pass 2
        with ProgressTracker(
            total=len(pending_edges),
            title="Pass 2: Adding edges from stored relationships",
            logger=self.logger,
        ) as tracker:
            # Iterate through all stored relationships from Pass 1
            for idx, (source_id, similar_list) in enumerate(pending_edges.items()):
                # Verify source node exists (defensive programming)
                # It should always exist, but we check to be safe
                if source_id not in self.graph:
                    continue

                # Add edges to all similar samples
                for target_id, score in similar_list:
                    target_id_str = str(target_id)

                    # Add edge only if:
                    # 1. Target node exists in graph (may not if it was outside limits)
                    # 2. Edge doesn't already exist (avoid duplicates)
                    # 3. Not a self-loop (source != target)
                    if (
                        target_id_str in self.graph
                        and not self.graph.has_edge(source_id, target_id_str)
                        and source_id != target_id_str
                    ):
                        self.graph.add_edge(
                            source_id, target_id_str, type="similar", weight=score
                        )
                        edge_count += 1

                # Periodic checkpoint during Pass 2 to preserve edge data
                edge_checkpoint_counter += 1
                if edge_checkpoint_counter >= edge_checkpoint_interval:
                    progress_pct = ((idx + 1) / len(pending_edges)) * 100
                    self.logger.info(
                        f"Pass 2 checkpoint: {edge_count} edges added so far "
                        f"({progress_pct:.1f}% complete)"
                    )
                    self._save_checkpoint(
                        {
                            "pass2_progress": progress_pct,
                            "edges_added_so_far": edge_count,
                            "sources_processed": idx + 1,
                            "total_sources": len(pending_edges),
                        }
                    )
                    edge_checkpoint_counter = 0

                # Update progress tracker
                tracker.update(idx + 1)

        total_elapsed = time.time() - start_time
        self.logger.info(
            f"Added {edge_count} edges to graph in {format_time_duration(total_elapsed)}"
        )

        # Log dormant node statistics
        if dormant_nodes_count > 0:
            dormant_msg = EmojiFormatter.format(
                "info",
                f"Dormant nodes detected: {dormant_nodes_count} nodes yielded 0 new samples",
            )
            self.logger.info(dormant_msg)

        # Log complete API efficiency statistics
        self.log_api_efficiency_stats()

        # Final checkpoint with completion metadata
        self._save_checkpoint(
            {
                "completed": True,
                "edges_added": edge_count,
                "dormant_nodes_count": dormant_nodes_count,
                "total_elapsed": format_time_duration(total_elapsed),
                "api_stats": self.stats,
            }
        )

    def calculate_node_priority(self, sample: dict[str, Any]) -> float:
        """
        Calculate expansion priority for a node using weighted formula.

        Priority = (w1 * Downloads) + (w2 * Degree) - (w3 * Age_in_days)

        Dormant nodes receive a heavy penalty multiplier (default: 0.01)

        Higher priority = more valuable to expand from

        Args:
            sample: Sample dictionary with metadata

        Returns:
            Priority score (higher = better)
        """
        # Configurable weights
        w1 = self.config.get(
            "priority_weight_downloads", self.DEFAULT_PRIORITY_WEIGHT_DOWNLOADS
        )
        w2 = self.config.get(
            "priority_weight_degree", self.DEFAULT_PRIORITY_WEIGHT_DEGREE
        )
        w3 = self.config.get("priority_weight_age", self.DEFAULT_PRIORITY_WEIGHT_AGE)
        dormant_penalty = self.config.get(
            "dormant_penalty_multiplier", self.DEFAULT_DORMANT_PENALTY_MULTIPLIER
        )

        # Extract metrics
        downloads = sample.get("num_downloads", 0)

        # Calculate degree (number of connections in graph)
        sample_id = str(sample["id"])
        degree: int = 0
        is_dormant = False

        if sample_id in self.graph:
            degree = self.graph.degree(sample_id)  # type: ignore[operator]
            is_dormant = self.graph.nodes[sample_id].get("is_dormant", False)

        # Calculate age in days since collection
        age_days = 0
        collected_at = sample.get("collected_at")
        if collected_at:
            try:
                collected_time = datetime.fromisoformat(
                    collected_at.replace("Z", "+00:00")
                )
                age_days = (datetime.now(timezone.utc) - collected_time).days
            except (ValueError, AttributeError):
                age_days = 0

        # Calculate base priority score
        priority = (w1 * downloads) + (w2 * degree) - (w3 * age_days)

        # Apply dormant penalty (heavily deprioritize dormant nodes)
        if is_dormant:
            priority *= dormant_penalty

        return priority

    def _increment_request_count(self) -> None:
        """Increment API request counter for circuit breaker."""
        self.session_request_count += 1

        # Log progress at milestones
        if self.session_request_count % 100 == 0:
            remaining = self.max_requests - self.session_request_count
            self.logger.info(
                f"API requests: {self.session_request_count}/{self.max_requests} "
                f"({remaining} remaining)"
            )

    def _check_circuit_breaker(self) -> bool:
        """
        Check if API quota circuit breaker should trigger.

        Returns:
            True if circuit breaker triggered (limit reached), False otherwise
        """
        if self.session_request_count >= self.max_requests:
            warning_msg = EmojiFormatter.format(
                "warning",
                f"Circuit breaker triggered: {self.session_request_count} requests "
                f"reached limit of {self.max_requests}",
            )
            self.logger.warning(warning_msg)
            return True
        return False

    def handle_throttling(
        self, attempt: int = 0, max_attempts: Optional[int] = None
    ) -> bool:
        """
        Handle 429 throttling responses with exponential backoff and jitter.

        Args:
            attempt: Current retry attempt number
            max_attempts: Maximum retry attempts (default: DEFAULT_MAX_THROTTLE_ATTEMPTS)

        Uses exponential backoff with random jitter to prevent thundering herd
        problem when multiple clients retry simultaneously.

        Jitter formula: delay = base_delay + random.uniform(0, 0.5 * base_delay)
        This adds 0-50% random variation to prevent synchronized retries.

        Args:
            attempt: Current attempt number (0-indexed)
            max_attempts: Maximum number of retry attempts

        Returns:
            True if should retry, False if max attempts reached
        """
        import random

        # Apply default if not specified
        if max_attempts is None:
            max_attempts = self.DEFAULT_MAX_THROTTLE_ATTEMPTS

        if attempt >= max_attempts:
            error_msg = EmojiFormatter.format(
                "error", f"Max throttling retries ({max_attempts}) reached"
            )
            self.logger.error(error_msg)
            self.stats["failed_requests"] = cast(int, self.stats["failed_requests"]) + 1
            return False

        # Exponential backoff: 2s, 4s, 8s
        base_delay = 2 ** (attempt + 1)

        # Add random jitter (0 to 50% of base delay) to prevent thundering herd
        jitter = random.uniform(0, 0.5 * base_delay)
        actual_delay = base_delay + jitter

        warning_msg = EmojiFormatter.format(
            "warning",
            f"Throttled (429) - attempt {attempt + 1}/{max_attempts}, "
            f"waiting {actual_delay:.2f}s (base: {base_delay}s + jitter: {jitter:.2f}s)...",
        )
        self.logger.warning(warning_msg)

        self.stats["throttle_retries"] = cast(int, self.stats["throttle_retries"]) + 1
        time.sleep(actual_delay)
        return True

    def log_api_efficiency_stats(self) -> None:
        """
        Log complete API efficiency statistics.

        Logs:
        - API requests saved (cache hits)
        - Samples skipped (quality thresholds)
        - Total requests used vs limit
        - Frontier expansion stats
        - Batch expansion stats
        - Error statistics
        """
        # Calculate cache efficiency
        total_cache_ops = self._cache_hits + self._cache_misses
        cache_hit_ratio = (
            (self._cache_hits / total_cache_ops * 100) if total_cache_ops > 0 else 0
        )

        # Log API efficiency
        efficiency_msg = EmojiFormatter.format("success", "API Efficiency Statistics:")
        self.logger.info(efficiency_msg)
        self.logger.info(
            f"  Total API requests: {self.session_request_count}/{self.max_requests}"
        )
        self.logger.info(f"  Requests saved (cache): {self._cache_hits}")
        self.logger.info(f"  Cache hit ratio: {cache_hit_ratio:.1f}%")
        self.logger.info(
            f"  Samples skipped (quality): {self.stats['samples_skipped']}"
        )

        # Log frontier expansion stats
        expansion_list = cast(list, self.stats["new_samples_per_expansion"])
        if expansion_list:
            avg_new_samples = sum(expansion_list) / len(expansion_list)
            self.logger.info(f"  Avg new samples per expansion: {avg_new_samples:.1f}")

        self.logger.info(
            f"  Dormant nodes identified: {self.stats['dormant_nodes_identified']}"
        )

        # Log batch expansion stats
        if (
            cast(int, self.stats["user_edges_created"]) > 0
            or cast(int, self.stats["pack_edges_created"]) > 0
        ):
            batch_msg = EmojiFormatter.format("info", "Batch Expansion Statistics:")
            self.logger.info(batch_msg)
            self.logger.info(
                f"  User edges created: {self.stats['user_edges_created']}"
            )
            self.logger.info(
                f"  Pack edges created: {self.stats['pack_edges_created']}"
            )

        # Log error statistics
        if (
            cast(int, self.stats["throttle_retries"]) > 0
            or cast(int, self.stats["failed_requests"]) > 0
        ):
            error_msg = EmojiFormatter.format("warning", "Error Statistics:")
            self.logger.info(error_msg)
            self.logger.info(
                f"  Throttle retries (429): {self.stats['throttle_retries']}"
            )
            self.logger.info(f"  Failed requests: {self.stats['failed_requests']}")

    def handle_api_error(self, error: Exception, context: str) -> str:
        """
        Handle specific API error codes with appropriate actions.

        Error handling strategy:
        - 400: Log and skip (bad request)
        - 401: Fatal error (invalid credentials)
        - 403: Fatal error (forbidden)
        - 404: Mark as deleted, continue
        - 429: Retry with exponential backoff
        - 5xx: Retry with exponential backoff

        Args:
            error: Exception from API call
            context: Context string describing what was being attempted

        Returns:
            Action to take: 'skip', 'fatal', 'deleted', 'retry'
        """
        error_str = str(error).lower()

        # Check for specific error codes
        if "400" in error_str or "bad request" in error_str:
            error_msg = EmojiFormatter.format(
                "error", f"Bad request (400) in {context}: {error}"
            )
            self.logger.error(error_msg)
            return "skip"

        elif "401" in error_str or "unauthorized" in error_str:
            error_msg = EmojiFormatter.format(
                "error", f"FATAL: Invalid credentials (401) in {context}"
            )
            self.logger.error(error_msg)
            return "fatal"

        elif "403" in error_str or "forbidden" in error_str:
            error_msg = EmojiFormatter.format(
                "error", f"FATAL: Forbidden (403) in {context}"
            )
            self.logger.error(error_msg)
            return "fatal"

        elif "404" in error_str or "not found" in error_str:
            info_msg = EmojiFormatter.format(
                "info", f"Resource not found (404) in {context} - marking as deleted"
            )
            self.logger.info(info_msg)
            return "deleted"

        elif "429" in error_str or "throttl" in error_str or "rate limit" in error_str:
            warning_msg = EmojiFormatter.format(
                "warning", f"Rate limited (429) in {context} - will retry"
            )
            self.logger.warning(warning_msg)
            return "retry"

        elif "5" in error_str[:3]:  # 5xx errors
            warning_msg = EmojiFormatter.format(
                "warning", f"Server error (5xx) in {context} - will retry"
            )
            self.logger.warning(warning_msg)
            return "retry"

        else:
            # Unknown error - log and skip
            error_msg = EmojiFormatter.format(
                "error", f"Unknown error in {context}: {error}"
            )
            self.logger.error(error_msg)
            return "skip"

    def _add_batch_user_pack_edges(self) -> dict[str, int]:
        """
        Add user, pack, and tag relationship edges using batch filtering.

        This method aggregates unique usernames and pack names from collected samples,
        then uses batch text search filtering to discover relationships efficiently.
        Also generates tag-based edges using Jaccard similarity.

        Returns:
            Dictionary with statistics: user_edges_added, pack_edges_added, tag_edges_added
        """
        # Check if user/pack/tag edges are enabled
        include_user_edges = self.config.get("include_user_edges", True)
        include_pack_edges = self.config.get("include_pack_edges", True)
        include_tag_edges = self.config.get("include_tag_edges", False)

        if not include_user_edges and not include_pack_edges and not include_tag_edges:
            return {"user_edges_added": 0, "pack_edges_added": 0, "tag_edges_added": 0}

        stats = {"user_edges_added": 0, "pack_edges_added": 0, "tag_edges_added": 0}

        # Aggregate unique usernames and pack names from graph
        usernames = set()
        pack_names = set()

        for node_id in self.graph.nodes():
            node_data = self.graph.nodes[node_id]

            if include_user_edges:
                username = node_data.get("user") or node_data.get("username")
                if username:
                    usernames.add(username)

            if include_pack_edges:
                pack = node_data.get("pack")
                if pack:
                    # Extract pack name from URI if needed
                    if isinstance(pack, str) and "/" in pack:
                        pack_name = pack.split("/")[-2]  # Extract from URI
                    else:
                        pack_name = pack
                    if pack_name:
                        pack_names.add(pack_name)

        self.logger.info(
            f"Batch edge creation: {len(usernames)} unique users, "
            f"{len(pack_names)} unique packs"
        )

        # Add user relationship edges
        if include_user_edges and usernames:
            user_edges = self._add_user_edges_batch()
            stats["user_edges_added"] = user_edges
            self.stats["user_edges_created"] = user_edges

        # Add pack relationship edges
        if include_pack_edges and pack_names:
            pack_edges = self._add_pack_edges_batch()
            stats["pack_edges_added"] = pack_edges
            self.stats["pack_edges_created"] = pack_edges

        # Add tag similarity edges
        if include_tag_edges:
            tag_similarity_threshold = self.config.get(
                "tag_similarity_threshold", self.DEFAULT_TAG_SIMILARITY_THRESHOLD
            )
            tag_edges = self._add_tag_edges_batch(tag_similarity_threshold)
            stats["tag_edges_added"] = tag_edges
            self.stats["tag_edges_created"] = tag_edges

        return stats

    def _add_user_edges_batch(self) -> int:
        """
        Add edges between samples by the same user using existing graph data.

        NO API REQUESTS - uses only data already in the graph nodes.

        Returns:
            Number of edges added
        """
        edge_count = 0

        # Group samples by username from existing graph data
        samples_by_user: dict[str, list[str]] = {}

        for node_id in self.graph.nodes():
            node_data = self.graph.nodes[node_id]
            username = node_data.get("username")

            if username:
                if username not in samples_by_user:
                    samples_by_user[username] = []
                samples_by_user[username].append(node_id)

        # Add edges between samples by same user
        for _username, sample_ids in samples_by_user.items():
            # Only create edges if user has multiple samples
            if len(sample_ids) < 2:
                continue

            # Add edges between all pairs
            for j in range(len(sample_ids)):
                for k in range(j + 1, len(sample_ids)):
                    source = sample_ids[j]
                    target = sample_ids[k]

                    # Add bidirectional edges
                    if not self.graph.has_edge(source, target):
                        self.graph.add_edge(
                            source, target, type="by_same_user", weight=1.0
                        )
                        edge_count += 1
                    if not self.graph.has_edge(target, source):
                        self.graph.add_edge(
                            target, source, type="by_same_user", weight=1.0
                        )
                        edge_count += 1

        if edge_count > 0:
            self.logger.info(
                f"âœ… Added {edge_count} user relationship edges (0 API requests)"
            )

        return edge_count

    def _add_pack_edges_batch(self) -> int:
        """
        Add edges between samples in the same pack using existing graph data.

        NO API REQUESTS - uses only data already in the graph nodes.

        Returns:
            Number of edges added
        """
        edge_count = 0

        # Group samples by pack from existing graph data
        samples_by_pack: dict[str, list[str]] = {}

        for node_id in self.graph.nodes():
            node_data = self.graph.nodes[node_id]
            pack = node_data.get("pack")

            if pack:
                # Extract pack name from URI if needed
                if isinstance(pack, str) and "/" in pack:
                    pack_name = pack.split("/")[-2]
                else:
                    pack_name = str(pack)

                if pack_name:
                    if pack_name not in samples_by_pack:
                        samples_by_pack[pack_name] = []
                    samples_by_pack[pack_name].append(node_id)

        # Add edges between samples in same pack
        for _pack_name, sample_ids in samples_by_pack.items():
            # Only create edges if pack has multiple samples
            if len(sample_ids) < 2:
                continue

            # Add edges between all pairs
            for j in range(len(sample_ids)):
                for k in range(j + 1, len(sample_ids)):
                    source = sample_ids[j]
                    target = sample_ids[k]

                    # Add bidirectional edges
                    if not self.graph.has_edge(source, target):
                        self.graph.add_edge(
                            source, target, type="in_same_pack", weight=1.0
                        )
                        edge_count += 1
                    if not self.graph.has_edge(target, source):
                        self.graph.add_edge(
                            target, source, type="in_same_pack", weight=1.0
                        )
                        edge_count += 1

        if edge_count > 0:
            self.logger.info(
                f"âœ… Added {edge_count} pack relationship edges (0 API requests)"
            )

        return edge_count

    def _add_tag_edges_batch(self, similarity_threshold: Optional[float] = None) -> int:
        """
        Add edges between samples with similar tags using Jaccard similarity.

        This is a convenience wrapper around _add_tag_edges_incremental that
        treats all nodes as "new" to regenerate all edges. Use this when:
        - Threshold has changed
        - Full regeneration is needed
        - Initial edge generation

        For incremental updates, use _add_tag_edges_incremental directly.

        NO API REQUESTS - uses only data already in the graph nodes.

        Calculates Jaccard similarity between sample tag sets:
        similarity = |tags1 âˆ© tags2| / |tags1 âˆª tags2|

        Adds edges for samples with similarity above threshold.

        Args:
            similarity_threshold: Minimum Jaccard similarity to create edge
                                 (default: DEFAULT_TAG_SIMILARITY_THRESHOLD)

        Returns:
            Number of edges added
        """
        # Apply default if not specified
        if similarity_threshold is None:
            similarity_threshold = self.DEFAULT_TAG_SIMILARITY_THRESHOLD

        # Treat all nodes as "new" to regenerate all edges
        all_node_ids = set(self.graph.nodes())

        if len(all_node_ids) < 2:
            self.logger.info(
                "Not enough samples with tags for tag-based edge generation"
            )
            return 0

        self.logger.info(
            f"Full tag edge regeneration for {len(all_node_ids)} nodes "
            f"(threshold: {similarity_threshold})"
        )

        # Use incremental method with all nodes marked as "new"
        return self._add_tag_edges_incremental(similarity_threshold, all_node_ids)

    def _add_tag_edges_incremental(
        self,
        similarity_threshold: float,
        new_node_ids: Optional[set[str]] = None,
    ) -> int:
        """
        Add tag similarity edges incrementally.

        Only checks:
        - New node â†” New node pairs
        - New node â†” Existing node pairs

        Skips existing node â†” existing node pairs (already processed).

        Args:
            similarity_threshold: Minimum Jaccard similarity
            new_node_ids: Set of newly added node IDs (if None, regenerate all)

        Returns:
            Number of edges added
        """
        edge_count = 0

        # Get all nodes with tags
        all_nodes_with_tags = []
        new_nodes_with_tags = []
        existing_nodes_with_tags = []

        for node_id in self.graph.nodes():
            node_data = self.graph.nodes[node_id]
            tags = node_data.get("tags", [])

            if tags and len(tags) > 0:
                tag_set = set(tags) if isinstance(tags, list) else {tags}
                node_tuple = (node_id, tag_set)
                all_nodes_with_tags.append(node_tuple)

                if new_node_ids and node_id in new_node_ids:
                    new_nodes_with_tags.append(node_tuple)
                else:
                    existing_nodes_with_tags.append(node_tuple)

        # If no new nodes specified, nothing to do
        if not new_node_ids:
            self.logger.info("No new nodes specified, skipping edge generation")
            return 0

        # If no nodes have tags, nothing to do
        if len(new_nodes_with_tags) == 0 and len(existing_nodes_with_tags) == 0:
            self.logger.info("No nodes with tags found")
            return 0

        # Determine if this is full regeneration or incremental
        is_full_regen = len(existing_nodes_with_tags) == 0

        if is_full_regen:
            self.logger.info(
                f"Generating tag edges for {len(new_nodes_with_tags)} nodes "
                f"(threshold: {similarity_threshold})"
            )
        else:
            self.logger.info(
                f"Incremental tag edge generation: "
                f"{len(new_nodes_with_tags)} new nodes, "
                f"{len(existing_nodes_with_tags)} existing nodes "
                f"(threshold: {similarity_threshold})"
            )

        # 1. Check new node â†” new node pairs
        for i in range(len(new_nodes_with_tags)):
            node1_id, tags1 = new_nodes_with_tags[i]

            for j in range(i + 1, len(new_nodes_with_tags)):
                node2_id, tags2 = new_nodes_with_tags[j]

                # Calculate Jaccard similarity
                intersection = len(tags1 & tags2)
                union = len(tags1 | tags2)
                similarity = intersection / union if union > 0 else 0

                if similarity >= similarity_threshold:
                    # Add bidirectional edges
                    if not self.graph.has_edge(node1_id, node2_id):
                        self.graph.add_edge(
                            node1_id, node2_id, type="similar_tags", weight=similarity
                        )
                        edge_count += 1
                    if not self.graph.has_edge(node2_id, node1_id):
                        self.graph.add_edge(
                            node2_id, node1_id, type="similar_tags", weight=similarity
                        )
                        edge_count += 1

        # 2. Check new node â†” existing node pairs
        for new_node_id, new_tags in new_nodes_with_tags:
            for existing_node_id, existing_tags in existing_nodes_with_tags:
                # Calculate Jaccard similarity
                intersection = len(new_tags & existing_tags)
                union = len(new_tags | existing_tags)
                similarity = intersection / union if union > 0 else 0

                if similarity >= similarity_threshold:
                    # Add bidirectional edges
                    if not self.graph.has_edge(new_node_id, existing_node_id):
                        self.graph.add_edge(
                            new_node_id,
                            existing_node_id,
                            type="similar_tags",
                            weight=similarity,
                        )
                        edge_count += 1
                    if not self.graph.has_edge(existing_node_id, new_node_id):
                        self.graph.add_edge(
                            existing_node_id,
                            new_node_id,
                            type="similar_tags",
                            weight=similarity,
                        )
                        edge_count += 1

        if edge_count > 0:
            self.logger.info(
                f"âœ… Added {edge_count} tag similarity edges incrementally "
                f"(threshold: {similarity_threshold}, 0 API requests)"
            )

        return edge_count

    def _generate_all_edges(
        self,
        include_user: bool = True,
        include_pack: bool = True,
        include_tag: bool = False,
        tag_threshold: float = 0.3,
    ) -> dict[str, int]:
        """
        Generate all edge types from existing graph data.

        Wrapper method that calls individual edge generation methods based on flags.
        NO API REQUESTS - all methods work from existing graph node data.

        This method is designed for validation/visualization pipelines where edges
        need to be generated after data collection is complete.

        Args:
            include_user: Create edges between samples by same user
            include_pack: Create edges between samples in same pack
            include_tag: Create edges based on tag similarity
            tag_threshold: Minimum Jaccard similarity for tag edges (default: 0.15)

        Returns:
            Dictionary with edge counts by type:
                - user_edges: Number of user relationship edges added
                - pack_edges: Number of pack relationship edges added
                - tag_edges: Number of tag similarity edges added
                - total_edges: Total number of edges added

        Example:
            # Generate all edge types
            stats = loader._generate_all_edges(
                include_user=True,
                include_pack=True,
                include_tag=True,
                tag_threshold=0.3
            )
            # Returns: {'user_edges': 150, 'pack_edges': 75, 'tag_edges': 200, 'total_edges': 425}
        """
        self.logger.info(
            f"Generating edges from existing graph data: "
            f"user={include_user}, pack={include_pack}, tag={include_tag}"
        )

        edge_stats = {
            "user_edges": 0,
            "pack_edges": 0,
            "tag_edges": 0,
            "total_edges": 0,
        }

        # Generate user edges if requested
        if include_user:
            user_edges = self._add_user_edges_batch()
            edge_stats["user_edges"] = user_edges
            self.stats["user_edges_created"] = user_edges

        # Generate pack edges if requested
        if include_pack:
            pack_edges = self._add_pack_edges_batch()
            edge_stats["pack_edges"] = pack_edges
            self.stats["pack_edges_created"] = pack_edges

        # Generate tag edges if requested
        if include_tag:
            tag_edges = self._add_tag_edges_batch(tag_threshold)
            edge_stats["tag_edges"] = tag_edges
            self.stats["tag_edges_created"] = tag_edges

        # Calculate total edges added
        edge_stats["total_edges"] = (
            edge_stats["user_edges"]
            + edge_stats["pack_edges"]
            + edge_stats["tag_edges"]
        )

        self.logger.info(
            f"Edge generation complete: {edge_stats['total_edges']} total edges added "
            f"(user: {edge_stats['user_edges']}, pack: {edge_stats['pack_edges']}, "
            f"tag: {edge_stats['tag_edges']}) - 0 API requests"
        )

        return edge_stats

    def _validate_sample_filesize(self, sample: dict[str, Any]) -> bool:
        """
        Validate sample has non-zero filesize.

        Args:
            sample: Sample dictionary with metadata

        Returns:
            True if sample is valid, False if it should be skipped
        """
        sample_id = str(sample["id"])
        filesize = sample.get("filesize", 0)

        if filesize == 0:
            self.logger.warning(
                f"Skipping sample {sample_id} ({sample.get('name', 'unknown')}): "
                f"invalid filesize (0 bytes)"
            )
            # Don't raise - just skip this sample
            return False
        return True

    def _add_node_to_graph(self, sample: dict[str, Any]) -> None:
        """
        Add a single sample node to the graph without edges.

        Calculates and stores priority_score for SQL-based seed selection.

        Args:
            sample: Sample dictionary with metadata
        """
        sample_id = str(sample["id"])

        # Validate filesize - skip empty files
        if not self._validate_sample_filesize(sample):
            return  # Skip this sample

        # Add node if not already in graph
        if sample_id not in self.graph:
            now = datetime.now(timezone.utc).isoformat()

            # Calculate priority score for this node
            priority_score = self.calculate_node_priority(sample)

            self.graph.add_node(
                sample_id,
                # Basic metadata
                name=sample["name"],
                tags=sample.get("tags", []),
                description=sample.get("description", ""),
                duration=sample.get("duration", 0),
                # User and pack relationships (for edge generation)
                username=sample.get("username", ""),
                pack=sample.get("pack", ""),  # Pack URI or empty string
                # License and attribution (LEGAL REQUIREMENT)
                license=sample.get("license", ""),
                created=sample.get("created", ""),  # Upload timestamp
                url=sample.get("url", ""),  # Freesound website URL (for attribution)
                # Sound taxonomy (Broad Sound Taxonomy)
                category=sample.get("category", []),  # [category, subcategory]
                category_code=sample.get("category_code", ""),  # e.g. "fx-a"
                category_is_user_provided=sample.get(
                    "category_is_user_provided", False
                ),
                # Technical audio properties
                file_type=sample.get("type", ""),  # File type (wav, mp3, ogg, etc.)
                channels=sample.get("channels", 0),  # Mono=1, Stereo=2
                filesize=sample.get("filesize", 0),  # Bytes
                samplerate=sample.get("samplerate", 0),  # Hz (e.g. 44100, 48000)
                # URLs and media assets - extract uploader_id for space efficiency (~200 bytes â†’ ~7 bytes)
                uploader_id=self._extract_uploader_id(sample.get("previews", {})),
                images=sample.get("images", {}),  # Waveform and spectrogram URLs
                # Note: download and analysis_files can be fetched on-demand via API
                # Engagement and quality metrics
                num_downloads=sample.get("num_downloads", 0),
                num_ratings=sample.get("num_ratings", 0),  # Sample size for avg_rating
                avg_rating=sample.get("avg_rating", 0.0),  # 0-5 scale
                num_comments=sample.get("num_comments", 0),  # Community engagement
                # Geographic metadata
                geotag=sample.get("geotag", ""),  # "lat lon" format
                # Internal metadata
                collected_at=now,
                # Validation history timestamps (ISO format)
                last_existence_check_at=None,  # When we last verified sample exists
                last_metadata_update_at=now,  # When we last refreshed metadata
                # Priority scoring for SQL-based seed selection
                priority_score=priority_score,
                is_dormant=False,
            )

    def _add_sample_to_graph(
        self, sample: dict[str, Any], include_similar: bool = True
    ) -> None:
        """
        Add a single sample to the graph with relationships.

        Calculates and stores priority_score for SQL-based seed selection.

        Args:
            sample: Sample dictionary with metadata
            include_similar: Whether to fetch and add similar sounds
        """
        # Use the shared node creation logic
        self._add_node_to_graph(sample)

        sample_id = str(sample["id"])

        # If node wasn't added (e.g., invalid filesize), return early
        if sample_id not in self.graph:
            return

        # Fetch and add similar sounds relationships
        if include_similar:
            try:
                similar_list = self._fetch_similar_sounds_for_sample(int(sample["id"]))

                for target_id, similarity_score in similar_list:
                    target_node = str(target_id)

                    # Only add edge if target exists in graph
                    if target_node in self.graph:
                        self.graph.add_edge(
                            sample_id,
                            target_node,
                            type="similar",
                            weight=similarity_score,
                        )

            except Exception as e:
                self.logger.warning(
                    f"Failed to add similar sounds for {sample_id}: {e}"
                )

    def build_graph(self, data: dict[str, Any]) -> nx.DiGraph:
        """
        Return the incrementally-built graph.

        Overrides parent method to return the graph that has been built
        incrementally during fetch_data().

        Args:
            data: Dictionary from fetch_data() (not used in incremental mode)

        Returns:
            The incrementally-built NetworkX DiGraph
        """
        success_msg = EmojiFormatter.format(
            "success",
            f"Incremental graph complete: {self.graph.number_of_nodes():,} nodes, "
            f"{self.graph.number_of_edges():,} edges",
        )
        self.logger.info(success_msg)

        return self.graph

    def cleanup_deleted_samples(self) -> int:
        """
        Remove nodes for samples that no longer exist on Freesound.

        Queries the Freesound API to verify sample existence and removes
        nodes for samples that return 404. Uses rate limiting to avoid
        exceeding API limits.

        Returns:
            Number of deleted samples removed

        Raises:
            DataProcessingError: If verification is not enabled in config
        """
        if not self.verify_existing_sounds:
            raise DataProcessingError(
                "Sample verification not enabled. Set verify_existing_sounds=True"
            )

        self.logger.info(
            f"Verifying {self.graph.number_of_nodes()} samples for deletion..."
        )

        deleted_nodes = []

        with ProgressTracker(
            total=self.graph.number_of_nodes(),
            title="Verifying sample existence",
            logger=self.logger,
        ) as tracker:
            for i, node_id in enumerate(list(self.graph.nodes())):
                try:
                    # Rate limit the request
                    self.rate_limiter.acquire()

                    # Try to fetch the sound
                    self.client.get_sound(int(node_id))
                    self._increment_request_count()

                except Exception as e:
                    # Check if it's a 404 error
                    error_str = str(e).lower()
                    if "404" in error_str or "not found" in error_str:
                        deleted_nodes.append(node_id)
                        self.logger.debug(f"Sample {node_id} no longer exists")

                tracker.update(i + 1)

        # Remove deleted nodes from graph
        for node_id in deleted_nodes:
            self.graph.remove_node(node_id)
            self.processed_ids.discard(node_id)

        if deleted_nodes:
            self.logger.info(f"Removed {len(deleted_nodes)} deleted samples")
            self._save_checkpoint({"cleanup_performed": True})

        return len(deleted_nodes)

    def update_metadata(
        self, mode: str = "merge", sample_ids: Optional[list[str]] = None
    ) -> dict[str, Any]:
        """
        Update metadata for existing nodes.

        Fetches fresh metadata from Freesound API and updates node attributes.
        Supports both merge (add new fields, update existing) and replace
        (completely replace metadata) modes.

        Args:
            mode: Update mode - 'merge' or 'replace' (default: 'merge')
            sample_ids: Optional list of specific sample IDs to update.
                       If None, updates all nodes.

        Returns:
            Dictionary with update statistics:
            - nodes_updated: Number of nodes successfully updated
            - nodes_failed: Number of nodes that failed to update
            - fields_added: Set of new fields added
            - fields_updated: Set of existing fields updated

        Raises:
            ValueError: If mode is not 'merge' or 'replace'
        """
        if mode not in ("merge", "replace"):
            raise ValueError(f"Invalid mode '{mode}'. Must be 'merge' or 'replace'")

        # Determine which nodes to update
        nodes_to_update = sample_ids if sample_ids else list(self.graph.nodes())

        self.logger.info(
            f"Updating metadata for {len(nodes_to_update)} nodes (mode={mode})..."
        )

        stats: dict[str, Union[int, set[str]]] = {
            "nodes_updated": 0,
            "nodes_failed": 0,
            "fields_added": set(),
            "fields_updated": set(),
        }

        now = datetime.now(timezone.utc).isoformat()

        with ProgressTracker(
            total=len(nodes_to_update),
            title=f"Updating metadata ({mode} mode)",
            logger=self.logger,
        ) as tracker:
            for i, node_id in enumerate(nodes_to_update):
                try:
                    # Rate limit the request
                    self.rate_limiter.acquire()

                    # Fetch fresh metadata
                    sound = self.client.get_sound(int(node_id))
                    self._increment_request_count()
                    new_metadata = self._extract_sample_metadata(sound)

                    # Skip if metadata is invalid
                    if new_metadata is None:
                        self.logger.warning(
                            f"Skipping node {node_id} with invalid metadata"
                        )
                        continue

                    # Get current attributes
                    current_attrs = dict(self.graph.nodes[node_id])

                    if mode == "merge":
                        # Merge: add new fields, update existing
                        for key, value in new_metadata.items():
                            if key not in current_attrs:
                                cast(set, stats["fields_added"]).add(key)
                            elif current_attrs.get(key) != value:
                                cast(set, stats["fields_updated"]).add(key)

                            self.graph.nodes[node_id][key] = value

                    else:  # mode == 'replace'
                        # Replace: clear and set new attributes
                        self.graph.nodes[node_id].clear()
                        self.graph.nodes[node_id].update(new_metadata)
                        self.graph.nodes[node_id]["type"] = "sample"

                    # Update metadata refresh timestamp
                    self.graph.nodes[node_id]["last_metadata_update_at"] = now

                    stats["nodes_updated"] = cast(int, stats["nodes_updated"]) + 1

                except Exception as e:
                    self.logger.warning(f"Failed to update metadata for {node_id}: {e}")
                    stats["nodes_failed"] = cast(int, stats["nodes_failed"]) + 1

                tracker.update(i + 1)

        # Save checkpoint after metadata update
        self._save_checkpoint({"metadata_updated": True})

        self.logger.info(
            f"Metadata update complete: {stats['nodes_updated']} updated, "
            f"{stats['nodes_failed']} failed, "
            f"{len(cast(set, stats['fields_added']))} new fields, "
            f"{len(cast(set, stats['fields_updated']))} fields updated"
        )

        return stats

    def get_samples_by_existence_check_age(
        self, max_age_days: Optional[int] = None, limit: Optional[int] = None
    ) -> list[str]:
        """
        Get sample IDs sorted by last existence check age.

        Returns samples that haven't been checked recently, with samples
        that have never been checked appearing first.

        Args:
            max_age_days: Optional maximum age in days. If provided, only returns
                         samples older than this threshold.
            limit: Optional maximum number of samples to return.

        Returns:
            List of sample IDs sorted by age (oldest/never-checked first)

        Example:
            # Get 300 oldest samples for partial validation
            samples = loader.get_samples_by_existence_check_age(limit=300)

            # Get all samples not checked in last 30 days
            samples = loader.get_samples_by_existence_check_age(max_age_days=30)
        """
        from datetime import datetime, timedelta, timezone

        # Build list of (sample_id, last_check_timestamp) tuples
        samples_with_age: list[tuple[str, Optional[datetime]]] = []

        for node_id in self.graph.nodes():
            last_check = self.graph.nodes[node_id].get("last_existence_check_at")

            # Samples with no timestamp (never checked) get priority
            if last_check is None:
                samples_with_age.append((str(node_id), None))
            else:
                # Parse ISO timestamp
                try:
                    check_time = datetime.fromisoformat(
                        last_check.replace("Z", "+00:00")
                    )
                    samples_with_age.append((str(node_id), check_time))
                except (ValueError, AttributeError):
                    # Invalid timestamp, treat as never checked
                    samples_with_age.append((str(node_id), None))

        # Sort: None (never checked) first, then oldest timestamps
        samples_with_age.sort(
            key=lambda x: (
                x[1] is not None,
                x[1] or datetime.min.replace(tzinfo=timezone.utc),
            )
        )

        # Filter by max age if specified
        if max_age_days is not None:
            cutoff_time = datetime.now(timezone.utc) - timedelta(days=max_age_days)
            samples_with_age = [
                (sample_id, check_time)
                for sample_id, check_time in samples_with_age
                if check_time is None or check_time < cutoff_time
            ]

        # Extract sample IDs
        sample_ids = [sample_id for sample_id, _ in samples_with_age]

        # Apply limit if specified
        if limit is not None:
            sample_ids = sample_ids[:limit]

        return sample_ids

    def get_samples_by_metadata_age(
        self, max_age_days: Optional[int] = None, limit: Optional[int] = None
    ) -> list[str]:
        """
        Get sample IDs sorted by last metadata update age.

        Returns samples whose metadata hasn't been refreshed recently,
        with samples that have never been updated appearing first.

        Args:
            max_age_days: Optional maximum age in days. If provided, only returns
                         samples older than this threshold.
            limit: Optional maximum number of samples to return.

        Returns:
            List of sample IDs sorted by age (oldest/never-updated first)

        Example:
            # Get 100 oldest samples for metadata refresh
            samples = loader.get_samples_by_metadata_age(limit=100)

            # Get all samples not updated in last 90 days
            samples = loader.get_samples_by_metadata_age(max_age_days=90)
        """
        from datetime import datetime, timedelta, timezone

        # Build list of (sample_id, last_update_timestamp) tuples
        samples_with_age: list[tuple[str, Optional[datetime]]] = []

        for node_id in self.graph.nodes():
            last_update = self.graph.nodes[node_id].get("last_metadata_update_at")

            # Samples with no timestamp (never updated) get priority
            if last_update is None:
                samples_with_age.append((str(node_id), None))
            else:
                # Parse ISO timestamp
                try:
                    update_time = datetime.fromisoformat(
                        last_update.replace("Z", "+00:00")
                    )
                    samples_with_age.append((str(node_id), update_time))
                except (ValueError, AttributeError):
                    # Invalid timestamp, treat as never updated
                    samples_with_age.append((str(node_id), None))

        # Sort: None (never updated) first, then oldest timestamps
        samples_with_age.sort(
            key=lambda x: (
                x[1] is not None,
                x[1] or datetime.min.replace(tzinfo=timezone.utc),
            )
        )

        # Filter by max age if specified
        if max_age_days is not None:
            cutoff_time = datetime.now(timezone.utc) - timedelta(days=max_age_days)
            samples_with_age = [
                (sample_id, update_time)
                for sample_id, update_time in samples_with_age
                if update_time is None or update_time < cutoff_time
            ]

        # Extract sample IDs
        sample_ids = [sample_id for sample_id, _ in samples_with_age]

        # Apply limit if specified
        if limit is not None:
            sample_ids = sample_ids[:limit]

        return sample_ids

    def _search_samples(
        self, query: Optional[str], tags: Optional[list[str]], max_samples: int
    ) -> list[dict[str, Any]]:
        """
        Override parent method to optimize with page_size=150 and complete fields.

        This optimization fetches up to 150 samples with complete metadata per request,
        eliminating the need for follow-up metadata requests.

        Args:
            query: Text search query
            tags: List of tags to filter by
            max_samples: Maximum number of samples to fetch

        Returns:
            List of sample dictionaries with metadata
        """
        # complete fields parameter (29 response fields)
        # Note: original_filename and md5 are filter-only parameters, not response fields
        comprehensive_fields = (
            "id,url,name,tags,description,category,subcategory,geotag,created,"
            "license,type,channels,filesize,bitrate,bitdepth,duration,samplerate,"
            "username,pack,previews,images,num_downloads,avg_rating,num_ratings,"
            "num_comments,comments,similar_sounds,analysis,ac_analysis"
        )

        samples: list[Any] = []
        now = datetime.now(timezone.utc).isoformat()

        try:
            # Build search filter
            search_filter = ""
            if tags:
                tag_filter = " ".join(f"tag:{tag}" for tag in tags)
                search_filter = tag_filter

            # Perform search with rate limiting and retry on 429
            self.rate_limiter.acquire()
            self._increment_request_count()

            # Use text_search with pagination and complete fields
            page_size = min(150, max_samples)  # Freesound max page size is 150

            # Get pagination state to resume from last position
            pagination_state = getattr(
                self,
                "pagination_state",
                {"page": 1, "query": "", "sort": "downloads_desc"},
            )
            start_page = pagination_state.get("page", 1)
            current_sort = pagination_state.get("sort", "downloads_desc")

            # Cycle through different sort orders to discover new samples
            # This avoids always getting the same top samples
            sort_strategies = [
                "downloads_desc",  # Most downloaded
                "rating_desc",  # Highest rated
                "created_desc",  # Most recent
                "duration_desc",  # Longest
                "duration_asc",  # Shortest
            ]

            # If we've processed many samples with current sort, try next strategy
            if len(self.processed_ids) > 0 and len(self.processed_ids) % 1000 == 0:
                current_index = (
                    sort_strategies.index(current_sort)
                    if current_sort in sort_strategies
                    else 0
                )
                current_sort = sort_strategies[
                    (current_index + 1) % len(sort_strategies)
                ]
                start_page = 1  # Reset to page 1 with new sort
                self.logger.info(f"Switching to sort strategy: {current_sort}")

            # Wrap API call with retry logic
            def _do_search():
                return self.client.text_search(
                    query=query or "",
                    filter=search_filter if search_filter else None,
                    page_size=page_size,
                    sort=current_sort,
                    fields=comprehensive_fields,  # Get ALL metadata in one call!
                    page=start_page,  # Resume from last page
                )

            results = self._retry_with_backoff(_do_search)

            # Update pagination state
            self.pagination_state = {
                "page": start_page,
                "query": query or "",
                "sort": current_sort,
            }

            # Extract metadata directly from search results (no follow-up calls needed!)
            samples_on_first_page = 0
            for sound in results:
                if len(samples) >= max_samples:
                    break

                samples_on_first_page += 1

                # Skip if already processed (check SQL cache for O(1) lookup)
                if hasattr(self, "metadata_cache") and self.metadata_cache.exists(
                    sound.id
                ):
                    continue

                # Cache the sound object for future use
                self._sound_cache[sound.id] = sound

                # Extract complete metadata
                sample_data = self._extract_sample_metadata(sound)

                # Set last_metadata_update_at timestamp
                sample_data["last_metadata_update_at"] = now

                samples.append(sample_data)

            # If first page had samples but we got 0 new ones, advance to next page
            # This prevents getting stuck on a page where all samples are already processed
            if samples_on_first_page > 0 and len(samples) == 0:
                self.pagination_state["page"] = start_page + 1
                self.logger.info(
                    f"Page {start_page} had {samples_on_first_page} samples but 0 new ones, "
                    f"advancing to page {self.pagination_state['page']}"
                )

            # Fetch additional pages if needed
            current_page = start_page
            while len(samples) < max_samples and results.next:
                # Check circuit breaker BEFORE making next request
                if self._check_circuit_breaker():
                    self.logger.warning(
                        f"Circuit breaker triggered during pagination at page {current_page}. "
                        f"Collected {len(samples)} samples so far."
                    )
                    break

                self.rate_limiter.acquire()
                self._increment_request_count()
                results = self._retry_with_backoff(results.next_page)
                current_page += 1

                samples_before_page = len(samples)
                samples_on_page = 0

                for sound in results:
                    if len(samples) >= max_samples:
                        break

                    samples_on_page += 1

                    # Skip if already processed (check SQL cache for O(1) lookup)
                    if hasattr(self, "metadata_cache") and self.metadata_cache.exists(
                        sound.id
                    ):
                        continue

                    # Cache the sound object
                    self._sound_cache[sound.id] = sound

                    # Extract complete metadata
                    sample_data = self._extract_sample_metadata(sound)

                    # Skip invalid samples (e.g., 0 byte filesize) but mark as processed
                    if sample_data is None:
                        # Add to metadata cache so we don't retry this sample
                        if hasattr(self, "metadata_cache"):
                            invalid_metadata = {
                                "id": sound.id,
                                "invalid": True,
                                "invalid_reason": "zero_byte_filesize",
                                "last_metadata_update_at": now,
                            }
                            self.metadata_cache.set(sound.id, invalid_metadata)
                        continue

                    sample_data["last_metadata_update_at"] = now

                    samples.append(sample_data)

                # Update pagination state after each page
                self.pagination_state["page"] = current_page

                # If this page had samples but we got 0 new ones, log it
                # The pagination state is already updated above, so next run will try the next page
                samples_added_from_page = len(samples) - samples_before_page
                if samples_on_page > 0 and samples_added_from_page == 0:
                    self.logger.info(
                        f"Page {current_page} had {samples_on_page} samples but 0 new ones, "
                        f"continuing to next page"
                    )

            self.logger.info(
                f"Search returned {len(samples)} samples with complete metadata "
                f"(no follow-up calls needed)"
            )

        except Exception as e:
            raise DataProcessingError(f"Failed to search Freesound API: {e}") from e

        return samples

    def _fetch_sample_metadata(
        self, sample_id: int, return_sound: bool = False
    ) -> Union[dict[str, Any], tuple[dict[str, Any], Any]]:
        """
        Override parent method to track API requests and set metadata timestamp.

        Args:
            sample_id: Freesound sample ID
            return_sound: If True, returns (metadata, sound_object) tuple

        Returns:
            Dictionary with sample metadata, or tuple if return_sound=True
        """
        # Only increment if not in cache (parent checks cache first)
        if sample_id not in self._sound_cache:
            self._increment_request_count()

        # Call parent implementation
        result = super()._fetch_sample_metadata(sample_id, return_sound)

        # Set last_metadata_update_at timestamp
        now = datetime.now(timezone.utc).isoformat()

        if return_sound:
            if isinstance(result, tuple):
                metadata, sound = result
                metadata["last_metadata_update_at"] = now
                return metadata, sound
            else:
                # Shouldn't happen but handle gracefully
                result["last_metadata_update_at"] = now
                return result
        else:
            if isinstance(result, dict):
                result["last_metadata_update_at"] = now
                return result
            else:
                # Shouldn't happen but handle gracefully
                return result

    def _fetch_similar_sounds_for_sample(
        self, sample_id: int
    ) -> list[tuple[int, float]]:
        """
        Override parent method to optimize with page_size=150 and complete fields.

        This optimization fetches up to 150 similar sounds with complete metadata
        in a single API call, eliminating the need for follow-up metadata requests.

        complete fields include ALL 30 available fields:
        id, url, name, tags, description, category, subcategory, geotag, created,
        license, type, channels, filesize, bitrate, bitdepth, duration, samplerate,
        username, pack, previews, images, num_downloads, avg_rating, num_ratings,
        num_comments, comments, similar_sounds, analysis, ac_analysis,
        original_filename, md5

        Args:
            sample_id: Freesound sample ID

        Returns:
            List of (similar_id, score) tuples (excluding self-loops)
        """
        # Increment counter for similar sounds request
        self._increment_request_count()

        # complete fields parameter (ALL 30 available fields)
        # Note: OAuth2-only fields (bookmark, rate) and non-public fields excluded
        # analysis_stats/analysis_frames are inside analysis object
        # Note: original_filename and md5 are filter-only parameters, not response fields
        comprehensive_fields = (
            "id,url,name,tags,description,category,subcategory,geotag,created,"
            "license,type,channels,filesize,bitrate,bitdepth,duration,samplerate,"
            "username,pack,previews,images,num_downloads,avg_rating,num_ratings,"
            "num_comments,comments,similar_sounds,analysis,ac_analysis"
        )

        similar_list = []

        try:
            # Rate limit the request
            self.rate_limiter.acquire()

            # Fetch similar sounds with tuned parameters
            # page_size=150 (API maximum) and complete fields
            similar_sounds = self._retry_with_backoff(
                self.client.get_sound, sample_id
            ).get_similar(
                page_size=150,  # API maximum for list endpoints
                fields=comprehensive_fields,
            )
            self._increment_request_count()  # Count the get_similar API call

            # Extract IDs and scores, cache metadata for similar sounds
            now = datetime.now(timezone.utc).isoformat()

            for similar in similar_sounds:
                similar_id = similar.id

                # Skip self-loops
                if similar_id == sample_id:
                    continue

                # Cache the complete sound object (saves future API calls!)
                self._sound_cache[similar_id] = similar

                # Update last_metadata_update_at for cached sample
                # This is stored in the sound object's metadata
                if hasattr(similar, "_metadata"):
                    similar._metadata["last_metadata_update_at"] = now

                score = 1.0
                similar_list.append((similar_id, score))

            if similar_list:
                self.logger.debug(
                    f"Sample {sample_id}: found {len(similar_list)} similar sounds "
                    f"with complete metadata (cached)"
                )

        except Exception as e:
            self.logger.debug(f"Could not fetch similar sounds for {sample_id}: {e}")

        return similar_list

    def handle_error_with_data_preservation(
        self, error: Exception, context: str = "unknown"
    ) -> None:
        """
        Handle errors with fail-fast data preservation.

        Implements Requirements 11.1, 11.2, 11.3:
        - On ANY error, saves collected data to permanent storage FIRST
        - Then saves to cache as secondary backup
        - Only fails after data is safely stored

        Args:
            error: The exception that occurred
            context: Context string describing what was being attempted

        Raises:
            RuntimeError: Always raises after data preservation
        """
        self.logger.error(f"ðŸ”´ CRITICAL ERROR in {context}: {error}")
        self.logger.error("ðŸ”´ Attempting to preserve data before failing...")

        saved_to_permanent = False
        saved_to_cache = False
        saved_to_disk = False

        # Step 1: Save current checkpoint to disk
        try:
            self.logger.info("Step 1/3: Saving checkpoint to disk...")
            self._save_checkpoint(
                {
                    "error_context": context,
                    "error_message": str(error),
                    "error_type": type(error).__name__,
                    "emergency_save": True,
                }
            )
            saved_to_disk = True
            self.logger.info("âœ… Checkpoint saved to disk")
        except Exception as checkpoint_error:
            self.logger.error(f"âŒ Failed to save checkpoint: {checkpoint_error}")

        # Step 2: Upload to permanent storage (with verification and retry)
        try:
            self.logger.info("Step 2/3: Uploading to permanent storage...")
            from pathlib import Path

            checkpoint_dir = Path(
                self.config.get("checkpoint_dir", "data/freesound_library")
            )

            # Create backup using BackupManager
            if hasattr(self, "backup_manager"):
                topology_path = checkpoint_dir / "graph_topology.gpickle"
                metadata_db_path = checkpoint_dir / "metadata_cache.db"

                checkpoint_metadata = {
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                    "nodes": self.graph.number_of_nodes(),
                    "edges": self.graph.number_of_edges(),
                    "error_recovery": True,
                    "error_context": context,
                }

                success = self.backup_manager.create_backup(
                    topology_path=topology_path,
                    metadata_db_path=metadata_db_path,
                    checkpoint_metadata=checkpoint_metadata,
                )

                if success:
                    saved_to_permanent = True
                    self.logger.info("âœ… Data uploaded to permanent storage")
                else:
                    self.logger.error("âŒ Failed to upload to permanent storage")
        except Exception as upload_error:
            self.logger.error(
                f"âŒ Failed to upload to permanent storage: {upload_error}"
            )

        # Step 3: Save to cache as FALLBACK (ephemeral, 7-day retention)
        self.logger.info("Step 3/3: Saving to cache as fallback...")
        try:
            # This would be done by the workflow, but we can trigger it
            self.logger.info("âš ï¸ Cache save will be handled by workflow")
            saved_to_cache = True  # Assume workflow will handle it
        except Exception as cache_error:
            self.logger.error(f"âŒ Failed to save to cache: {cache_error}")

        # Report final status
        if saved_to_permanent:
            self.logger.info("âœ… FINAL CHECKPOINT SAVED TO PERMANENT STORAGE")
            self.logger.info("âœ… Data is safe and will not be lost")
        elif saved_to_cache:
            self.logger.warning("âš ï¸ FINAL CHECKPOINT SAVED TO CACHE ONLY")
            self.logger.warning(
                "âš ï¸ Data may be lost after 7 days - manual backup recommended"
            )
        elif saved_to_disk:
            self.logger.error("âŒ FINAL CHECKPOINT SAVED TO DISK ONLY")
            self.logger.error(
                "âŒ Data will be lost when workflow completes - MANUAL RECOVERY REQUIRED"
            )
        else:
            self.logger.critical("ðŸ”´ FAILED TO SAVE FINAL CHECKPOINT ANYWHERE")
            self.logger.critical("ðŸ”´ DATA LOSS IMMINENT - MANUAL RECOVERY REQUIRED")

        # Set failure flag for workflow coordination
        try:
            from pathlib import Path

            from ...utils import FailureHandler

            flag_dir = Path(self.config.get("checkpoint_dir", "data/freesound_library"))
            failure_handler = FailureHandler(flag_dir, self.logger)

            # Get workflow info from environment if available
            import os

            workflow_name = os.environ.get("GITHUB_WORKFLOW", "unknown")
            run_id = os.environ.get("GITHUB_RUN_ID", "unknown")

            failure_handler.set_failure_flag(
                workflow_name=workflow_name,
                run_id=run_id,
                error_message=f"{context}: {str(error)}",
                data_preserved=saved_to_permanent or saved_to_cache,
                additional_info={
                    "nodes": self.graph.number_of_nodes(),
                    "edges": self.graph.number_of_edges(),
                    "saved_to_permanent": saved_to_permanent,
                    "saved_to_cache": saved_to_cache,
                    "saved_to_disk": saved_to_disk,
                },
            )
        except Exception as flag_error:
            self.logger.error(f"Failed to set failure flag: {flag_error}")

        # Create GitHub issue for critical failure
        try:
            from ...utils import GitHubIssueCreator

            issue_creator = GitHubIssueCreator(self.logger)

            checkpoint_status = {
                "nodes": self.graph.number_of_nodes(),
                "edges": self.graph.number_of_edges(),
                "saved_to_permanent": saved_to_permanent,
                "saved_to_cache": saved_to_cache,
                "saved_to_disk": saved_to_disk,
            }

            issue_creator.create_failure_issue(
                title=f"Critical Failure: {workflow_name} - {context}",
                error_message=str(error),
                workflow_name=workflow_name,
                run_id=run_id,
                checkpoint_status=checkpoint_status,
                additional_info={
                    "error_type": type(error).__name__,
                    "context": context,
                },
            )
        except Exception as issue_error:
            self.logger.error(f"Failed to create GitHub issue: {issue_error}")

        # Raise exception to trigger fail-fast behavior
        raise RuntimeError(
            f"Critical error in {context}: {error}. "
            f"Data preservation: permanent={saved_to_permanent}, "
            f"cache={saved_to_cache}, disk={saved_to_disk}"
        ) from error
