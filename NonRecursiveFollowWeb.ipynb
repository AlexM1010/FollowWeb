{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import packages - Modernized imports with type hints and additional libraries\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import nx_parallel as nxp\n",
    "from networkx.algorithms import community\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "import hashlib\n",
    "import datetime\n",
    "from pyvis.network import Network\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "from collections import defaultdict\n",
    "from typing import Optional, Dict, List, Tuple\n",
    "\n",
    "# Enable parallel backend globally for NetworkX\n",
    "# Set this to the number of CPU cores you want to utilize.\n",
    "# A safe default is often (os.cpu_count() or 4)\n",
    "nx.config.backends.parallel.active = True\n",
    "nx.config.backends.parallel.n_jobs = (os.cpu_count() or 4) - 1\n",
    "\n",
    "# =================================== CENTRALIZED CONFIGURATION SYSTEM ===================================\n",
    "# Comprehensive CONFIG dictionary structure for different analysis aspects\n",
    "# This replaces all hardcoded parameters throughout the notebook\n",
    "\n",
    "CONFIG = {\n",
    "    # --- INPUT/OUTPUT CONFIGURATION ---\n",
    "    \"input_file\": \"followers_following.json\",  # Path to input data file (JSON format preferred)\n",
    "    \"output_file_prefix\": \"Output/FollowWeb\",   # Base path for all output files\n",
    "    \n",
    "    # --- PIPELINE CONFIGURATION ---\n",
    "    \"pipeline\": {\n",
    "        # Analysis strategy selection:\n",
    "        # 1. \"k-core\": (Default) Prunes the full L1+L2 graph. Good for general overview.\n",
    "        # 2. \"reciprocal_k-core\": Filters for mutuals (A follows B AND B follows A) then prunes. Good for finding \"real\" friend groups.\n",
    "        # 3. \"ego_alter_k-core\": Creates a graph of only your L1 contacts, connected if they follow each other. Good for analyzing your immediate circle.\n",
    "        \"strategy\": \"k-core\",    # Options: \"k-core\", \"reciprocal_k-core\", \"ego_alter_k-core\"\n",
    "        \n",
    "        # Skip computationally expensive structural analysis (community detection, centrality)\n",
    "        \"skip_analysis\": False,  # Set to True to skip ALL computationally expensive structural analysis\n",
    "        \n",
    "        # Required for \"ego_alter_k-core\" strategy - the central node (you)\n",
    "        \"ego_username\": \"_alexs.life\"  # Must be set if using \"ego_alter_k-core\"\n",
    "    },\n",
    "\n",
    "    # --- ANALYSIS CONFIGURATION ---\n",
    "    \"analysis\": {\n",
    "        # Specific username to find a path to (must be in your followers_following.json file)\n",
    "        # Set to \"\" or None to disable manual path finding\n",
    "        \"contact_path_target\": None,\n",
    "    },\n",
    "\n",
    "    # --- FAME ANALYSIS CONFIGURATION ---\n",
    "    \"fame_analysis\": {\n",
    "        # Find contact paths to every famous account identified\n",
    "        \"find_paths_to_all_famous\": True, \n",
    "        \n",
    "        # Minimum followers within your L1/L2 network for an account to be considered\n",
    "        \"min_followers_in_network\": 5, \n",
    "        \n",
    "        # Minimum ratio of (followers / following) to be considered famous\n",
    "        # (e.g., 5.0 means 5 followers for every 1 person they follow)\n",
    "        \"min_fame_ratio\": 5.0 \n",
    "    },\n",
    "\n",
    "    # --- PRUNING CONFIGURATION ---\n",
    "    \"pruning\": {\n",
    "        # Strategy-specific k-values (minimum connections required)\n",
    "        # Nodes with fewer connections than this will be removed\n",
    "        \"k_values\": {\n",
    "            \"k-core\": 1,              # Conservative pruning for full network\n",
    "            \"reciprocal_k-core\": 6,   # More aggressive pruning for mutual connections\n",
    "            \"ego_alter_k-core\": 3,    # Moderate pruning for ego network\n",
    "        },\n",
    "        \"default_k_value\": 2  # Fallback if strategy name is incorrect\n",
    "    },\n",
    "    \n",
    "    # --- VISUALIZATION CONFIGURATION ---\n",
    "    \"visualization\": {\n",
    "        # --- Shared Settings for Both HTML and PNG ---\n",
    "        \"node_size_metric\": \"degree\",        # Options: \"degree\", \"betweenness\", \"eigenvector\"\n",
    "        \"base_node_size\": 6,                 # Base size for nodes\n",
    "        \"node_size_multiplier\": 5,           # Multiplier for node size scaling\n",
    "        \"scaling_algorithm\": \"logarithmic\",  # Options: \"logarithmic\", \"linear\"\n",
    "        \n",
    "        \"base_edge_width\": 0.5,              # Base width for edges\n",
    "        \"edge_width_multiplier\": 2,          # Multiplier for edge width scaling\n",
    "        \"edge_width_scaling\": \"logarithmic\", # Options: \"logarithmic\", \"linear\"\n",
    "        \"intra_community_color\": \"#c0c0c0\",  # Gray color for within-community edges\n",
    "        \"bridge_color\": \"#6e6e6e\",           # Darker gray for between-community edges\n",
    "\n",
    "        # --- Interactive HTML Visualization (Pyvis) Configuration ---\n",
    "        \"pyvis_interactive\": { \n",
    "            \"width\": \"100%\",                 # Canvas width\n",
    "            \"height\": \"90vh\",                # Canvas height\n",
    "            \"notebook\": False,               # Set to True for Jupyter notebook display\n",
    "            \"show_labels\": True,             # Set to False to hide node names for faster rendering\n",
    "            \"show_tooltips\": True,           # Set to False to disable hover tooltips for faster loading\n",
    "            \"physics_solver\": \"forceAtlas2Based\",  # Physics simulation algorithm\n",
    "        },\n",
    "\n",
    "        # --- Static PNG Image Configuration ---\n",
    "        \"static_image\": {\n",
    "            \"generate\": True,                # Set to True to generate static PNG images\n",
    "            \"layout\": \"spring\",             # Options: \"spring\", \"kamada_kawai\", \"circular\", \"shell\"\n",
    "            \"with_labels\": False,            # Labels are often too cluttered on static graphs\n",
    "            \"font_size\": 8,                 # Font size for labels (if enabled)\n",
    "            \"image_size_inches\": (25, 25),   # Image dimensions (width, height) in inches\n",
    "            \"dpi\": 300,                     # Dots Per Inch - higher for better quality\n",
    "            \"spring_k\": 0.3,               # Spring layout parameter (adjusts node repulsion)\n",
    "            \"spring_iterations\": 50,        # Number of iterations for spring layout\n",
    "            \"edge_alpha\": 0.3,              # Edge transparency (0.0 to 1.0)\n",
    "            \"node_alpha\": 0.8,              # Node transparency (0.0 to 1.0)\n",
    "            \"edge_arrow_size\": 8,            # Size of arrow heads on edges\n",
    "            \"show_legend\": True              # Include legend in static images\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# =================================== CONFIGURATION VALIDATION ===================================\n",
    "# Comprehensive configuration validation with clear error messages\n",
    "# This function validates all configuration parameters before execution\n",
    "\n",
    "def validate_config(config: dict) -> bool:\n",
    "    \"\"\"\n",
    "    Validates CONFIG before running pipeline.\n",
    "    Raises exceptions for critical errors, prints warnings for minor issues.\n",
    "    \n",
    "    Args:\n",
    "        config (dict): The configuration dictionary to validate\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if validation passes\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If input file doesn't exist\n",
    "        ValueError: If configuration parameters are invalid\n",
    "    \"\"\"\n",
    "    print(\"=== VALIDATING CONFIGURATION ===\")\n",
    "    \n",
    "    # --- INPUT FILE VALIDATION ---\n",
    "    if not os.path.exists(config['input_file']):\n",
    "        raise FileNotFoundError(f\"Input file not found: {config['input_file']}. Please ensure the file exists.\")\n",
    "    print(f\"\u00e2\u0153\u201c Input file exists: {config['input_file']}\")\n",
    "    \n",
    "    # --- STRATEGY VALIDATION ---\n",
    "    strategy = config['pipeline']['strategy']\n",
    "    valid_strategies = ['k-core', 'reciprocal_k-core', 'ego_alter_k-core']\n",
    "    if strategy not in valid_strategies:\n",
    "        raise ValueError(f\"Invalid strategy '{strategy}'. Must be one of: {valid_strategies}\")\n",
    "    print(f\"\u00e2\u0153\u201c Strategy is valid: {strategy}\")\n",
    "    \n",
    "    # --- EGO USERNAME VALIDATION (for ego_alter_k-core strategy) ---\n",
    "    if strategy == \"ego_alter_k-core\":\n",
    "        ego = config['pipeline']['ego_username']\n",
    "        if not ego or ego == \"_alexs.life\":  # Check for placeholder value\n",
    "            raise ValueError(\"'ego_username' must be set in CONFIG for 'ego_alter_k-core' strategy. Please replace the placeholder value.\")\n",
    "        print(f\"\u00e2\u0153\u201c Ego username set for ego_alter_k-core: {ego}\")\n",
    "    \n",
    "    # --- K-VALUES VALIDATION ---\n",
    "    for strat, k_val in config['pruning']['k_values'].items():\n",
    "        if not isinstance(k_val, int) or k_val < 0:\n",
    "            raise ValueError(f\"k-value for '{strat}' must be a non-negative integer: {k_val}\")\n",
    "    \n",
    "    default_k = config['pruning']['default_k_value']\n",
    "    if not isinstance(default_k, int) or default_k < 0:\n",
    "        raise ValueError(f\"default_k_value must be a non-negative integer: {default_k}\")\n",
    "    print(f\"\u00e2\u0153\u201c K-values are valid: {config['pruning']['k_values']}\")\n",
    "    \n",
    "    # --- FAME ANALYSIS VALIDATION ---\n",
    "    min_followers = config['fame_analysis']['min_followers_in_network']\n",
    "    if not isinstance(min_followers, int) or min_followers < 0:\n",
    "        raise ValueError(f\"min_followers_in_network must be a non-negative integer: {min_followers}\")\n",
    "    \n",
    "    min_ratio = config['fame_analysis']['min_fame_ratio']\n",
    "    if not isinstance(min_ratio, (int, float)) or min_ratio <= 0:\n",
    "        raise ValueError(f\"min_fame_ratio must be a positive number: {min_ratio}\")\n",
    "    print(f\"\u00e2\u0153\u201c Fame analysis parameters are valid\")\n",
    "    \n",
    "    # --- VISUALIZATION CONFIGURATION VALIDATION ---\n",
    "    vis_config = config['visualization']\n",
    "    \n",
    "    # Validate node size metric\n",
    "    valid_metrics = ['degree', 'betweenness', 'eigenvector']\n",
    "    if vis_config['node_size_metric'] not in valid_metrics:\n",
    "        raise ValueError(f\"Invalid 'node_size_metric': {vis_config['node_size_metric']}. Must be one of: {valid_metrics}\")\n",
    "    \n",
    "    # Validate scaling algorithms\n",
    "    valid_scaling = ['logarithmic', 'linear']\n",
    "    if vis_config['scaling_algorithm'] not in valid_scaling:\n",
    "        raise ValueError(f\"Invalid 'scaling_algorithm': {vis_config['scaling_algorithm']}. Must be one of: {valid_scaling}\")\n",
    "    \n",
    "    if vis_config['edge_width_scaling'] not in valid_scaling:\n",
    "        raise ValueError(f\"Invalid 'edge_width_scaling': {vis_config['edge_width_scaling']}. Must be one of: {valid_scaling}\")\n",
    "    \n",
    "    # Validate pyvis interactive configuration\n",
    "    pyvis_config = vis_config.get('pyvis_interactive', {})\n",
    "    required_pyvis_keys = ['width', 'height', 'physics_solver']\n",
    "    missing_keys = [key for key in required_pyvis_keys if key not in pyvis_config]\n",
    "    if missing_keys:\n",
    "        raise ValueError(f\"Missing required keys in 'visualization.pyvis_interactive': {missing_keys}\")\n",
    "    \n",
    "    # Validate static image configuration\n",
    "    static_config = vis_config.get('static_image', {})\n",
    "    if static_config.get('generate', False):\n",
    "        valid_layouts = ['spring', 'kamada_kawai', 'circular', 'shell']\n",
    "        layout = static_config.get('layout', 'spring')\n",
    "        if layout not in valid_layouts:\n",
    "            raise ValueError(f\"Invalid static image layout '{layout}'. Must be one of: {valid_layouts}\")\n",
    "        \n",
    "        # Validate image dimensions\n",
    "        image_size = static_config.get('image_size_inches', (25, 25))\n",
    "        if not isinstance(image_size, (tuple, list)) or len(image_size) != 2:\n",
    "            raise ValueError(\"image_size_inches must be a tuple/list of (width, height)\")\n",
    "        \n",
    "        if any(not isinstance(dim, (int, float)) or dim <= 0 for dim in image_size):\n",
    "            raise ValueError(\"image_size_inches dimensions must be positive numbers\")\n",
    "    \n",
    "    print(f\"\u00e2\u0153\u201c Visualization configuration is valid\")\n",
    "    \n",
    "    # --- OUTPUT DIRECTORY VALIDATION ---\n",
    "    output_dir = os.path.dirname(config['output_file_prefix'])\n",
    "    if output_dir and not os.path.exists(output_dir):\n",
    "        try:\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            print(f\"\u00e2\u0153\u201c Created output directory: {output_dir}\")\n",
    "        except OSError as e:\n",
    "            raise ValueError(f\"Cannot create output directory '{output_dir}': {e}\")\n",
    "    \n",
    "    print(\"SUCCESS: Configuration validated successfully\")\n",
    "    print(\"=================================\\n\")\n",
    "    return True\n",
    "\n",
    "# Validate the configuration before proceeding\n",
    "validate_config(CONFIG)",
    "\n",
    "# Print configuration summary\n",
    "print(\"=== CONFIGURATION LOADED ===\")\n",
    "print(f\"Strategy: {CONFIG['pipeline']['strategy']}\")\n",
    "print(f\"Input file: {CONFIG['input_file']}\")\n",
    "print(f\"Output prefix: {CONFIG['output_file_prefix']}\")\n",
    "print(f\"Skip analysis: {CONFIG['pipeline']['skip_analysis']}\")\n",
    "print(f\"K-value for {CONFIG['pipeline']['strategy']}: {CONFIG['pruning']['k_values'].get(CONFIG['pipeline']['strategy'], CONFIG['pruning']['default_k_value'])}\")\n",
    "print(\"============================\\n\")\n",
    "\n",
    "# =================================== DIRECTORY MANAGEMENT AND LOGGING ===================================\n",
    "# Comprehensive directory management and logging throughout the analysis pipeline\n",
    "\n",
    "def ensure_output_directory(path: str) -> bool:\n",
    "    \"\"\"\n",
    "    Ensures that the output directory exists, creating it if necessary.\n",
    "    \n",
    "    Args:\n",
    "        path (str): Path to the output file or directory\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if directory exists or was created successfully, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract directory from file path\n",
    "        directory = os.path.dirname(path)\n",
    "        \n",
    "        # If no directory specified, use current directory\n",
    "        if not directory:\n",
    "            return True\n",
    "        \n",
    "        # Create directory if it doesn't exist\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory, exist_ok=True)\n",
    "            print(f\"Created output directory: {directory}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except OSError as e:\n",
    "        print(f\"ERROR: Could not create output directory for '{path}': {e}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Unexpected error creating directory for '{path}': {e}\")\n",
    "        return False\n",
    "\n",
    "def log_analysis_step(step_name: str, details: str = \"\", timing: float = None) -> None:\n",
    "    \"\"\"\n",
    "    Logs an analysis step with optional details and timing information.\n",
    "    \n",
    "    Args:\n",
    "        step_name (str): Name of the analysis step\n",
    "        details (str): Optional details about the step\n",
    "        timing (float): Optional timing information in seconds\n",
    "    \"\"\"\n",
    "    timestamp = datetime.datetime.now().strftime('%H:%M:%S')\n",
    "    \n",
    "    log_message = f\"[{timestamp}] {step_name}\"\n",
    "    \n",
    "    if details:\n",
    "        log_message += f\" - {details}\"\n",
    "    \n",
    "    if timing is not None:\n",
    "        log_message += f\" (completed in {timing:.4f}s)\"\n",
    "    \n",
    "    print(log_message)\n",
    "\n",
    "def log_analysis_summary(G: nx.DiGraph, config: dict, timing_info: dict = None) -> None:\n",
    "    \"\"\"\n",
    "    Logs a comprehensive summary of the analysis results.\n",
    "    \n",
    "    Args:\n",
    "        G (nx.DiGraph): The analyzed graph\n",
    "        config (dict): Configuration dictionary used for analysis\n",
    "        timing_info (dict): Optional timing information for different analysis steps\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ANALYSIS SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Basic graph information\n",
    "    print(f\"Strategy: {config['pipeline']['strategy']}\")\n",
    "    print(f\"Input File: {config['input_file']}\")\n",
    "    print(f\"Final Graph: {G.number_of_nodes():,} nodes, {G.number_of_edges():,} edges\")\n",
    "    \n",
    "    if G.number_of_nodes() > 0:\n",
    "        density = nx.density(G)\n",
    "        print(f\"Network Density: {density:.6f}\")\n",
    "        \n",
    "        # Community information\n",
    "        communities_attr = nx.get_node_attributes(G, 'community')\n",
    "        if communities_attr:\n",
    "            num_communities = len(set(communities_attr.values()))\n",
    "            print(f\"Communities Detected: {num_communities}\")\n",
    "        else:\n",
    "            print(\"Communities Detected: None (analysis skipped or failed)\")\n",
    "        \n",
    "        # Degree statistics\n",
    "        degrees = dict(G.degree())\n",
    "        if degrees:\n",
    "            degree_values = list(degrees.values())\n",
    "            print(f\"Average Degree: {sum(degree_values) / len(degree_values):.2f}\")\n",
    "            print(f\"Degree Range: {min(degree_values)} - {max(degree_values)}\")\n",
    "    \n",
    "    # Timing information\n",
    "    if timing_info:\n",
    "        print(\"\\nPERFORMANCE BREAKDOWN:\")\n",
    "        total_time = sum(timing_info.values())\n",
    "        for step_name, duration in timing_info.items():\n",
    "            percentage = (duration / total_time * 100) if total_time > 0 else 0\n",
    "            print(f\"  {step_name:<25}: {duration:8.4f}s ({percentage:5.1f}%)\")\n",
    "        print(f\"  {'TOTAL':<25}: {total_time:8.4f}s\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "\n",
    "def create_progress_logger(total_steps: int, operation_name: str) -> callable:\n",
    "    \"\"\"\n",
    "    Creates a progress logging function for multi-step operations.\n",
    "    \n",
    "    Args:\n",
    "        total_steps (int): Total number of steps in the operation\n",
    "        operation_name (str): Name of the operation being logged\n",
    "        \n",
    "    Returns:\n",
    "        callable: Function to call for logging progress\n",
    "    \"\"\"\n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    def log_progress(current_step: int, step_description: str = \"\"):\n",
    "        elapsed = time.perf_counter() - start_time\n",
    "        percentage = (current_step / total_steps) * 100\n",
    "        \n",
    "        message = f\"[{operation_name}] Step {current_step}/{total_steps} ({percentage:.1f}%)\"\n",
    "        if step_description:\n",
    "            message += f\" - {step_description}\"\n",
    "        message += f\" [Elapsed: {elapsed:.2f}s]\"\n",
    "        \n",
    "        print(message)\n",
    "    \n",
    "    return log_progress\n",
    "\n",
    "# =================================== GRAPH LOADING AND STRATEGY FILTERING ===================================\n",
    "# Advanced network analysis capabilities with strategy-specific filtering\n",
    "\n",
    "def load_graph_from_json(filepath: str) -> nx.DiGraph:\n",
    "    \"\"\"\n",
    "    Loads a directed graph from a JSON file with proper error handling.\n",
    "    \n",
    "    Expected format: A list of user objects\n",
    "    [\n",
    "     {\n",
    "       \"user\": \"username1\",\n",
    "       \"followers\": [\"user2\", \"user3\"],\n",
    "       \"following\": [\"user4\", \"user5\"]\n",
    "     },\n",
    "     ...\n",
    "    ]\n",
    "    \n",
    "    Args:\n",
    "        filepath (str): Path to the JSON data file\n",
    "        \n",
    "    Returns:\n",
    "        nx.DiGraph: Loaded graph or empty graph on error\n",
    "    \"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    try:\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"ERROR: Invalid JSON format in {filepath}\")\n",
    "        print(f\"       JSON error: {e}\")\n",
    "        return G\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Could not read file {filepath}: {e}\")\n",
    "        return G\n",
    "    \n",
    "    if not isinstance(data, list):\n",
    "        print(f\"ERROR: JSON root must be a LIST, got {type(data)}\")\n",
    "        return G\n",
    "    \n",
    "    print(f\"Processing {len(data)} user entries...\")\n",
    "    \n",
    "    # Process each user's data from the list\n",
    "    for user_entry in data:\n",
    "        if not isinstance(user_entry, dict):\n",
    "            print(f\"WARNING: Skipping item in list - data is not a dict\")\n",
    "            continue\n",
    "        \n",
    "        # Get username from the 'user' key\n",
    "        username = user_entry.get('user')\n",
    "        if not username:\n",
    "            print(\"WARNING: Skipping item in list - 'user' key is missing or empty.\")\n",
    "            continue\n",
    "        \n",
    "        # Validate required keys\n",
    "        if 'followers' not in user_entry or 'following' not in user_entry:\n",
    "            print(f\"WARNING: User '{username}' missing 'followers' or 'following' key\")\n",
    "            continue\n",
    "        \n",
    "        followers = user_entry.get('followers', [])\n",
    "        following = user_entry.get('following', [])\n",
    "        \n",
    "        if not isinstance(followers, list):\n",
    "            print(f\"WARNING: User '{username}' - 'followers' is not a list\")\n",
    "            followers = []\n",
    "        if not isinstance(following, list):\n",
    "            print(f\"WARNING: User '{username}' - 'following' is not a list\")\n",
    "            following = []\n",
    "        \n",
    "        # Add edges\n",
    "        for follower in followers:\n",
    "            if follower:  # Skip empty strings\n",
    "                G.add_edge(follower, username)  # Follower -> User\n",
    "        \n",
    "        for followee in following:\n",
    "            if followee:  # Skip empty strings\n",
    "                G.add_edge(username, followee)  # User -> Followee\n",
    "    \n",
    "    print(f\"Initial graph loaded: {G.number_of_nodes():,} nodes, {G.number_of_edges():,} edges.\")\n",
    "    return G\n",
    "\n",
    "def filter_by_reciprocity(G: nx.DiGraph) -> nx.DiGraph:\n",
    "    \"\"\"\n",
    "    Creates a new graph containing only reciprocal edges (mutual followers).\n",
    "    \n",
    "    Args:\n",
    "        G (nx.DiGraph): Input graph\n",
    "        \n",
    "    Returns:\n",
    "        nx.DiGraph: New graph with only mutual connections, or empty graph if none exist\n",
    "    \"\"\"\n",
    "    G_reciprocal = nx.DiGraph()\n",
    "    \n",
    "    # Keep only edges where the reverse edge also exists\n",
    "    reciprocal_edges = [edge for edge in G.edges() if G.has_edge(edge[1], edge[0])]\n",
    "    G_reciprocal.add_edges_from(reciprocal_edges)\n",
    "\n",
    "    # Remove nodes that now have 0 degree\n",
    "    G_reciprocal.remove_nodes_from(list(nx.isolates(G_reciprocal)))\n",
    "\n",
    "    print(f\"Filtered for mutuals: {G_reciprocal.number_of_nodes():,} nodes, {G_reciprocal.number_of_edges():,} edges.\")\n",
    "    return G_reciprocal\n",
    "\n",
    "def create_ego_alter_graph(G: nx.DiGraph, ego_username: str) -> nx.DiGraph:\n",
    "    \"\"\"\n",
    "    Creates an \"alter graph\" showing connections between the ego's L1 contacts.\n",
    "    \n",
    "    Args:\n",
    "        G (nx.DiGraph): Input graph\n",
    "        ego_username (str): The central node (ego)\n",
    "        \n",
    "    Returns:\n",
    "        nx.DiGraph: Graph of L1 contacts and their connections, or empty graph on error\n",
    "    \"\"\"\n",
    "    if ego_username not in G:\n",
    "        print(f\"ERROR: Ego node '{ego_username}' not in graph. Check CONFIG.\")\n",
    "        return nx.DiGraph()\n",
    "\n",
    "    # 1. Identify Alters (L1 nodes)\n",
    "    try:\n",
    "        followers = set(G.predecessors(ego_username))\n",
    "    except nx.NetworkXError:\n",
    "        followers = set()\n",
    "        \n",
    "    try:\n",
    "        following = set(G.successors(ego_username))\n",
    "    except nx.NetworkXError:\n",
    "        following = set()\n",
    "        \n",
    "    alters = followers.union(following)\n",
    "\n",
    "    if not alters:\n",
    "        print(\"WARNING: No alters (L1 connections) found for this ego.\")\n",
    "        return nx.DiGraph()\n",
    "\n",
    "    # 2. Create a new graph containing only connections between alters\n",
    "    alter_graph = G.subgraph(alters).copy()\n",
    "    \n",
    "    # Remove isolates (alters who don't connect to any other alters)\n",
    "    alter_graph.remove_nodes_from(list(nx.isolates(alter_graph)))\n",
    "\n",
    "    print(f\"Alter graph created: {alter_graph.number_of_nodes():,} alters, {alter_graph.number_of_edges():,} connections between them.\")\n",
    "    return alter_graph\n",
    "\n",
    "def prune_graph(G: nx.DiGraph, min_degree: int) -> nx.DiGraph:\n",
    "    \"\"\"\n",
    "    Uses nx.k_core function to find the maximal subgraph \n",
    "    where all nodes have degree >= min_degree.\n",
    "    \n",
    "    Args:\n",
    "        G (nx.DiGraph): Input graph\n",
    "        min_degree (int): Minimum degree threshold (k-value)\n",
    "        \n",
    "    Returns:\n",
    "        nx.DiGraph: Pruned graph (k-core subgraph)\n",
    "    \"\"\"\n",
    "    if min_degree <= 0:\n",
    "        print(\"Pruning skipped (min_degree <= 0).\")\n",
    "        return G\n",
    "\n",
    "    # nx.k_core finds the subgraph where all nodes have at least k-degree\n",
    "    # For DiGraphs, .degree() is in+out, which matches your original logic.\n",
    "    G_pruned = nx.k_core(G, k=min_degree)\n",
    "\n",
    "    nodes_removed = G.number_of_nodes() - G_pruned.number_of_nodes()\n",
    "    \n",
    "    print(f\"Pruning complete. Removed {nodes_removed:,} nodes.\")\n",
    "    print(f\"Final pruned graph: {G_pruned.number_of_nodes():,} nodes, {G_pruned.number_of_edges():,} edges.\")\n",
    "    return G_pruned\n",
    "\n",
    "def apply_strategy_filtering(G: nx.DiGraph, strategy: str, ego_username: Optional[str] = None) -> nx.DiGraph:\n",
    "    \"\"\"\n",
    "    Applies strategy-specific graph filtering for different analysis approaches.\n",
    "    \n",
    "    Args:\n",
    "        G (nx.DiGraph): Input graph\n",
    "        strategy (str): Analysis strategy (\"k-core\", \"reciprocal_k-core\", \"ego_alter_k-core\")\n",
    "        ego_username (Optional[str]): Required for \"ego_alter_k-core\" strategy\n",
    "        \n",
    "    Returns:\n",
    "        nx.DiGraph: Filtered graph based on strategy\n",
    "    \"\"\"\n",
    "    print(f\"Applying '{strategy}' strategy filtering...\")\n",
    "    \n",
    "    if strategy == \"k-core\":\n",
    "        # Use the full graph as-is\n",
    "        return G\n",
    "    elif strategy == \"reciprocal_k-core\":\n",
    "        # Filter for mutual connections only\n",
    "        return filter_by_reciprocity(G)\n",
    "    elif strategy == \"ego_alter_k-core\":\n",
    "        # Create ego-alter network\n",
    "        if not ego_username:\n",
    "            print(\"ERROR: ego_username required for 'ego_alter_k-core' strategy\")\n",
    "            return nx.DiGraph()\n",
    "        return create_ego_alter_graph(G, ego_username)\n",
    "    else:\n",
    "        print(f\"WARNING: Unknown strategy '{strategy}'. Using full graph.\")\n",
    "        return G\n",
    "\n",
    "# =================================== LOAD AND PROCESS GRAPH ===================================\n",
    "# Load graph from JSON data file with strategy-specific filtering\n",
    "\n",
    "# Initialize timing tracking\n",
    "pipeline_start_time = time.perf_counter()\n",
    "timing_info = {}\n",
    "\n",
    "print(\"=== LOADING GRAPH FROM DATA FILE ===\")\n",
    "log_analysis_step(\"Pipeline Started\", f\"Strategy: {CONFIG['pipeline']['strategy']}\")\n",
    "\n",
    "# Ensure output directory exists\n",
    "ensure_output_directory(CONFIG['output_file_prefix'])\n",
    "\n",
    "graph_load_start = time.perf_counter()\n",
    "DATA_FILE = CONFIG['input_file']\n",
    "\n",
    "# Load the initial graph from JSON\n",
    "G = load_graph_from_json(DATA_FILE)\n",
    "\n",
    "if G.number_of_nodes() == 0:\n",
    "    print(\"ERROR: No graph data loaded. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "# Apply strategy-specific filtering\n",
    "strategy = CONFIG['pipeline']['strategy']\n",
    "ego_username = CONFIG['pipeline'].get('ego_username')\n",
    "\n",
    "G_filtered = apply_strategy_filtering(G, strategy, ego_username)\n",
    "\n",
    "if G_filtered.number_of_nodes() == 0:\n",
    "    print(\"WARNING: Strategy filtering resulted in empty graph.\")\n",
    "    G_filtered = G  # Fall back to original graph\n",
    "\n",
    "# Apply k-core pruning based on strategy\n",
    "k_value = CONFIG['pruning']['k_values'].get(strategy, CONFIG['pruning']['default_k_value'])\n",
    "print(f\"Applying k-core pruning with k={k_value}...\")\n",
    "G_final = prune_graph(G_filtered, k_value)\n",
    "\n",
    "print(f\"Final processed graph: {G_final.number_of_nodes():,} nodes, {G_final.number_of_edges():,} edges.\")\n",
    "\n",
    "# Record graph loading timing\n",
    "graph_load_end = time.perf_counter()\n",
    "timing_info['Graph Loading'] = graph_load_end - graph_load_start\n",
    "log_analysis_step(\"Graph Loading Complete\", \n",
    "                 f\"{G_final.number_of_nodes():,} nodes, {G_final.number_of_edges():,} edges\", \n",
    "                 timing_info['Graph Loading'])\n",
    "\n",
    "print(\"=====================================\\n\")\n",
    "\n",
    "# Update the main graph variable for downstream processing\n",
    "G = G_final\n",
    "\n",
    "# =================================== PROGRESS TRACKER CLASS ===================================\n",
    "# Comprehensive progress tracking for long-running operations\n",
    "\n",
    "class ProgressTracker:\n",
    "    \"\"\"\n",
    "    A utility class to print progress updates for long-running loops.\n",
    "    \n",
    "    Usage:\n",
    "        tracker = ProgressTracker(total=1000, title=\"Processing items\")\n",
    "        for i in range(1000):\n",
    "            tracker.update(i + 1)\n",
    "        tracker.complete()\n",
    "        \n",
    "    For chunked tasks, set num_updates to number of chunks:\n",
    "        tracker = ProgressTracker(total=50, title=\"Processing\", num_updates=50)\n",
    "        for i in range(0, 5000, 100):  # 50 chunks of 100 items\n",
    "            # ... do work ...\n",
    "            tracker.update((i // 100) + 1)  # Update with chunk number\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def _format_time(seconds: float) -> str:\n",
    "        \"\"\"Convert seconds to human-readable format.\"\"\"\n",
    "        if seconds < 60.0:\n",
    "            return f\"{seconds:.1f}s\"\n",
    "        minutes = int(seconds // 60)\n",
    "        remaining_sec = int(seconds % 60)\n",
    "        if minutes < 60:\n",
    "            return f\"{minutes}m {remaining_sec}s\"\n",
    "        hours = minutes // 60\n",
    "        remaining_min = minutes % 60\n",
    "        return f\"{hours}h {remaining_min}m\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        total: int,\n",
    "        title: str,\n",
    "        num_updates: int = 10,\n",
    "        threshold_sec: float = 3.0,\n",
    "        show_percentage: bool = True,\n",
    "        show_count: bool = True,\n",
    "        bar_width: int = 30\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the progress tracker.\n",
    "        \n",
    "        Args:\n",
    "            total: Total number of items/chunks to process\n",
    "            title: Title to display\n",
    "            num_updates: Target number of progress updates (usually = num chunks)\n",
    "            threshold_sec: Only show progress if total task estimated to exceed this (seconds)\n",
    "            show_percentage: Include percentage in output\n",
    "            show_count: Include item count in output\n",
    "            bar_width: Width of progress bar in characters (0 to disable)\n",
    "        \"\"\"\n",
    "        self.total = max(1, total)\n",
    "        self.title = title\n",
    "        self.num_updates = max(1, num_updates)\n",
    "        self.threshold_sec = max(0.1, threshold_sec)\n",
    "        self.show_percentage = show_percentage\n",
    "        self.show_count = show_count\n",
    "        self.bar_width = max(0, bar_width)\n",
    "        self.show_progress = False\n",
    "\n",
    "        self.title_printed = False \n",
    "        self.decision_made = False \n",
    "        self.update_every_n = max(1, self.total // self.num_updates)\n",
    "        \n",
    "        self.start_time = time.perf_counter()\n",
    "        self.last_printed_item = -1\n",
    "        \n",
    "        # For time estimation\n",
    "        self.first_chunk_time = None  # Time taken for first chunk\n",
    "        self.ema_rate = None  # Exponential moving average of processing rate\n",
    "        self.ema_alpha = 0.3  # Smoothing factor\n",
    "\n",
    "        # Print the title and an initial \"0%\" line immediately.\n",
    "        print(f\"{self.title}...\")\n",
    "        self.title_printed = True\n",
    "        \n",
    "        progress_str = self._format_progress_line(\n",
    "            current_item=0,\n",
    "            percent=0.0,\n",
    "            elapsed=0.0,\n",
    "            remaining_time=0.0  # 0.0 remaining_time triggers our fallback\n",
    "        )\n",
    "        print(progress_str, end=\"\\r\", flush=True)\n",
    "        self.last_printed_item = 0 # Mark item 0 as \"printed\"\n",
    "    \n",
    "    def __enter__(self):\n",
    "        \"\"\"Context manager entry.\"\"\"\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        \"\"\"Context manager exit.\"\"\"\n",
    "        self.complete()\n",
    "        return False\n",
    "    \n",
    "    def _make_decision(self, current_item: int) -> None:\n",
    "        \"\"\"\n",
    "        After first chunk, estimate total time and decide whether to show progress.\n",
    "        \"\"\"\n",
    "        if self.decision_made or current_item == 0:\n",
    "            return\n",
    "        \n",
    "        elapsed = time.perf_counter() - self.start_time\n",
    "        self.first_chunk_time = elapsed\n",
    "        \n",
    "        # Estimate total time based on first chunk\n",
    "        if current_item > 0:\n",
    "            chunks_remaining = self.total - current_item\n",
    "            estimated_total_time = elapsed + (chunks_remaining * (elapsed / current_item))\n",
    "        else:\n",
    "            estimated_total_time = 0\n",
    "        \n",
    "        # Decide: show progress only if estimated total exceeds threshold\n",
    "        self.show_progress = estimated_total_time > self.threshold_sec\n",
    "        self.decision_made = True\n",
    "        \n",
    "        if self.show_progress and not self.title_printed:\n",
    "            print(f\"{self.title}...\")\n",
    "            self.title_printed = True\n",
    "    \n",
    "    def _estimate_remaining_time(self, current_item: int) -> float:\n",
    "        \"\"\"\n",
    "        Estimate remaining time using exponential moving average of rate.\n",
    "        \"\"\"\n",
    "        if current_item <= 0:\n",
    "            return 0.0\n",
    "        \n",
    "        elapsed = time.perf_counter() - self.start_time\n",
    "        \n",
    "        if elapsed <= 0:\n",
    "            return 0.0\n",
    "        \n",
    "        # Calculate rate for this item\n",
    "        current_rate = current_item / elapsed\n",
    "        \n",
    "        # Update EMA\n",
    "        if self.ema_rate is None:\n",
    "            self.ema_rate = current_rate\n",
    "        else:\n",
    "            self.ema_rate = (self.ema_alpha * current_rate + \n",
    "                            (1 - self.ema_alpha) * self.ema_rate)\n",
    "        \n",
    "        # Estimate remaining time\n",
    "        remaining_items = self.total - current_item\n",
    "        if self.ema_rate > 0:\n",
    "            return remaining_items / self.ema_rate\n",
    "        return 0.0\n",
    "    \n",
    "    def _create_progress_bar(self, percent: float) -> str:\n",
    "        \"\"\"\n",
    "        Generate ASCII progress bar.\n",
    "        \"\"\"\n",
    "        if self.bar_width <= 0:\n",
    "            return \"\"\n",
    "        \n",
    "        # Calculate how many full '█' characters to show\n",
    "        filled_len = int(self.bar_width * percent / 100.0)\n",
    "        \n",
    "        # Create the bar string\n",
    "        bar_fill = '█' * filled_len\n",
    "        \n",
    "        # Pad the rest with spaces to match the total bar_width\n",
    "        bar = bar_fill.ljust(self.bar_width, ' ')\n",
    "        \n",
    "        return f\" [{bar}]\"\n",
    "    \n",
    "    def _format_progress_line(\n",
    "        self,\n",
    "        current_item: int,\n",
    "        percent: float,\n",
    "        elapsed: float,\n",
    "        remaining_time: float\n",
    "    ) -> str:\n",
    "        \"\"\"Format the progress line with all requested information.\"\"\"\n",
    "        parts = []\n",
    "        \n",
    "        # Count information\n",
    "        if self.show_count:\n",
    "            parts.append(f\"{current_item}/{self.total}\")\n",
    "        \n",
    "        # Percentage\n",
    "        if self.show_percentage:\n",
    "            parts.append(f\"({percent:.1f}%)\")\n",
    "        \n",
    "        # Progress bar\n",
    "        bar = self._create_progress_bar(percent)\n",
    "        if bar:\n",
    "            parts.append(bar)\n",
    "        \n",
    "        # Time information\n",
    "        if percent < 100.0 and percent > 1.0 and current_item < self.total:\n",
    "            if remaining_time > 0:\n",
    "                remaining_str = self._format_time(remaining_time)\n",
    "                parts.append(f\"- Est. {remaining_str} remaining\")\n",
    "            else:\n",
    "                parts.append(\"- Processing...\") \n",
    "        elif current_item >= self.total:\n",
    "            elapsed_str = self._format_time(elapsed)\n",
    "            parts.append(f\"- Completed in {elapsed_str}\")\n",
    "        \n",
    "        return \"    Progress: \" + \" \".join(parts)\n",
    "    \n",
    "    def update(self, current_item: int) -> None:\n",
    "        \"\"\"\n",
    "        Call this inside the loop with the current item count (1-based).\n",
    "        \n",
    "        Args:\n",
    "            current_item: Current iteration number (1-based) or chunk number\n",
    "        \"\"\"\n",
    "        is_last_item = (current_item >= self.total)\n",
    "        is_update_step = (current_item % self.update_every_n == 0)\n",
    "\n",
    "        # Decide whether to show progress\n",
    "        if not self.decision_made:\n",
    "            if (is_update_step and current_item > 0) or is_last_item:\n",
    "                self._make_decision(current_item)\n",
    "        \n",
    "        if not self.show_progress:\n",
    "            return  # Hides all output for short tasks\n",
    "\n",
    "        if (is_update_step or is_last_item) and current_item > self.last_printed_item:\n",
    "            \n",
    "            self.last_printed_item = current_item\n",
    "            current_item = min(current_item, self.total)\n",
    "            \n",
    "            elapsed = time.perf_counter() - self.start_time\n",
    "            percent = (current_item / self.total) * 100.0\n",
    "            remaining_time = self._estimate_remaining_time(current_item)\n",
    "            \n",
    "            # Format and print progress\n",
    "            progress_str = self._format_progress_line(\n",
    "                current_item,\n",
    "                percent,\n",
    "                elapsed,\n",
    "                remaining_time\n",
    "            )\n",
    "            print(progress_str, flush=True)\n",
    "    \n",
    "    def complete(self) -> None:\n",
    "        \"\"\"Call after the loop finishes. Prints completion info.\"\"\"\n",
    "        \n",
    "        if self.show_progress:\n",
    "            # Print final 100% line\n",
    "            self.update(self.total)\n",
    "    \n",
    "    def reset(self, total: Optional[int] = None) -> None:\n",
    "        \"\"\"Reset the tracker to start a new task (useful for reuse).\"\"\"\n",
    "        if total is not None:\n",
    "            self.total = max(1, total)\n",
    "        \n",
    "        self.start_time = time.perf_counter()\n",
    "        self.last_printed_item = -1 \n",
    "        self.ema_rate = None\n",
    "        self.show_progress = False\n",
    "        self.title_printed = False\n",
    "        self.decision_made = False\n",
    "        self.first_chunk_time = None\n",
    "\n",
    "# =================================== NETWORK ANALYSIS AND COMMUNITY DETECTION ===================================\n",
    "# Advanced network analysis with community detection and centrality calculations\n",
    "\n",
    "def analyze_network(G: nx.DiGraph) -> nx.DiGraph:\n",
    "    \"\"\"\n",
    "    Calculates network metrics and attaches them as node attributes.\n",
    "    Performs community detection and centrality calculations.\n",
    "    \n",
    "    Args:\n",
    "        G (nx.DiGraph): Input graph\n",
    "        \n",
    "    Returns:\n",
    "        nx.DiGraph: Graph with analysis results stored as node attributes\n",
    "    \"\"\"\n",
    "    MIN_NODES_FOR_ANALYSIS = 2\n",
    "    \n",
    "    if G.number_of_nodes() < MIN_NODES_FOR_ANALYSIS:\n",
    "        print(f\"WARNING: Graph has fewer than {MIN_NODES_FOR_ANALYSIS} nodes. Skipping analysis.\")\n",
    "        return G\n",
    "    \n",
    "    # This task has 3 main steps: 1. Louvain, 2. Betweenness, 3. Eigenvector\n",
    "    # We set total=3 and update after each step.\n",
    "    # We also give it a short threshold_sec (e.g., 1.0s) so it appears\n",
    "    # even if the first step (Louvain) is fast.\n",
    "    tracker_title = \"Analyzing structure (Communities, Centrality)\"\n",
    "    tracker = ProgressTracker(\n",
    "        total=3, \n",
    "        title=tracker_title, \n",
    "        num_updates=3, \n",
    "        threshold_sec=1.0, \n",
    "        bar_width=30\n",
    "    )\n",
    "    \n",
    "    # --- 1. Community Detection (Louvain) ---\n",
    "    G_undirected = G.to_undirected()\n",
    "    \n",
    "    if G_undirected.number_of_edges() == 0:\n",
    "        print(\"WARNING: Graph has no edges. Skipping Louvain community detection.\")\n",
    "        communities = []\n",
    "    else:\n",
    "        communities = community.louvain_communities(G_undirected, seed=123)\n",
    "    \n",
    "    print(f\"Detected {len(communities)} communities (Louvain).\")\n",
    "    partition = {node: i for i, comm in enumerate(communities) for node in comm}\n",
    "    nx.set_node_attributes(G, partition, 'community')\n",
    "    \n",
    "    tracker.update(1)  # Step 1 complete\n",
    "\n",
    "    # --- 2. Centrality Measures ---\n",
    "    degree_dict = dict(G.degree())\n",
    "    \n",
    "    try:\n",
    "        k_sample = int(math.sqrt(G.number_of_nodes()))\n",
    "        betweenness_dict = nx.betweenness_centrality(G, k=k_sample, seed=123)\n",
    "    except Exception as e:\n",
    "        print(f\"WARNING: Could not calculate betweenness centrality ({e}). Defaulting to 0.\")\n",
    "        betweenness_dict = {node: 0 for node in G.nodes()}\n",
    "\n",
    "    tracker.update(2)  # Step 2 complete\n",
    "\n",
    "    try:\n",
    "        eigenvector_dict = nx.eigenvector_centrality(G, max_iter=1000, nstart={n:1 for n in G.nodes()})\n",
    "    except Exception as e:\n",
    "        print(f\"WARNING: Could not calculate eigenvector centrality ({e}). Defaulting to 0.\")\n",
    "        eigenvector_dict = {node: 0 for node in G.nodes()}\n",
    "\n",
    "    tracker.update(3)  # Step 3 complete\n",
    "    tracker.complete()  # All steps done\n",
    "\n",
    "    # Store all metrics as node attributes\n",
    "    nx.set_node_attributes(G, degree_dict, 'degree')\n",
    "    nx.set_node_attributes(G, betweenness_dict, 'betweenness')\n",
    "    nx.set_node_attributes(G, eigenvector_dict, 'eigenvector')\n",
    "    \n",
    "    print(f\"Network analysis complete. Communities: {len(communities)}, Nodes analyzed: {G.number_of_nodes():,}\")\n",
    "    return G\n",
    "\n",
    "# =================================== PERFORM NETWORK ANALYSIS ===================================\n",
    "# Apply community detection and centrality analysis if not skipped\n",
    "\n",
    "if not CONFIG['pipeline']['skip_analysis']:\n",
    "    print(\"=== PERFORMING NETWORK ANALYSIS ===\")\n",
    "    network_analysis_start = time.perf_counter()\n",
    "    log_analysis_step(\"Network Analysis Started\", \"Community detection and centrality calculations\")\n",
    "    \n",
    "    G = analyze_network(G)\n",
    "    \n",
    "    network_analysis_end = time.perf_counter()\n",
    "    timing_info['Network Analysis'] = network_analysis_end - network_analysis_start\n",
    "    log_analysis_step(\"Network Analysis Complete\", \n",
    "                     f\"Communities and centrality calculated\", \n",
    "                     timing_info['Network Analysis'])\n",
    "    \n",
    "    print(\"Network analysis completed.\")\n",
    "    print(\"====================================\\n\")\n",
    "else:\n",
    "    print(\"=== SKIPPING NETWORK ANALYSIS (as configured) ===\")\n",
    "    log_analysis_step(\"Network Analysis Skipped\", \"Using default attributes for visualization\")\n",
    "    \n",
    "    # Set default attributes for visualization compatibility\n",
    "    degree_dict = dict(G.degree())\n",
    "    nx.set_node_attributes(G, degree_dict, 'degree')\n",
    "    nx.set_node_attributes(G, {n: 0 for n in G.nodes()}, 'community')\n",
    "    nx.set_node_attributes(G, {n: 0.0 for n in G.nodes()}, 'betweenness')\n",
    "    nx.set_node_attributes(G, {n: 0.0 for n in G.nodes()}, 'eigenvector')\n",
    "    \n",
    "    timing_info['Network Analysis'] = 0.0\n",
    "    print(\"Default attributes set for visualization compatibility.\")\n",
    "    print(\"====================================================\\n\")\n",
    "\n",
    "# =================================== FAME ANALYSIS AND PATH FINDING ===================================\n",
    "# Advanced fame analysis with configurable thresholds and path finding capabilities\n",
    "\n",
    "def find_famous_accounts(G: nx.DiGraph, min_followers: int, min_ratio: float) -> Tuple[List[Dict], List[Dict]]:\n",
    "    \"\"\"\n",
    "    Analyzes the full graph to find accounts that match the \"famous\" heuristic:\n",
    "    - High followers (in-degree)\n",
    "    - Low following (out-degree)\n",
    "    - High ratio of followers-to-following\n",
    "    \n",
    "    Args:\n",
    "        G (nx.DiGraph): Input graph\n",
    "        min_followers (int): Minimum followers within network\n",
    "        min_ratio (float): Minimum followers/following ratio\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[List[Dict], List[Dict]]: (unreachable_famous, reachable_famous)\n",
    "                                     - unreachable_famous: List of famous accounts with following_in_network=0\n",
    "                                     - reachable_famous: List of famous accounts with following_in_network>=1\n",
    "    \"\"\"\n",
    "    unreachable_famous = []\n",
    "    reachable_famous = []\n",
    "    \n",
    "    if G.number_of_nodes() == 0:\n",
    "        return unreachable_famous, reachable_famous\n",
    "    \n",
    "    # Get all degrees at once in optimized bulk operations\n",
    "    print(\"       Caching all in-degrees and out-degrees...\")\n",
    "    in_degree_dict = dict(G.in_degree())\n",
    "    out_degree_dict = dict(G.out_degree())\n",
    "\n",
    "    # Use .items() to iterate over nodes and their pre-calculated in-degree\n",
    "    for node, in_deg in in_degree_dict.items():\n",
    "        # 1. Filter out nodes that aren't popular enough in your network\n",
    "        if in_deg < min_followers:\n",
    "            continue\n",
    "        \n",
    "        # Get the pre-calculated out-degree\n",
    "        out_deg = out_degree_dict.get(node, 0)  # .get() is safer\n",
    "\n",
    "        # 2. Calculate fame ratio (handle division by zero)\n",
    "        ratio = float('inf')  # Highest possible ratio\n",
    "        if out_deg > 0:\n",
    "            ratio = in_deg / out_deg\n",
    "        \n",
    "        # 3. Filter by ratio\n",
    "        if ratio >= min_ratio:\n",
    "            account_info = {\n",
    "                \"username\": node,\n",
    "                \"followers_in_network\": in_deg,\n",
    "                \"following_in_network\": out_deg,\n",
    "                \"ratio\": ratio\n",
    "            }\n",
    "            \n",
    "            # Separate into unreachable (follows nobody) and reachable\n",
    "            if out_deg == 0:\n",
    "                unreachable_famous.append(account_info)\n",
    "            else:\n",
    "                reachable_famous.append(account_info)\n",
    "    \n",
    "    # Sort by ratio (descending), then by followers (descending)\n",
    "    unreachable_famous.sort(key=lambda x: (x['ratio'], x['followers_in_network']), reverse=True)\n",
    "    reachable_famous.sort(key=lambda x: (x['ratio'], x['followers_in_network']), reverse=True)\n",
    "    \n",
    "    return unreachable_famous, reachable_famous\n",
    "\n",
    "def get_contact_path(G: nx.DiGraph, ego_username: str, target_username: str) -> Optional[List[str]]:\n",
    "    \"\"\"\n",
    "    Silently finds the shortest path from target to ego.\n",
    "    \n",
    "    Args:\n",
    "        G (nx.DiGraph): Input graph\n",
    "        ego_username (str): The ego node (destination)\n",
    "        target_username (str): The target node (source)\n",
    "    \n",
    "    Returns:\n",
    "        Optional[List[str]]: The path (a list of nodes) if found, None otherwise\n",
    "    \"\"\"\n",
    "    if ego_username not in G or target_username not in G:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        path = nx.shortest_path(G, source=target_username, target=ego_username)\n",
    "        return path\n",
    "    except (nx.NetworkXNoPath, nx.NodeNotFound):\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Unexpected error in get_contact_path ({target_username}): {e}\")\n",
    "        return None\n",
    "        \n",
    "def print_detailed_contact_path(G: nx.DiGraph, ego_username: str, target_username: str) -> bool:\n",
    "    \"\"\"\n",
    "    Finds and prints a detailed \"chain of influence\" from a target user back to the ego.\n",
    "    This is a verbose function for a single manual target.\n",
    "    \n",
    "    Args:\n",
    "        G (nx.DiGraph): Input graph\n",
    "        ego_username (str): The ego node (destination)\n",
    "        target_username (str): The target node (source)\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if a path was found, False otherwise\n",
    "    \"\"\"\n",
    "    print(f\"\\n---- Finding Path: {target_username} -> {ego_username} ----\")\n",
    "\n",
    "    if ego_username not in G:\n",
    "        print(f\"ERROR: Ego node '{ego_username}' not in the *full* graph.\")\n",
    "        return False\n",
    "    if target_username not in G:\n",
    "        print(f\"ERROR: Target node '{target_username}' not in the *full* graph.\")\n",
    "        print(\"       This person might not be in your L1/L2 network, or you may have a typo.\")\n",
    "        return False\n",
    "\n",
    "    path = get_contact_path(G, ego_username, target_username)\n",
    "\n",
    "    if path:\n",
    "        print(\"\\n*** SUCCESS: Contact path found! ***\")\n",
    "        \n",
    "        print(\"       Follows Chain (Target to Ego):\")\n",
    "        for i in range(len(path) - 1):\n",
    "            print(f\"         {path[i]:<20} -> follows -> {path[i+1]}\")\n",
    "            \n",
    "        print(f\"       Path length: {len(path) - 1} step(s).\")\n",
    "        \n",
    "        print(\"       Action Plan (Read from bottom up):\")\n",
    "        print(f\"         1. You contact:         {path[-2]}\")\n",
    "        \n",
    "        action_step = 2\n",
    "        for i in range(len(path) - 2, 0, -1):\n",
    "            print(f\"         {action_step}. {path[i]} contacts: {path[i-1]}\")\n",
    "            action_step += 1\n",
    "            \n",
    "        print(f\"         ...who can contact '{path[0]}'.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"\\n*** NO PATH found from '{target_username}' to '{ego_username}'. ***\")\n",
    "        print(\"       No 'chain of influence' exists in your network.\")\n",
    "        return False\n",
    "\n",
    "def perform_fame_analysis(G: nx.DiGraph, config: dict) -> None:\n",
    "    \"\"\"\n",
    "    Performs comprehensive fame analysis and path finding based on configuration.\n",
    "    \n",
    "    Args:\n",
    "        G (nx.DiGraph): Input graph\n",
    "        config (dict): Configuration dictionary\n",
    "    \"\"\"\n",
    "    print(\"=== PERFORMING FAME ANALYSIS ===\")\n",
    "    \n",
    "    # Get fame analysis parameters\n",
    "    min_followers = config['fame_analysis']['min_followers_in_network']\n",
    "    min_ratio = config['fame_analysis']['min_fame_ratio']\n",
    "    find_paths_to_all = config['fame_analysis']['find_paths_to_all_famous']\n",
    "    \n",
    "    print(f\"Fame criteria: min {min_followers} followers, min {min_ratio:.1f}x ratio\")\n",
    "    \n",
    "    # Find famous accounts\n",
    "    unreachable_famous, reachable_famous = find_famous_accounts(G, min_followers, min_ratio)\n",
    "    \n",
    "    total_famous = len(unreachable_famous) + len(reachable_famous)\n",
    "    print(f\"Found {total_famous} famous accounts ({len(reachable_famous)} reachable, {len(unreachable_famous)} unreachable)\")\n",
    "    \n",
    "    if total_famous == 0:\n",
    "        print(\"No famous accounts found with current criteria.\")\n",
    "        return\n",
    "    \n",
    "    # Display top famous accounts\n",
    "    print(\"\\n--- TOP FAMOUS ACCOUNTS (REACHABLE) ---\")\n",
    "    for i, account in enumerate(reachable_famous[:10]):  # Show top 10\n",
    "        ratio_str = \"∞\" if account['ratio'] == float('inf') else f\"{account['ratio']:.1f}\"\n",
    "        print(f\"  {i+1:2d}. {account['username']:<20} | {account['followers_in_network']:4d} followers | {account['following_in_network']:3d} following | {ratio_str}x ratio\")\n",
    "    \n",
    "    if unreachable_famous:\n",
    "        print(\"\\n--- TOP FAMOUS ACCOUNTS (UNREACHABLE) ---\")\n",
    "        for i, account in enumerate(unreachable_famous[:5]):  # Show top 5\n",
    "            print(f\"  {i+1:2d}. {account['username']:<20} | {account['followers_in_network']:4d} followers | 0 following | ∞x ratio\")\n",
    "    \n",
    "    # Manual path finding for specific target\n",
    "    manual_target = config['analysis'].get('contact_path_target')\n",
    "    if manual_target:\n",
    "        ego_username = config['pipeline'].get('ego_username', '_alexs.life')\n",
    "        print_detailed_contact_path(G, ego_username, manual_target)\n",
    "    \n",
    "    # Path finding for all famous accounts\n",
    "    if find_paths_to_all and reachable_famous:\n",
    "        ego_username = config['pipeline'].get('ego_username', '_alexs.life')\n",
    "        print(f\"\\n--- CONTACT PATHS TO FAMOUS ACCOUNTS ---\")\n",
    "        \n",
    "        paths_found = 0\n",
    "        for account in reachable_famous[:20]:  # Limit to top 20 for performance\n",
    "            path = get_contact_path(G, ego_username, account['username'])\n",
    "            if path:\n",
    "                paths_found += 1\n",
    "                path_length = len(path) - 1\n",
    "                print(f\"  {account['username']:<20} | {path_length} steps | Path: {' -> '.join(path[:3])}{'...' if len(path) > 3 else ''}\")\n",
    "        \n",
    "        print(f\"\\nFound paths to {paths_found}/{len(reachable_famous[:20])} famous accounts.\")\n",
    "    \n",
    "    print(\"Fame analysis completed.\")\n",
    "    print(\"=================================\\n\")\n",
    "\n",
    "# =================================== PERFORM FAME ANALYSIS ===================================\n",
    "# Execute fame analysis and path finding based on configuration\n",
    "\n",
    "if G.number_of_nodes() > 0:\n",
    "    fame_analysis_start = time.perf_counter()\n",
    "    log_analysis_step(\"Fame Analysis Started\", \"Identifying influential accounts and paths\")\n",
    "    \n",
    "    perform_fame_analysis(G, CONFIG)\n",
    "    \n",
    "    fame_analysis_end = time.perf_counter()\n",
    "    timing_info['Fame Analysis'] = fame_analysis_end - fame_analysis_start\n",
    "    log_analysis_step(\"Fame Analysis Complete\", \n",
    "                     \"Famous accounts identified and paths calculated\", \n",
    "                     timing_info['Fame Analysis'])\n",
    "else:\n",
    "    print(\"=== SKIPPING FAME ANALYSIS (empty graph) ===\")\n",
    "    log_analysis_step(\"Fame Analysis Skipped\", \"Empty graph\")\n",
    "    timing_info['Fame Analysis'] = 0.0\n",
    "    print(\"============================================\\n\")\n",
    "\n",
    "# =================================== UTILITY FUNCTIONS FOR VISUALIZATION ===================================\n",
    "# Helper functions for filename generation, color mapping, and scaling algorithms\n",
    "\n",
    "def generate_output_filename(prefix: str, strategy: str, k_value: int, extension: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a descriptive and unique output filename based on config and time.\n",
    "    Format: {prefix}-{strategy}-k{k_value}-{config_hash}.{extension}\n",
    "    \n",
    "    Args:\n",
    "        prefix (str): Base filename prefix\n",
    "        strategy (str): Analysis strategy name\n",
    "        k_value (int): K-value used for pruning\n",
    "        extension (str): File extension (html, png, txt)\n",
    "        \n",
    "    Returns:\n",
    "        str: Unique filename with timestamp hash\n",
    "    \"\"\"\n",
    "    # Get current timestamp to ensure uniqueness\n",
    "    timestamp = datetime.datetime.now().isoformat()\n",
    "    \n",
    "    # Create a short hash of relevant config and time to avoid collisions\n",
    "    config_str = f\"{strategy}-{k_value}-{timestamp}\"\n",
    "    config_hash = hashlib.sha256(config_str.encode()).hexdigest()[:6]\n",
    "    \n",
    "    return f\"{prefix}-{strategy}-k{k_value}-{config_hash}.{extension}\"\n",
    "\n",
    "def get_community_colors(num_communities: int) -> dict:\n",
    "    \"\"\"\n",
    "    Generates a color map for communities.\n",
    "    Returns a dict mapping community_id -> hex color string and RGBA tuple.\n",
    "    \n",
    "    Args:\n",
    "        num_communities (int): Number of communities to generate colors for\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with 'hex' and 'rgba' color mappings\n",
    "    \"\"\"\n",
    "    if num_communities > 0:\n",
    "        palette = plt.colormaps.get_cmap('viridis').resampled(num_communities)\n",
    "        colors = palette(range(num_communities))\n",
    "        return {\n",
    "            'hex': {i: f'#{int(c[0]*255):02x}{int(c[1]*255):02x}{int(c[2]*255):02x}' \n",
    "                    for i, c in enumerate(colors)},\n",
    "            'rgba': {i: c for i, c in enumerate(colors)}\n",
    "        }\n",
    "    return {'hex': {0: '#808080'}, 'rgba': {0: (0.5, 0.5, 0.5, 1.0)}}\n",
    "\n",
    "def _get_scaled_size(value: float, base_size: float, multiplier: float, algorithm: str) -> float:\n",
    "    \"\"\"\n",
    "    Helper to calculate node/edge size based on a metric's value.\n",
    "    \n",
    "    Args:\n",
    "        value (float): The metric value to scale\n",
    "        base_size (float): Base size before scaling\n",
    "        multiplier (float): Scaling multiplier\n",
    "        algorithm (str): Scaling algorithm ('logarithmic' or 'linear')\n",
    "        \n",
    "    Returns:\n",
    "        float: Scaled size value\n",
    "    \"\"\"\n",
    "    if algorithm == 'logarithmic':\n",
    "        # Use log1p (log(1+x)) to handle 0 values gracefully\n",
    "        return base_size + math.log1p(value) * multiplier\n",
    "    else:  # linear\n",
    "        return base_size + value * multiplier\n",
    "\n",
    "# =================================== NODE AND EDGE METRIC CALCULATIONS ===================================\n",
    "# Advanced metric calculations for both HTML and PNG visualizations\n",
    "\n",
    "def calculate_edge_metrics(G: nx.DiGraph, vis_config: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Calculates edge metrics for both HTML and PNG visualizations.\n",
    "    Returns a dict mapping (u, v) -> {'width': float, 'color': str, 'is_mutual': bool, 'is_bridge': bool, 'common_neighbors': int}\n",
    "    \n",
    "    This function iterates over the undirected graph's edges to avoid processing mutual pairs twice.\n",
    "    It calculates shared metrics (like common_neighbors) once per undirected pair\n",
    "    and then adds the correct directed edge entry/entries to the dictionary.\n",
    "    \n",
    "    Args:\n",
    "        G (nx.DiGraph): Input graph with community attributes\n",
    "        vis_config (dict): Visualization configuration\n",
    "        \n",
    "    Returns:\n",
    "        dict: Edge metrics mapping (u, v) -> metrics dict\n",
    "    \"\"\"\n",
    "    print(\"Calculating shared edge metrics (embeddedness, bridging, reciprocity)...\")\n",
    "    \n",
    "    G_undirected = G.to_undirected()\n",
    "    edge_metrics = {}\n",
    "    \n",
    "    base_edge_width = vis_config.get(\"base_edge_width\", 0.5)\n",
    "    edge_width_multiplier = vis_config.get(\"edge_width_multiplier\", 1.5)\n",
    "    edge_width_scaling = vis_config.get(\"edge_width_scaling\", \"logarithmic\")\n",
    "    bridge_color = vis_config.get(\"bridge_color\", \"#6e6e6e\")\n",
    "    \n",
    "    # Get community color map\n",
    "    num_communities = len(set(nx.get_node_attributes(G, 'community').values()))\n",
    "    color_maps = get_community_colors(num_communities)\n",
    "    color_map_hex = color_maps['hex']\n",
    "    \n",
    "    # Iterate over each UNDIRECTED edge pair once\n",
    "    for u, v in G_undirected.edges():\n",
    "        \n",
    "        # --- 1. Calculate metrics shared by the pair (u, v) ---\n",
    "        \n",
    "        # Calculate common neighbors (embeddedness)\n",
    "        try:\n",
    "            # Use sum(1 for...) to efficiently get iterator length without list()\n",
    "            common_neighbors_iter = nx.common_neighbors(G_undirected, u, v)\n",
    "            num_common = sum(1 for _ in common_neighbors_iter)\n",
    "        except nx.NetworkXError:\n",
    "            num_common = 0\n",
    "        \n",
    "        # Calculate edge width\n",
    "        edge_width = _get_scaled_size(\n",
    "            num_common,\n",
    "            base_edge_width,\n",
    "            edge_width_multiplier,\n",
    "            edge_width_scaling\n",
    "        )\n",
    "        \n",
    "        # Determine if bridge edge\n",
    "        u_comm = G.nodes[u].get('community', -1)\n",
    "        v_comm = G.nodes[v].get('community', -2)\n",
    "        is_bridge = (u_comm != v_comm)\n",
    "        \n",
    "        # Set edge color\n",
    "        if is_bridge:\n",
    "            edge_color = bridge_color\n",
    "        else:\n",
    "            edge_color = color_map_hex.get(u_comm, vis_config.get(\"intra_community_color\", \"#c0c0c0\"))\n",
    "\n",
    "        # --- 2. Check directionality in G and add to metrics dict ---\n",
    "            \n",
    "        has_u_v = G.has_edge(u, v)\n",
    "        has_v_u = G.has_edge(v, u)\n",
    "        is_mutual = has_u_v and has_v_u\n",
    "\n",
    "        # Define the base metrics for this pair\n",
    "        base_metrics = {\n",
    "            'width': edge_width,\n",
    "            'color': edge_color,\n",
    "            'is_bridge': is_bridge,\n",
    "            'common_neighbors': num_common,\n",
    "        }\n",
    "\n",
    "        if is_mutual:\n",
    "            # Add ONE entry for the mutual pair, key (u, v) by convention.\n",
    "            # Downstream functions know to render this as a mutual arrow.\n",
    "            edge_metrics[(u, v)] = {\n",
    "                **base_metrics,\n",
    "                'is_mutual': True,\n",
    "                'u_comm': u_comm, # 'from' node is u\n",
    "                'v_comm': v_comm  # 'to' node is v\n",
    "            }\n",
    "        \n",
    "        else:\n",
    "            # Add entries for each one-way edge that exists\n",
    "            if has_u_v:\n",
    "                edge_metrics[(u, v)] = {\n",
    "                    **base_metrics,\n",
    "                    'is_mutual': False,\n",
    "                    'u_comm': u_comm, # 'from' node is u\n",
    "                    'v_comm': v_comm  # 'to' node is v\n",
    "                }\n",
    "            \n",
    "            if has_v_u:\n",
    "                edge_metrics[(v, u)] = {\n",
    "                    **base_metrics,\n",
    "                    'is_mutual': False,\n",
    "                    'u_comm': v_comm, # 'from' node is v\n",
    "                    'v_comm': u_comm  # 'to' node is u\n",
    "                }\n",
    "                \n",
    "    return edge_metrics\n",
    "\n",
    "def calculate_node_metrics(G: nx.DiGraph, vis_config: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Calculates node size and color information for both HTML and PNG visualizations.\n",
    "    Returns a dict mapping node -> {'size': float, 'community': int, 'color_hex': str, 'color_rgba': tuple}\n",
    "    \n",
    "    Args:\n",
    "        G (nx.DiGraph): Input graph with analysis attributes\n",
    "        vis_config (dict): Visualization configuration\n",
    "        \n",
    "    Returns:\n",
    "        dict: Node metrics mapping node -> metrics dict\n",
    "    \"\"\"\n",
    "    \n",
    "    # Handle the case where analysis was skipped (attributes are missing)\n",
    "    if not nx.get_node_attributes(G, 'community'):\n",
    "        print(\"Using default metrics (degree, community 0) as analysis was skipped.\")\n",
    "        node_metrics = {}\n",
    "        # Set all community IDs to 0\n",
    "        nx.set_node_attributes(G, {n: 0 for n in G.nodes()}, 'community') \n",
    "        \n",
    "        # Set all centrality/degree to 0 or 1 for visualization fallback\n",
    "        for n in G.nodes():\n",
    "            G.nodes[n]['degree'] = G.degree(n) # Use actual degree for sizing fallback\n",
    "            G.nodes[n]['betweenness'] = 0.0\n",
    "            G.nodes[n]['eigenvector'] = 0.0\n",
    "            \n",
    "    # Regular metric calculation\n",
    "    node_metrics = {}\n",
    "    size_metric = vis_config['node_size_metric']\n",
    "    base_size = vis_config['base_node_size']\n",
    "    multiplier = vis_config['node_size_multiplier']\n",
    "    scaling_alg = vis_config['scaling_algorithm']\n",
    "    \n",
    "    # Get community colors\n",
    "    num_communities = len(set(nx.get_node_attributes(G, 'community').values()))\n",
    "    color_maps = get_community_colors(num_communities)\n",
    "    \n",
    "    for node, attrs in G.nodes(data=True):\n",
    "        # Use actual metric if available, otherwise fallback to degree\n",
    "        metric_value = attrs.get(size_metric, attrs.get('degree', 1))\n",
    "        community_id = attrs.get('community', 0)\n",
    "        \n",
    "        node_size = _get_scaled_size(\n",
    "            metric_value,\n",
    "            base_size,\n",
    "            multiplier,\n",
    "            scaling_alg\n",
    "        )\n",
    "        \n",
    "        node_metrics[node] = {\n",
    "            'size': node_size,\n",
    "            'community': community_id,\n",
    "            'color_hex': color_maps['hex'].get(community_id, '#808080'),\n",
    "            'color_rgba': color_maps['rgba'].get(community_id, (0.5, 0.5, 0.5, 1.0)),\n",
    "            'degree': attrs.get('degree', 0),\n",
    "            'betweenness': attrs.get('betweenness', 0),\n",
    "            'eigenvector': attrs.get('eigenvector', 0)\n",
    "        }\n",
    "    \n",
    "    return node_metrics\n",
    "\n",
    "# =================================== INTERACTIVE HTML VISUALIZATION ===================================\n",
    "# Advanced Pyvis-based interactive network visualization with legends and tooltips\n",
    "\n",
    "def visualize_network(G: nx.DiGraph, output_filename: str, node_metrics: dict, edge_metrics: dict) -> bool:\n",
    "    \"\"\"\n",
    "    Generates an interactive HTML file to visualize the network graph.\n",
    "    \n",
    "    Args:\n",
    "        G (nx.DiGraph): The analyzed graph\n",
    "        output_filename (str): Path to save the HTML file\n",
    "        node_metrics (dict): Pre-calculated node size/color metrics\n",
    "        edge_metrics (dict): Pre-calculated edge width/color metrics\n",
    "\n",
    "    Returns:\n",
    "        bool: True if successful, False otherwise\n",
    "    \"\"\"\n",
    "\n",
    "    vis_config = CONFIG['visualization']\n",
    "    pyvis_config = vis_config.get('pyvis_interactive', {}) # Get the new sub-config\n",
    "    \n",
    "    # Use fallback in case the new key is missing\n",
    "    width = pyvis_config.get('width', '100%')\n",
    "    height = pyvis_config.get('height', '90vh')\n",
    "    notebook = pyvis_config.get('notebook', False)\n",
    "    show_labels = pyvis_config.get(\"show_labels\", True)\n",
    "    show_tooltips = pyvis_config.get(\"show_tooltips\", True)\n",
    "    physics_solver = pyvis_config.get(\"physics_solver\", \"forceAtlas2Based\")\n",
    "    \n",
    "    net = Network(\n",
    "        height=height,\n",
    "        width=width,\n",
    "        directed=True,\n",
    "        notebook=notebook,\n",
    "        cdn_resources='remote',\n",
    "        select_menu=True,\n",
    "    )\n",
    "\n",
    "    # Grey out nodes not connected to selected \n",
    "    net.highlight_nearest = True\n",
    "\n",
    "    # The 'highlightNearest' object controls what happens to *unselected* items.\n",
    "    options_json = {\n",
    "        # Enable the GUI (equivalent to net.show_buttons)\n",
    "        \"configure\": {\n",
    "            \"enabled\": True, \n",
    "            \"filter\": ['physics'] # Only show the physics tab\n",
    "        },\n",
    "        \n",
    "        # Custom 'physics' settings\n",
    "        \"physics\": {\n",
    "            \"solver\": physics_solver\n",
    "        },\n",
    "\n",
    "        \"highlightNearest\": {\n",
    "            \"enabled\": True,\n",
    "            \"degree\": 1, \n",
    "            \"nodes\": \"all\",\n",
    "            \"edges\": \"all\", # Ensure edges are included in the dimming logic\n",
    "            \"unselectedColor\": \"#808080\", # Grey color for unselected edges and nodes\n",
    "            \"unselectedNodeOpacity\": 0.3, # Optional: Add opacity control\n",
    "            \"unselectedEdgeOpacity\": 0.3, # Optional: Add opacity control\n",
    "            \"hover\": True, # Keep edges highlighted on hover\n",
    "            \"hideWhenZooming\": False # Keep dimming active when zooming\n",
    "        },\n",
    "\n",
    "        # Highlight selected edges \n",
    "        'interaction': {\n",
    "            'hover': True,\n",
    "            'hoverConnectedEdges': True,\n",
    "            'selectConnectedEdges': True\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Apply the custom options. This overrides the default highlighting behavior.\n",
    "    net.set_options(json.dumps(options_json))\n",
    "\n",
    "    print(\"--- Current net.options (after set_options):\")\n",
    "    # print(net.options) # Commented out for cleaner output\n",
    "    print(\"------------------------------------------\")\n",
    "\n",
    "    # Add nodes using shared metrics\n",
    "    for node, metrics in node_metrics.items():\n",
    "        title_text = (\n",
    "            f\"{node}\\n\\n\"\n",
    "            f\"Community ID: {metrics['community']}\\n\"\n",
    "            f\"Connections (Degree): {metrics['degree']}\\n\"\n",
    "            f\"Betweenness: {metrics['betweenness']:.4f}\\n\"\n",
    "            f\"Eigenvector (Influence): {metrics['eigenvector']:.4f}\"\n",
    "        )\n",
    "\n",
    "        node_label = node if show_labels else None\n",
    "        node_title = title_text if show_tooltips else None\n",
    "        font_config = {'size': 0} if not show_labels else {}\n",
    "\n",
    "        net.add_node(\n",
    "            node,\n",
    "            label=node_label,\n",
    "            size=metrics['size'],\n",
    "            color=metrics['color_hex'],\n",
    "            title=node_title,\n",
    "            font=font_config\n",
    "        )\n",
    "    \n",
    "    # Add edges using shared metrics\n",
    "    for (u, v), metrics in edge_metrics.items():\n",
    "        title = (\n",
    "            f\"{u} <-> {v} (Mutual)\\n\" if metrics['is_mutual'] \n",
    "            else f\"{u} -> {v} (One-way)\\n\"\n",
    "        )\n",
    "        title += f\"Common Neighbors: {metrics['common_neighbors']}\\n\"\n",
    "        title += (\n",
    "            f\"BRIDGE: Community {metrics['u_comm']} <-> {metrics['v_comm']}\" \n",
    "            if metrics['is_bridge'] \n",
    "            else f\"INTRA: Community {metrics['u_comm']}\"\n",
    "        )\n",
    "        \n",
    "        edge_title = title if show_tooltips else None\n",
    "        dashes = not metrics['is_mutual']\n",
    "        arrows = 'to, from' if metrics['is_mutual'] else 'to'\n",
    "\n",
    "        net.add_edge(\n",
    "            u, v, \n",
    "            title=edge_title,\n",
    "            color=metrics['color'],\n",
    "            width=metrics['width'],\n",
    "            dashes=dashes,\n",
    "            arrows=arrows\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        # Generate the HTML with legend\n",
    "        html_string = net.generate_html()\n",
    "        \n",
    "        # Create legend HTML\n",
    "        legend_html = create_html_legend(G, vis_config)\n",
    "        \n",
    "        # Insert legend into the HTML\n",
    "        modified_html = insert_legend_into_html(html_string, legend_html)\n",
    "        \n",
    "        # Save the modified HTML\n",
    "        with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(modified_html)\n",
    "        \n",
    "        print(f\"SUCCESS: Interactive HTML with legend saved to: {output_filename}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Could not save interactive HTML: {e}\")\n",
    "        return False\n",
    "\n",
    "def create_html_legend(G: nx.DiGraph, vis_config: dict) -> str:\n",
    "    \"\"\"\n",
    "    Creates an HTML legend for the interactive visualization.\n",
    "    \n",
    "    Args:\n",
    "        G (nx.DiGraph): Input graph with community attributes\n",
    "        vis_config (dict): Visualization configuration\n",
    "        \n",
    "    Returns:\n",
    "        str: HTML string for the legend\n",
    "    \"\"\"\n",
    "    # Get community information\n",
    "    communities_attr = nx.get_node_attributes(G, 'community')\n",
    "    if communities_attr:\n",
    "        num_communities = len(set(communities_attr.values()))\n",
    "    else:\n",
    "        # Fallback if analysis was skipped\n",
    "        num_communities = 0 \n",
    "        \n",
    "    color_maps = get_community_colors(num_communities)\n",
    "    color_map_hex = color_maps['hex']\n",
    "    \n",
    "    # Check if a metric was calculated or defaulted\n",
    "    node_size_metric = vis_config['node_size_metric']\n",
    "    if not communities_attr or node_size_metric == 'degree':\n",
    "        # Default metric for degree, or if analysis was skipped (will default to actual degree)\n",
    "        node_size_metric_display = 'Degree (Connections)' \n",
    "    else:\n",
    "        node_size_metric_display = node_size_metric.capitalize()\n",
    "    \n",
    "    # Create community legend items\n",
    "    community_items = \"\"\n",
    "    if num_communities > 0:\n",
    "        for i in range(min(num_communities, 10)):  # Limit to first 10 communities\n",
    "            color = color_map_hex.get(i, '#808080')\n",
    "            community_items += f'''\n",
    "            <div style=\"display: flex; align-items: center; margin-bottom: 5px;\">\n",
    "                <div style=\"width: 20px; height: 20px; background-color: {color}; margin-right: 10px; border: 1px solid #ccc;\"></div>\n",
    "                <span>Community {i}</span>\n",
    "            </div>\n",
    "            '''\n",
    "        \n",
    "        if num_communities > 10:\n",
    "            community_items += f'<div style=\"color: #666; font-style: italic;\">...and {num_communities - 10} more communities</div>'\n",
    "    else:\n",
    "        community_items = '<div style=\"color: #666; font-style: italic;\">(Community detection skipped or failed)</div>'\n",
    "        \n",
    "    \n",
    "    legend_html = f'''\n",
    "    <div id=\"legend\" style=\"\n",
    "        position: absolute; \n",
    "        bottom: 10px;\n",
    "        right: 10px;\n",
    "        background: rgba(255, 255, 255, 0.9); \n",
    "        border: 1px solid #ccc; \n",
    "        padding: 15px; \n",
    "        border-radius: 5px; \n",
    "        font-family: Arial, sans-serif; \n",
    "        font-size: 14px; \n",
    "        max-width: 300px;\n",
    "        box-shadow: 0 2px 10px rgba(0,0,0,0.1);\n",
    "        z-index: 1000;\n",
    "    \">\n",
    "        <h3 style=\"margin-top: 0; margin-bottom: 15px; font-size: 16px; border-bottom: 1px solid #eee; padding-bottom: 5px;\">Network Legend</h3>\n",
    "        \n",
    "        <div style=\"margin-bottom: 15px;\">\n",
    "            <h4 style=\"margin-bottom: 10px; font-size: 14px;\">Communities</h4>\n",
    "            {community_items}\n",
    "        </div>\n",
    "        \n",
    "        <div style=\"margin-bottom: 15px;\">\n",
    "            <h4 style=\"margin-bottom: 10px; font-size: 14px;\">Edge Types</h4>\n",
    "            <div style=\"display: flex; align-items: center; margin-bottom: 5px;\">\n",
    "                <div style=\"width: 40px; height: 6px; background: #c0c0c0; margin-right: 10px;\"></div>\n",
    "                <span>Mutual Connection</span>\n",
    "            </div>\n",
    "            <div style=\"display: flex; align-items: center; margin-bottom: 5px;\">\n",
    "                <div style=\"width: 40px; height: 2px; background: #c0c0c0; margin-right: 10px; border: 1px dashed #666;\"></div>\n",
    "                <span>One-way Connection</span>\n",
    "            </div>\n",
    "            <div style=\"display: flex; align-items: center;\">\n",
    "                <div style=\"width: 40px; height: 2px; background: {vis_config.get('bridge_color', '#6e6e6e')}; margin-right: 10px;\"></div>\n",
    "                <span>Bridge Connection</span>\n",
    "            </div>\n",
    "        </div>\n",
    "        \n",
    "        <div style=\"margin-bottom: 15px;\">\n",
    "            <h4 style=\"margin-bottom: 10px; font-size: 14px;\">Node Size</h4>\n",
    "            <div>Larger nodes have higher {node_size_metric_display}</div>\n",
    "        </div>\n",
    "        \n",
    "        <div style=\"font-size: 12px; color: #666;\">\n",
    "            <strong>Interactions:</strong><br>\n",
    "            • Click and drag to move nodes<br>\n",
    "            • Scroll to zoom<br>\n",
    "            • Hover for details<br>\n",
    "            • Drag background to pan\n",
    "        </div>\n",
    "    </div>\n",
    "    '''\n",
    "    \n",
    "    return legend_html\n",
    "\n",
    "def insert_legend_into_html(html_string: str, legend_html: str) -> str:\n",
    "    \"\"\"\n",
    "    Inserts the legend HTML into the main HTML string.\n",
    "    \n",
    "    Args:\n",
    "        html_string (str): Original HTML content\n",
    "        legend_html (str): Legend HTML to insert\n",
    "        \n",
    "    Returns:\n",
    "        str: Modified HTML with legend inserted\n",
    "    \"\"\"\n",
    "    # Find the position to insert the legend (before the closing body tag)\n",
    "    insert_position = html_string.find('</body>')\n",
    "    \n",
    "    if insert_position != -1:\n",
    "        return html_string[:insert_position] + legend_html + html_string[insert_position:]\n",
    "    else:\n",
    "        # If no body tag found, append to the end\n",
    "        return html_string + legend_html\n",
    "\n",
    "# =================================== STATIC PNG VISUALIZATION ===================================\n",
    "# Advanced matplotlib-based static network visualization with multiple layout options\n",
    "\n",
    "def generate_static_graph(G: nx.DiGraph, output_filename: str, node_metrics: dict, edge_metrics: dict) -> bool:\n",
    "    \"\"\"\n",
    "    Generates a static PNG image of the network graph using matplotlib.\n",
    "    \n",
    "    Args:\n",
    "        G (nx.DiGraph): The analyzed graph\n",
    "        output_filename (str): Path to save the PNG file\n",
    "        node_metrics (dict): Pre-calculated node size/color metrics\n",
    "        edge_metrics (dict): Pre-calculated edge width/color metrics\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    static_config = CONFIG['visualization']['static_image']\n",
    "    \n",
    "    if G.number_of_nodes() == 0:\n",
    "        print(\"WARNING: Cannot generate static graph. Graph is empty.\")\n",
    "        return False\n",
    "\n",
    "    # Setup figure\n",
    "    fig, ax = plt.subplots(figsize=static_config['image_size_inches'])\n",
    "    ax.set_facecolor('white')\n",
    "    fig.patch.set_facecolor('white')\n",
    "\n",
    "    # Calculate layout\n",
    "    layout_type = static_config['layout']\n",
    "    G_undirected = G.to_undirected() \n",
    "    \n",
    "    # Use ProgressTracker for spring layout, add timing for others\n",
    "    if layout_type == 'spring':\n",
    "        # Get parameters from config\n",
    "        total_iterations = static_config.get('spring_iterations', 50)\n",
    "        k_value = static_config.get('spring_k', 0.3)\n",
    "        seed_value = 123\n",
    "        \n",
    "        # Using a fixed number of updates (e.g., 10 updates)\n",
    "        num_updates = 10 \n",
    "        chunk_size = max(1, total_iterations // num_updates)\n",
    "        \n",
    "        pos = None # Initialize position to None to start fresh\n",
    "        \n",
    "        # Use ProgressTracker\n",
    "        title = f\"Calculating 'spring' layout ({total_iterations} iterations)\"\n",
    "        tracker = ProgressTracker(total=total_iterations, title=title, num_updates=num_updates)\n",
    "        \n",
    "        # Loop through chunks\n",
    "        for i in range(0, total_iterations, chunk_size):\n",
    "            remaining_iters = total_iterations - i\n",
    "            iters_in_this_step = min(chunk_size, remaining_iters)\n",
    "            \n",
    "            # Use 'pos' from the previous step to continue the layout calculation\n",
    "            pos = nx.spring_layout(G_undirected, \n",
    "                                  k=k_value, \n",
    "                                  iterations=iters_in_this_step, \n",
    "                                  seed=seed_value, \n",
    "                                  pos=pos) \n",
    "            \n",
    "            # Print progress update\n",
    "            current_progress = i + iters_in_this_step\n",
    "            tracker.update(current_progress)\n",
    "            \n",
    "        tracker.complete()\n",
    "\n",
    "    else:\n",
    "        # Add timing for other non-iterative layouts\n",
    "        print(f\"Calculating '{layout_type}' layout (this may take a while)...\")\n",
    "        start_layout_time = time.perf_counter()\n",
    "        \n",
    "        if layout_type == 'kamada_kawai':\n",
    "            pos = nx.kamada_kawai_layout(G_undirected)\n",
    "        elif layout_type == 'circular':\n",
    "            pos = nx.circular_layout(G_undirected)\n",
    "        elif layout_type == 'shell':\n",
    "            communities_dict = nx.get_node_attributes(G, 'community')\n",
    "            if communities_dict:\n",
    "                num_communities = len(set(communities_dict.values()))\n",
    "                nodelist = [[] for _ in range(num_communities)]\n",
    "                for node, comm in communities_dict.items():\n",
    "                    if comm < num_communities:\n",
    "                        nodelist[comm].append(node)\n",
    "                pos = nx.shell_layout(G, nodelist=nodelist)\n",
    "            else:\n",
    "                pos = nx.shell_layout(G_undirected)\n",
    "        else:\n",
    "            print(f\"WARNING: Unknown layout '{layout_type}'. Defaulting to 'spring'.\")\n",
    "            # Use a single, non-iterative spring_layout as fallback\n",
    "            pos = nx.spring_layout(G_undirected, seed=123)\n",
    "\n",
    "        end_layout_time = time.perf_counter()\n",
    "        print(f\"    Layout finished in {end_layout_time - start_layout_time:.2f}s\")\n",
    "\n",
    "    # Prepare lists for batch drawing instead of drawing in a loop\n",
    "    print(\"Preparing edge batches...\")\n",
    "    \n",
    "    mutual_edges = []\n",
    "    mutual_colors_rgb = []\n",
    "    mutual_widths_mpl = []\n",
    "    \n",
    "    oneway_edges = []\n",
    "    oneway_colors_rgb = []\n",
    "    oneway_widths_mpl = []\n",
    "    \n",
    "    alpha = static_config.get('edge_alpha', 0.3)\n",
    "    \n",
    "    for (u, v), metrics in edge_metrics.items():\n",
    "        # Convert hex color to RGB tuple\n",
    "        hex_color = metrics['color'].lstrip('#')\n",
    "        rgb_color = tuple(int(hex_color[i:i+2], 16) / 255.0 for i in (0, 2, 4))\n",
    "        \n",
    "        # Scale width for matplotlib (needs to be smaller)\n",
    "        mpl_width = metrics['width'] * 0.8\n",
    "        \n",
    "        if metrics['is_mutual']:\n",
    "            mutual_edges.append((u, v))\n",
    "            mutual_colors_rgb.append(rgb_color)\n",
    "            mutual_widths_mpl.append(mpl_width)\n",
    "        else:\n",
    "            oneway_edges.append((u, v))\n",
    "            oneway_colors_rgb.append(rgb_color)\n",
    "            oneway_widths_mpl.append(mpl_width)\n",
    "\n",
    "    print(f\"Drawing {len(mutual_edges):,} mutual edges...\")\n",
    "    nx.draw_networkx_edges(\n",
    "        G, pos,\n",
    "        edgelist=mutual_edges,\n",
    "        width=mutual_widths_mpl,\n",
    "        alpha=alpha,\n",
    "        edge_color=mutual_colors_rgb,\n",
    "        style='-',\n",
    "        arrows=True,\n",
    "        arrowsize=static_config.get('edge_arrow_size', 8),\n",
    "        arrowstyle='<|-|>',\n",
    "        ax=ax,\n",
    "        node_size=0 # Prevent overlap issues\n",
    "    )\n",
    "    \n",
    "    print(f\"Drawing {len(oneway_edges):,} one-way edges...\")\n",
    "    nx.draw_networkx_edges(\n",
    "        G, pos,\n",
    "        edgelist=oneway_edges,\n",
    "        width=oneway_widths_mpl,\n",
    "        alpha=alpha,\n",
    "        edge_color=oneway_colors_rgb,\n",
    "        style='--',\n",
    "        arrows=True,\n",
    "        arrowsize=static_config.get('edge_arrow_size', 8),\n",
    "        arrowstyle='-|>',\n",
    "        ax=ax,\n",
    "        node_size=0 # Prevent overlap issues\n",
    "    )\n",
    "\n",
    "    # Draw nodes\n",
    "    print(\"Drawing nodes...\")\n",
    "    node_colors = [node_metrics[node]['color_rgba'] for node in G.nodes()]\n",
    "    # Scale node sizes for matplotlib (multiply by large factor)\n",
    "    node_sizes = [node_metrics[node]['size'] * 40 for node in G.nodes()]\n",
    "    \n",
    "    nx.draw_networkx_nodes(\n",
    "        G, pos,\n",
    "        node_color=node_colors,\n",
    "        node_size=node_sizes,\n",
    "        alpha=static_config.get('node_alpha', 0.8),\n",
    "        ax=ax\n",
    "    )\n",
    "\n",
    "    # Add labels if requested\n",
    "    if static_config['with_labels']:\n",
    "        nx.draw_networkx_labels(\n",
    "            G, pos,\n",
    "            font_size=static_config['font_size'],\n",
    "            font_color='black',\n",
    "            ax=ax\n",
    "        )\n",
    "\n",
    "    # Add legend\n",
    "    if static_config.get('show_legend', True):\n",
    "        \n",
    "        # Only show legend if community detection was run\n",
    "        communities_attr = nx.get_node_attributes(G, 'community')\n",
    "        if communities_attr:\n",
    "            num_communities = len(set(communities_attr.values()))\n",
    "            color_maps = get_community_colors(num_communities)\n",
    "            \n",
    "            legend_elements = [\n",
    "                mpatches.Patch(facecolor=color_maps['rgba'][i], \n",
    "                               edgecolor='black', \n",
    "                               label=f'Community {i}')\n",
    "                for i in range(min(num_communities, 10))  # Limit legend entries\n",
    "            ]\n",
    "        else:\n",
    "            # Add a single default legend element if analysis skipped\n",
    "            legend_elements = [\n",
    "                mpatches.Patch(facecolor='#808080', \n",
    "                               edgecolor='black', \n",
    "                               label='Default/Unanalyzed')\n",
    "            ]\n",
    "        \n",
    "        # Add edge type indicators\n",
    "        legend_elements.extend([\n",
    "            mpatches.Patch(facecolor='none', edgecolor='black', \n",
    "                           linestyle='-', label='Mutual Connection'),\n",
    "            mpatches.Patch(facecolor='none', edgecolor='black', \n",
    "                           linestyle='--', label='One-way Connection')\n",
    "        ])\n",
    "        \n",
    "        ax.legend(handles=legend_elements, loc='upper left', \n",
    "                  framealpha=0.9, fontsize=10)\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure\n",
    "    try:\n",
    "        plt.savefig(output_filename, dpi=static_config['dpi'], bbox_inches='tight')\n",
    "        print(f\"SUCCESS: Static PNG saved to: {output_filename}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Could not save static PNG: {e}\")\n",
    "        return False\n",
    "    finally:\n",
    "        plt.close()\n",
    "\n",
    "# =================================== METRICS REPORTING AND TEXT OUTPUT GENERATION ===================================\n",
    "# Comprehensive metrics reporting with analysis parameters and timing information\n",
    "\n",
    "def save_metrics_report(G: nx.DiGraph, filename: str, config: dict, timing_info: dict = None) -> bool:\n",
    "    \"\"\"\n",
    "    Saves a detailed text report with network statistics, analysis parameters, and timing information.\n",
    "    \n",
    "    Args:\n",
    "        G (nx.DiGraph): The analyzed graph\n",
    "        filename (str): Path to save the text report\n",
    "        config (dict): Configuration dictionary used for analysis\n",
    "        timing_info (dict): Optional timing information for different analysis steps\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            # Header\n",
    "            f.write(\"=\" * 80 + \"\\n\")\n",
    "            f.write(\"FOLLOWWEB NETWORK ANALYSIS REPORT\\n\")\n",
    "            f.write(\"=\" * 80 + \"\\n\")\n",
    "            f.write(f\"Generated: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            f.write(f\"Analysis Strategy: {config['pipeline']['strategy']}\\n\")\n",
    "            f.write(f\"Input File: {config['input_file']}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "            # Configuration Summary\n",
    "            f.write(\"CONFIGURATION SUMMARY\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            f.write(f\"Strategy: {config['pipeline']['strategy']}\\n\")\n",
    "            f.write(f\"Skip Analysis: {config['pipeline']['skip_analysis']}\\n\")\n",
    "            \n",
    "            if config['pipeline']['strategy'] == 'ego_alter_k-core':\n",
    "                f.write(f\"Ego Username: {config['pipeline']['ego_username']}\\n\")\n",
    "            \n",
    "            strategy = config['pipeline']['strategy']\n",
    "            k_value = config['pruning']['k_values'].get(strategy, config['pruning']['default_k_value'])\n",
    "            f.write(f\"K-value (pruning threshold): {k_value}\\n\")\n",
    "            \n",
    "            f.write(f\"Node Size Metric: {config['visualization']['node_size_metric']}\\n\")\n",
    "            f.write(f\"Scaling Algorithm: {config['visualization']['scaling_algorithm']}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "            # Fame Analysis Configuration\n",
    "            f.write(\"FAME ANALYSIS CONFIGURATION\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            f.write(f\"Find Paths to Famous: {config['fame_analysis']['find_paths_to_all_famous']}\\n\")\n",
    "            f.write(f\"Min Followers in Network: {config['fame_analysis']['min_followers_in_network']}\\n\")\n",
    "            f.write(f\"Min Fame Ratio: {config['fame_analysis']['min_fame_ratio']}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "            # Network Statistics\n",
    "            f.write(\"NETWORK STATISTICS\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            f.write(f\"Total Nodes: {G.number_of_nodes():,}\\n\")\n",
    "            f.write(f\"Total Edges: {G.number_of_edges():,}\\n\")\n",
    "            \n",
    "            if G.number_of_nodes() > 0:\n",
    "                # Basic network metrics\n",
    "                density = nx.density(G)\n",
    "                f.write(f\"Network Density: {density:.6f}\\n\")\n",
    "                \n",
    "                # Degree statistics\n",
    "                degrees = dict(G.degree())\n",
    "                if degrees:\n",
    "                    degree_values = list(degrees.values())\n",
    "                    f.write(f\"Average Degree: {sum(degree_values) / len(degree_values):.2f}\\n\")\n",
    "                    f.write(f\"Max Degree: {max(degree_values)}\\n\")\n",
    "                    f.write(f\"Min Degree: {min(degree_values)}\\n\")\n",
    "                \n",
    "                # Community information\n",
    "                communities_attr = nx.get_node_attributes(G, 'community')\n",
    "                if communities_attr:\n",
    "                    num_communities = len(set(communities_attr.values()))\n",
    "                    f.write(f\"Number of Communities: {num_communities}\\n\")\n",
    "                    \n",
    "                    # Community size distribution\n",
    "                    community_sizes = {}\n",
    "                    for node, comm in communities_attr.items():\n",
    "                        community_sizes[comm] = community_sizes.get(comm, 0) + 1\n",
    "                    \n",
    "                    if community_sizes:\n",
    "                        sizes = list(community_sizes.values())\n",
    "                        f.write(f\"Average Community Size: {sum(sizes) / len(sizes):.2f}\\n\")\n",
    "                        f.write(f\"Largest Community Size: {max(sizes)}\\n\")\n",
    "                        f.write(f\"Smallest Community Size: {min(sizes)}\\n\")\n",
    "                else:\n",
    "                    f.write(\"Community Detection: Skipped or Failed\\n\")\n",
    "                \n",
    "                # Centrality information\n",
    "                betweenness_attr = nx.get_node_attributes(G, 'betweenness')\n",
    "                eigenvector_attr = nx.get_node_attributes(G, 'eigenvector')\n",
    "                \n",
    "                if betweenness_attr:\n",
    "                    bet_values = list(betweenness_attr.values())\n",
    "                    f.write(f\"Average Betweenness Centrality: {sum(bet_values) / len(bet_values):.6f}\\n\")\n",
    "                    f.write(f\"Max Betweenness Centrality: {max(bet_values):.6f}\\n\")\n",
    "                \n",
    "                if eigenvector_attr:\n",
    "                    eig_values = list(eigenvector_attr.values())\n",
    "                    f.write(f\"Average Eigenvector Centrality: {sum(eig_values) / len(eig_values):.6f}\\n\")\n",
    "                    f.write(f\"Max Eigenvector Centrality: {max(eig_values):.6f}\\n\")\n",
    "                \n",
    "                f.write(\"\\n\")\n",
    "                \n",
    "                # Top nodes by different metrics\n",
    "                f.write(\"TOP NODES BY METRICS\\n\")\n",
    "                f.write(\"-\" * 40 + \"\\n\")\n",
    "                \n",
    "                # Top by degree\n",
    "                top_degree = sorted(degrees.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "                f.write(\"Top 10 Nodes by Degree:\\n\")\n",
    "                for i, (node, degree) in enumerate(top_degree, 1):\n",
    "                    f.write(f\"  {i:2d}. {node:<20} | Degree: {degree}\\n\")\n",
    "                f.write(\"\\n\")\n",
    "                \n",
    "                # Top by betweenness if available\n",
    "                if betweenness_attr:\n",
    "                    top_betweenness = sorted(betweenness_attr.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "                    f.write(\"Top 10 Nodes by Betweenness Centrality:\\n\")\n",
    "                    for i, (node, centrality) in enumerate(top_betweenness, 1):\n",
    "                        f.write(f\"  {i:2d}. {node:<20} | Betweenness: {centrality:.6f}\\n\")\n",
    "                    f.write(\"\\n\")\n",
    "                \n",
    "                # Top by eigenvector if available\n",
    "                if eigenvector_attr:\n",
    "                    top_eigenvector = sorted(eigenvector_attr.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "                    f.write(\"Top 10 Nodes by Eigenvector Centrality:\\n\")\n",
    "                    for i, (node, centrality) in enumerate(top_eigenvector, 1):\n",
    "                        f.write(f\"  {i:2d}. {node:<20} | Eigenvector: {centrality:.6f}\\n\")\n",
    "                    f.write(\"\\n\")\n",
    "            \n",
    "            # Timing Information\n",
    "            if timing_info:\n",
    "                f.write(\"PERFORMANCE TIMING\\n\")\n",
    "                f.write(\"-\" * 40 + \"\\n\")\n",
    "                for step_name, duration in timing_info.items():\n",
    "                    f.write(f\"{step_name}: {duration:.4f} seconds\\n\")\n",
    "                f.write(\"\\n\")\n",
    "            \n",
    "            # Analysis Parameters\n",
    "            f.write(\"DETAILED CONFIGURATION\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            f.write(\"Pipeline Configuration:\\n\")\n",
    "            for key, value in config['pipeline'].items():\n",
    "                f.write(f\"  {key}: {value}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "            f.write(\"Pruning Configuration:\\n\")\n",
    "            for key, value in config['pruning'].items():\n",
    "                f.write(f\"  {key}: {value}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "            f.write(\"Visualization Configuration:\\n\")\n",
    "            for key, value in config['visualization'].items():\n",
    "                if isinstance(value, dict):\n",
    "                    f.write(f\"  {key}:\\n\")\n",
    "                    for sub_key, sub_value in value.items():\n",
    "                        f.write(f\"    {sub_key}: {sub_value}\\n\")\n",
    "                else:\n",
    "                    f.write(f\"  {key}: {value}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "            # Footer\n",
    "            f.write(\"=\" * 80 + \"\\n\")\n",
    "            f.write(\"END OF REPORT\\n\")\n",
    "            f.write(\"=\" * 80 + \"\\n\")\n",
    "        \n",
    "        print(f\"SUCCESS: Metrics report saved to: {filename}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Could not save metrics report: {e}\")\n",
    "        return False\n",
    "\n",
    "def generate_text_output_filename(prefix: str, strategy: str, k_value: int) -> str:\n",
    "    \"\"\"\n",
    "    Generates a text report filename with the same base as visualizations.\n",
    "    \n",
    "    Args:\n",
    "        prefix (str): Base filename prefix\n",
    "        strategy (str): Analysis strategy name\n",
    "        k_value (int): K-value used for pruning\n",
    "        \n",
    "    Returns:\n",
    "        str: Text report filename\n",
    "    \"\"\"\n",
    "    return generate_output_filename(prefix, strategy, k_value, 'txt')\n",
    "\n",
    "\n",
    "# =================================== ENHANCED VISUALIZATION GENERATION ===================================\n",
    "# Generate both interactive HTML and static PNG visualizations with advanced styling\n",
    "\n",
    "def numEdges(node_id: str) -> int:\n",
    "    \"\"\"\n",
    "    Helper function to get the number of edges for a node (for legacy compatibility).\n",
    "    \n",
    "    Args:\n",
    "        node_id (str): Node identifier\n",
    "        \n",
    "    Returns:\n",
    "        int: Number of edges (degree) for the node\n",
    "    \"\"\"\n",
    "    if node_id in G:\n",
    "        return G.degree(node_id)\n",
    "    return 0\n",
    "\n",
    "# Calculate node and edge metrics for visualization\n",
    "print(\"=== CALCULATING VISUALIZATION METRICS ===\")\n",
    "viz_metrics_start = time.perf_counter()\n",
    "log_analysis_step(\"Visualization Metrics Started\", \"Calculating node and edge properties\")\n",
    "\n",
    "node_metrics = calculate_node_metrics(G, CONFIG['visualization'])\n",
    "edge_metrics = calculate_edge_metrics(G, CONFIG['visualization'])\n",
    "\n",
    "viz_metrics_end = time.perf_counter()\n",
    "timing_info['Visualization Metrics'] = viz_metrics_end - viz_metrics_start\n",
    "log_analysis_step(\"Visualization Metrics Complete\", \n",
    "                 f\"{len(node_metrics)} nodes, {len(edge_metrics)} edges\", \n",
    "                 timing_info['Visualization Metrics'])\n",
    "\n",
    "print(f\"Calculated metrics for {len(node_metrics)} nodes and {len(edge_metrics)} edges.\")\n",
    "print(\"=============================================\\n\")\n",
    "\n",
    "# =================================== GENERATE INTERACTIVE HTML VISUALIZATION ===================================\n",
    "# Create advanced interactive network visualization with legends and tooltips\n",
    "\n",
    "if G.number_of_nodes() > 0:\n",
    "    print(\"=== GENERATING INTERACTIVE HTML VISUALIZATION ===\")\n",
    "    html_gen_start = time.perf_counter()\n",
    "    \n",
    "    # Generate output filename\n",
    "    strategy = CONFIG['pipeline']['strategy']\n",
    "    k_value = CONFIG['pruning']['k_values'].get(strategy, CONFIG['pruning']['default_k_value'])\n",
    "    html_filename = generate_output_filename(\n",
    "        CONFIG['output_file_prefix'], \n",
    "        strategy, \n",
    "        k_value, \n",
    "        'html'\n",
    "    )\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    ensure_output_directory(html_filename)\n",
    "    log_analysis_step(\"HTML Generation Started\", f\"Output: {html_filename}\")\n",
    "    \n",
    "    # Generate interactive HTML visualization\n",
    "    success = visualize_network(G, html_filename, node_metrics, edge_metrics)\n",
    "    \n",
    "    html_gen_end = time.perf_counter()\n",
    "    timing_info['HTML Generation'] = html_gen_end - html_gen_start\n",
    "    \n",
    "    if success:\n",
    "        log_analysis_step(\"HTML Generation Complete\", \n",
    "                         f\"Interactive visualization saved\", \n",
    "                         timing_info['HTML Generation'])\n",
    "        print(f\"Interactive HTML visualization saved to: {html_filename}\")\n",
    "    else:\n",
    "        log_analysis_step(\"HTML Generation Failed\", \"Could not create interactive visualization\")\n",
    "        print(\"Failed to generate interactive HTML visualization\")\n",
    "        \n",
    "    print(\"================================================\\n\")\n",
    "else:\n",
    "    print(\"=== SKIPPING VISUALIZATION (empty graph) ===\")\n",
    "    log_analysis_step(\"HTML Generation Skipped\", \"Empty graph\")\n",
    "    timing_info['HTML Generation'] = 0.0\n",
    "    print(\"============================================\\n\")\n",
    "\n",
    "# =================================== GENERATE STATIC PNG VISUALIZATION ===================================\n",
    "# Create high-resolution static network visualization with multiple layout options\n",
    "\n",
    "if G.number_of_nodes() > 0 and CONFIG['visualization']['static_image']['generate']:\n",
    "    print(\"=== GENERATING STATIC PNG VISUALIZATION ===\")\n",
    "    png_gen_start = time.perf_counter()\n",
    "    \n",
    "    # Generate output filename\n",
    "    strategy = CONFIG['pipeline']['strategy']\n",
    "    k_value = CONFIG['pruning']['k_values'].get(strategy, CONFIG['pruning']['default_k_value'])\n",
    "    png_filename = generate_output_filename(\n",
    "        CONFIG['output_file_prefix'], \n",
    "        strategy, \n",
    "        k_value, \n",
    "        'png'\n",
    "    )\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    ensure_output_directory(png_filename)\n",
    "    log_analysis_step(\"PNG Generation Started\", f\"Output: {png_filename}\")\n",
    "    \n",
    "    # Generate static PNG visualization\n",
    "    success = generate_static_graph(G, png_filename, node_metrics, edge_metrics)\n",
    "    \n",
    "    png_gen_end = time.perf_counter()\n",
    "    timing_info['PNG Generation'] = png_gen_end - png_gen_start\n",
    "    \n",
    "    if success:\n",
    "        log_analysis_step(\"PNG Generation Complete\", \n",
    "                         f\"Static visualization saved\", \n",
    "                         timing_info['PNG Generation'])\n",
    "        print(f\"Static PNG visualization saved to: {png_filename}\")\n",
    "    else:\n",
    "        log_analysis_step(\"PNG Generation Failed\", \"Could not create static visualization\")\n",
    "        print(\"Failed to generate static PNG visualization\")\n",
    "        \n",
    "    print(\"===============================================\\n\")\n",
    "elif G.number_of_nodes() == 0:\n",
    "    print(\"=== SKIPPING STATIC PNG (empty graph) ===\")\n",
    "    log_analysis_step(\"PNG Generation Skipped\", \"Empty graph\")\n",
    "    timing_info['PNG Generation'] = 0.0\n",
    "    print(\"=========================================\\n\")\n",
    "else:\n",
    "    print(\"=== SKIPPING STATIC PNG (disabled in config) ===\")\n",
    "    log_analysis_step(\"PNG Generation Skipped\", \"Disabled in configuration\")\n",
    "    timing_info['PNG Generation'] = 0.0\n",
    "    print(\"================================================\\n\")\n",
    "\n",
    "# =================================== GENERATE METRICS REPORT ===================================\n",
    "# Create detailed text report with network statistics and analysis parameters\n",
    "\n",
    "if G.number_of_nodes() > 0:\n",
    "    print(\"=== GENERATING METRICS REPORT ===\")\n",
    "    metrics_report_start = time.perf_counter()\n",
    "    \n",
    "    # Generate output filename for metrics report\n",
    "    strategy = CONFIG['pipeline']['strategy']\n",
    "    k_value = CONFIG['pruning']['k_values'].get(strategy, CONFIG['pruning']['default_k_value'])\n",
    "    txt_filename = generate_text_output_filename(\n",
    "        CONFIG['output_file_prefix'], \n",
    "        strategy, \n",
    "        k_value\n",
    "    )\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    ensure_output_directory(txt_filename)\n",
    "    log_analysis_step(\"Metrics Report Started\", f\"Output: {txt_filename}\")\n",
    "    \n",
    "    # Generate metrics report with actual timing information\n",
    "    success = save_metrics_report(G, txt_filename, CONFIG, timing_info)\n",
    "    \n",
    "    metrics_report_end = time.perf_counter()\n",
    "    timing_info['Metrics Report'] = metrics_report_end - metrics_report_start\n",
    "    \n",
    "    if success:\n",
    "        log_analysis_step(\"Metrics Report Complete\", \n",
    "                         f\"Detailed report saved\", \n",
    "                         timing_info['Metrics Report'])\n",
    "        print(f\"Metrics report saved to: {txt_filename}\")\n",
    "    else:\n",
    "        log_analysis_step(\"Metrics Report Failed\", \"Could not create report\")\n",
    "        print(\"Failed to generate metrics report\")\n",
    "        \n",
    "    print(\"=====================================\\n\")\n",
    "else:\n",
    "    print(\"=== SKIPPING METRICS REPORT (empty graph) ===\")\n",
    "    log_analysis_step(\"Metrics Report Skipped\", \"Empty graph\")\n",
    "    timing_info['Metrics Report'] = 0.0\n",
    "    print(\"==============================================\\n\")\n",
    "\n",
    "# =================================== PIPELINE COMPLETION AND SUMMARY ===================================\n",
    "# Log comprehensive analysis summary and final timing information\n",
    "\n",
    "# Calculate total pipeline time\n",
    "pipeline_end_time = time.perf_counter()\n",
    "total_pipeline_time = pipeline_end_time - pipeline_start_time\n",
    "timing_info['Total Pipeline'] = total_pipeline_time\n",
    "\n",
    "# Log final summary\n",
    "log_analysis_step(\"Pipeline Complete\", f\"Total execution time: {total_pipeline_time:.4f}s\")\n",
    "log_analysis_summary(G, CONFIG, timing_info)\n",
    "\n",
    "# =================================== LEGACY COMPATIBILITY ===================================\n",
    "# Initialize Pyvis network for downstream compatibility\n",
    "net = Network(700, 700, directed=True, notebook=False)  # For jupyter notebook = True\n",
    "\n",
    "#======================================== NETWORKX TO PYVIS =============================================\n",
    "#Aesthetic Options\n",
    "sizeByConnections = 1 #Change a nodes size by number of connections \n",
    "\n",
    "# Add nodes and edges from the processed NetworkX graph to the Pyvis network\n",
    "net.from_nx(G)\n",
    "\n",
    "# Apply size scaling if enabled\n",
    "if sizeByConnections:\n",
    "    for node in net.nodes:\n",
    "        node['size'] = (numEdges(node['id'])/50)+9\n",
    "\n",
    "# Set physics options for the visualization\n",
    "net.force_atlas_2based(gravity=-50, central_gravity=0.01, spring_length=100, spring_strength=0.07, damping=0.8, overlap=1)\n",
    "net.show_buttons(filter_=['physics'])\n",
    "\n",
    "# Generate and show the HTML file\n",
    "net.save_graph(\"FollowWeb.html\")\n",
    "print(\"Legacy visualization complete. Check 'FollowWeb.html'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}