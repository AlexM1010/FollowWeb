{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import packages - Modernized imports with type hints and additional libraries\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import nx_parallel as nxp\n",
    "from networkx.algorithms import community\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "import hashlib\n",
    "import datetime\n",
    "from pyvis.network import Network\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "from collections import defaultdict\n",
    "from typing import Optional, Dict, List, Tuple\n",
    "\n",
    "# Enable parallel backend globally for NetworkX\n",
    "# Set this to the number of CPU cores you want to utilize.\n",
    "# A safe default is often (os.cpu_count() or 4)\n",
    "nx.config.backends.parallel.active = True\n",
    "nx.config.backends.parallel.n_jobs = (os.cpu_count() or 4) - 1\n",
    "\n",
    "# =================================== CENTRALIZED CONFIGURATION SYSTEM ===================================\n",
    "# Comprehensive CONFIG dictionary structure for different analysis aspects\n",
    "# This replaces all hardcoded parameters throughout the notebook\n",
    "\n",
    "CONFIG = {\n",
    "    # --- INPUT/OUTPUT CONFIGURATION ---\n",
    "    \"input_file\": \"followers_following.json\",  # Path to input data file (JSON format preferred)\n",
    "    \"output_file_prefix\": \"Output/FollowWeb\",   # Base path for all output files\n",
    "    \n",
    "    # --- PIPELINE CONFIGURATION ---\n",
    "    \"pipeline\": {\n",
    "        # Analysis strategy selection:\n",
    "        # 1. \"k-core\": (Default) Prunes the full L1+L2 graph. Good for general overview.\n",
    "        # 2. \"reciprocal_k-core\": Filters for mutuals (A follows B AND B follows A) then prunes. Good for finding \"real\" friend groups.\n",
    "        # 3. \"ego_alter_k-core\": Creates a graph of only your L1 contacts, connected if they follow each other. Good for analyzing your immediate circle.\n",
    "        \"strategy\": \"k-core\",    # Options: \"k-core\", \"reciprocal_k-core\", \"ego_alter_k-core\"\n",
    "        \n",
    "        # Skip computationally expensive structural analysis (community detection, centrality)\n",
    "        \"skip_analysis\": False,  # Set to True to skip ALL computationally expensive structural analysis\n",
    "        \n",
    "        # Required for \"ego_alter_k-core\" strategy - the central node (you)\n",
    "        \"ego_username\": \"_alexs.life\"  # Must be set if using \"ego_alter_k-core\"\n",
    "    },\n",
    "\n",
    "    # --- ANALYSIS CONFIGURATION ---\n",
    "    \"analysis\": {\n",
    "        # Specific username to find a path to (must be in your followers_following.json file)\n",
    "        # Set to \"\" or None to disable manual path finding\n",
    "        \"contact_path_target\": None,\n",
    "    },\n",
    "\n",
    "    # --- FAME ANALYSIS CONFIGURATION ---\n",
    "    \"fame_analysis\": {\n",
    "        # Find contact paths to every famous account identified\n",
    "        \"find_paths_to_all_famous\": True, \n",
    "        \n",
    "        # Minimum followers within your L1/L2 network for an account to be considered\n",
    "        \"min_followers_in_network\": 5, \n",
    "        \n",
    "        # Minimum ratio of (followers / following) to be considered famous\n",
    "        # (e.g., 5.0 means 5 followers for every 1 person they follow)\n",
    "        \"min_fame_ratio\": 5.0 \n",
    "    },\n",
    "\n",
    "    # --- PRUNING CONFIGURATION ---\n",
    "    \"pruning\": {\n",
    "        # Strategy-specific k-values (minimum connections required)\n",
    "        # Nodes with fewer connections than this will be removed\n",
    "        \"k_values\": {\n",
    "            \"k-core\": 1,              # Conservative pruning for full network\n",
    "            \"reciprocal_k-core\": 6,   # More aggressive pruning for mutual connections\n",
    "            \"ego_alter_k-core\": 3,    # Moderate pruning for ego network\n",
    "        },\n",
    "        \"default_k_value\": 2  # Fallback if strategy name is incorrect\n",
    "    },\n",
    "    \n",
    "    # --- VISUALIZATION CONFIGURATION ---\n",
    "    \"visualization\": {\n",
    "        # --- Shared Settings for Both HTML and PNG ---\n",
    "        \"node_size_metric\": \"degree\",        # Options: \"degree\", \"betweenness\", \"eigenvector\"\n",
    "        \"base_node_size\": 6,                 # Base size for nodes\n",
    "        \"node_size_multiplier\": 5,           # Multiplier for node size scaling\n",
    "        \"scaling_algorithm\": \"logarithmic\",  # Options: \"logarithmic\", \"linear\"\n",
    "        \n",
    "        \"base_edge_width\": 0.5,              # Base width for edges\n",
    "        \"edge_width_multiplier\": 2,          # Multiplier for edge width scaling\n",
    "        \"edge_width_scaling\": \"logarithmic\", # Options: \"logarithmic\", \"linear\"\n",
    "        \"intra_community_color\": \"#c0c0c0\",  # Gray color for within-community edges\n",
    "        \"bridge_color\": \"#6e6e6e\",           # Darker gray for between-community edges\n",
    "\n",
    "        # --- Interactive HTML Visualization (Pyvis) Configuration ---\n",
    "        \"pyvis_interactive\": { \n",
    "            \"width\": \"100%\",                 # Canvas width\n",
    "            \"height\": \"90vh\",                # Canvas height\n",
    "            \"notebook\": False,               # Set to True for Jupyter notebook display\n",
    "            \"show_labels\": True,             # Set to False to hide node names for faster rendering\n",
    "            \"show_tooltips\": True,           # Set to False to disable hover tooltips for faster loading\n",
    "            \"physics_solver\": \"forceAtlas2Based\",  # Physics simulation algorithm\n",
    "        },\n",
    "\n",
    "        # --- Static PNG Image Configuration ---\n",
    "        \"static_image\": {\n",
    "            \"generate\": True,                # Set to True to generate static PNG images\n",
    "            \"layout\": \"spring\",             # Options: \"spring\", \"kamada_kawai\", \"circular\", \"shell\"\n",
    "            \"with_labels\": False,            # Labels are often too cluttered on static graphs\n",
    "            \"font_size\": 8,                 # Font size for labels (if enabled)\n",
    "            \"image_size_inches\": (25, 25),   # Image dimensions (width, height) in inches\n",
    "            \"dpi\": 300,                     # Dots Per Inch - higher for better quality\n",
    "            \"spring_k\": 0.3,               # Spring layout parameter (adjusts node repulsion)\n",
    "            \"spring_iterations\": 50,        # Number of iterations for spring layout\n",
    "            \"edge_alpha\": 0.3,              # Edge transparency (0.0 to 1.0)\n",
    "            \"node_alpha\": 0.8,              # Node transparency (0.0 to 1.0)\n",
    "            \"edge_arrow_size\": 8,            # Size of arrow heads on edges\n",
    "            \"show_legend\": True              # Include legend in static images\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# =================================== CONFIGURATION VALIDATION ===================================\n",
    "# Comprehensive configuration validation with clear error messages\n",
    "# This function validates all configuration parameters before execution\n",
    "\n",
    "def validate_config(config: dict) -> bool:\n",
    "    \"\"\"\n",
    "    Validates CONFIG before running pipeline.\n",
    "    Raises exceptions for critical errors, prints warnings for minor issues.\n",
    "    \n",
    "    Args:\n",
    "        config (dict): The configuration dictionary to validate\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if validation passes\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If input file doesn't exist\n",
    "        ValueError: If configuration parameters are invalid\n",
    "    \"\"\"\n",
    "    print(\"=== VALIDATING CONFIGURATION ===\")\n",
    "    \n",
    "    # --- INPUT FILE VALIDATION ---\n",
    "    if not os.path.exists(config['input_file']):\n",
    "        raise FileNotFoundError(f\"Input file not found: {config['input_file']}. Please ensure the file exists.\")\n",
    "    print(f\"\u00e2\u0153\u201c Input file exists: {config['input_file']}\")\n",
    "    \n",
    "    # --- STRATEGY VALIDATION ---\n",
    "    strategy = config['pipeline']['strategy']\n",
    "    valid_strategies = ['k-core', 'reciprocal_k-core', 'ego_alter_k-core']\n",
    "    if strategy not in valid_strategies:\n",
    "        raise ValueError(f\"Invalid strategy '{strategy}'. Must be one of: {valid_strategies}\")\n",
    "    print(f\"\u00e2\u0153\u201c Strategy is valid: {strategy}\")\n",
    "    \n",
    "    # --- EGO USERNAME VALIDATION (for ego_alter_k-core strategy) ---\n",
    "    if strategy == \"ego_alter_k-core\":\n",
    "        ego = config['pipeline']['ego_username']\n",
    "        if not ego or ego == \"_alexs.life\":  # Check for placeholder value\n",
    "            raise ValueError(\"'ego_username' must be set in CONFIG for 'ego_alter_k-core' strategy. Please replace the placeholder value.\")\n",
    "        print(f\"\u00e2\u0153\u201c Ego username set for ego_alter_k-core: {ego}\")\n",
    "    \n",
    "    # --- K-VALUES VALIDATION ---\n",
    "    for strat, k_val in config['pruning']['k_values'].items():\n",
    "        if not isinstance(k_val, int) or k_val < 0:\n",
    "            raise ValueError(f\"k-value for '{strat}' must be a non-negative integer: {k_val}\")\n",
    "    \n",
    "    default_k = config['pruning']['default_k_value']\n",
    "    if not isinstance(default_k, int) or default_k < 0:\n",
    "        raise ValueError(f\"default_k_value must be a non-negative integer: {default_k}\")\n",
    "    print(f\"\u00e2\u0153\u201c K-values are valid: {config['pruning']['k_values']}\")\n",
    "    \n",
    "    # --- FAME ANALYSIS VALIDATION ---\n",
    "    min_followers = config['fame_analysis']['min_followers_in_network']\n",
    "    if not isinstance(min_followers, int) or min_followers < 0:\n",
    "        raise ValueError(f\"min_followers_in_network must be a non-negative integer: {min_followers}\")\n",
    "    \n",
    "    min_ratio = config['fame_analysis']['min_fame_ratio']\n",
    "    if not isinstance(min_ratio, (int, float)) or min_ratio <= 0:\n",
    "        raise ValueError(f\"min_fame_ratio must be a positive number: {min_ratio}\")\n",
    "    print(f\"\u00e2\u0153\u201c Fame analysis parameters are valid\")\n",
    "    \n",
    "    # --- VISUALIZATION CONFIGURATION VALIDATION ---\n",
    "    vis_config = config['visualization']\n",
    "    \n",
    "    # Validate node size metric\n",
    "    valid_metrics = ['degree', 'betweenness', 'eigenvector']\n",
    "    if vis_config['node_size_metric'] not in valid_metrics:\n",
    "        raise ValueError(f\"Invalid 'node_size_metric': {vis_config['node_size_metric']}. Must be one of: {valid_metrics}\")\n",
    "    \n",
    "    # Validate scaling algorithms\n",
    "    valid_scaling = ['logarithmic', 'linear']\n",
    "    if vis_config['scaling_algorithm'] not in valid_scaling:\n",
    "        raise ValueError(f\"Invalid 'scaling_algorithm': {vis_config['scaling_algorithm']}. Must be one of: {valid_scaling}\")\n",
    "    \n",
    "    if vis_config['edge_width_scaling'] not in valid_scaling:\n",
    "        raise ValueError(f\"Invalid 'edge_width_scaling': {vis_config['edge_width_scaling']}. Must be one of: {valid_scaling}\")\n",
    "    \n",
    "    # Validate pyvis interactive configuration\n",
    "    pyvis_config = vis_config.get('pyvis_interactive', {})\n",
    "    required_pyvis_keys = ['width', 'height', 'physics_solver']\n",
    "    missing_keys = [key for key in required_pyvis_keys if key not in pyvis_config]\n",
    "    if missing_keys:\n",
    "        raise ValueError(f\"Missing required keys in 'visualization.pyvis_interactive': {missing_keys}\")\n",
    "    \n",
    "    # Validate static image configuration\n",
    "    static_config = vis_config.get('static_image', {})\n",
    "    if static_config.get('generate', False):\n",
    "        valid_layouts = ['spring', 'kamada_kawai', 'circular', 'shell']\n",
    "        layout = static_config.get('layout', 'spring')\n",
    "        if layout not in valid_layouts:\n",
    "            raise ValueError(f\"Invalid static image layout '{layout}'. Must be one of: {valid_layouts}\")\n",
    "        \n",
    "        # Validate image dimensions\n",
    "        image_size = static_config.get('image_size_inches', (25, 25))\n",
    "        if not isinstance(image_size, (tuple, list)) or len(image_size) != 2:\n",
    "            raise ValueError(\"image_size_inches must be a tuple/list of (width, height)\")\n",
    "        \n",
    "        if any(not isinstance(dim, (int, float)) or dim <= 0 for dim in image_size):\n",
    "            raise ValueError(\"image_size_inches dimensions must be positive numbers\")\n",
    "    \n",
    "    print(f\"\u00e2\u0153\u201c Visualization configuration is valid\")\n",
    "    \n",
    "    # --- OUTPUT DIRECTORY VALIDATION ---\n",
    "    output_dir = os.path.dirname(config['output_file_prefix'])\n",
    "    if output_dir and not os.path.exists(output_dir):\n",
    "        try:\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            print(f\"\u00e2\u0153\u201c Created output directory: {output_dir}\")\n",
    "        except OSError as e:\n",
    "            raise ValueError(f\"Cannot create output directory '{output_dir}': {e}\")\n",
    "    \n",
    "    print(\"SUCCESS: Configuration validated successfully\")\n",
    "    print(\"=================================\\n\")\n",
    "    return True\n",
    "\n",
    "# Validate the configuration before proceeding\n",
    "validate_config(CONFIG)",
    "\n",
    "# Print configuration summary\n",
    "print(\"=== CONFIGURATION LOADED ===\")\n",
    "print(f\"Strategy: {CONFIG['pipeline']['strategy']}\")\n",
    "print(f\"Input file: {CONFIG['input_file']}\")\n",
    "print(f\"Output prefix: {CONFIG['output_file_prefix']}\")\n",
    "print(f\"Skip analysis: {CONFIG['pipeline']['skip_analysis']}\")\n",
    "print(f\"K-value for {CONFIG['pipeline']['strategy']}: {CONFIG['pruning']['k_values'].get(CONFIG['pipeline']['strategy'], CONFIG['pruning']['default_k_value'])}\")\n",
    "print(\"============================\\n\")\n",
    "\n",
    "# =================================== GRAPH LOADING AND STRATEGY FILTERING ===================================\n",
    "# Advanced network analysis capabilities with strategy-specific filtering\n",
    "\n",
    "def load_graph_from_json(filepath: str) -> nx.DiGraph:\n",
    "    \"\"\"\n",
    "    Loads a directed graph from a JSON file with proper error handling.\n",
    "    \n",
    "    Expected format: A list of user objects\n",
    "    [\n",
    "     {\n",
    "       \"user\": \"username1\",\n",
    "       \"followers\": [\"user2\", \"user3\"],\n",
    "       \"following\": [\"user4\", \"user5\"]\n",
    "     },\n",
    "     ...\n",
    "    ]\n",
    "    \n",
    "    Args:\n",
    "        filepath (str): Path to the JSON data file\n",
    "        \n",
    "    Returns:\n",
    "        nx.DiGraph: Loaded graph or empty graph on error\n",
    "    \"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    try:\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"ERROR: Invalid JSON format in {filepath}\")\n",
    "        print(f\"       JSON error: {e}\")\n",
    "        return G\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Could not read file {filepath}: {e}\")\n",
    "        return G\n",
    "    \n",
    "    if not isinstance(data, list):\n",
    "        print(f\"ERROR: JSON root must be a LIST, got {type(data)}\")\n",
    "        return G\n",
    "    \n",
    "    print(f\"Processing {len(data)} user entries...\")\n",
    "    \n",
    "    # Process each user's data from the list\n",
    "    for user_entry in data:\n",
    "        if not isinstance(user_entry, dict):\n",
    "            print(f\"WARNING: Skipping item in list - data is not a dict\")\n",
    "            continue\n",
    "        \n",
    "        # Get username from the 'user' key\n",
    "        username = user_entry.get('user')\n",
    "        if not username:\n",
    "            print(\"WARNING: Skipping item in list - 'user' key is missing or empty.\")\n",
    "            continue\n",
    "        \n",
    "        # Validate required keys\n",
    "        if 'followers' not in user_entry or 'following' not in user_entry:\n",
    "            print(f\"WARNING: User '{username}' missing 'followers' or 'following' key\")\n",
    "            continue\n",
    "        \n",
    "        followers = user_entry.get('followers', [])\n",
    "        following = user_entry.get('following', [])\n",
    "        \n",
    "        if not isinstance(followers, list):\n",
    "            print(f\"WARNING: User '{username}' - 'followers' is not a list\")\n",
    "            followers = []\n",
    "        if not isinstance(following, list):\n",
    "            print(f\"WARNING: User '{username}' - 'following' is not a list\")\n",
    "            following = []\n",
    "        \n",
    "        # Add edges\n",
    "        for follower in followers:\n",
    "            if follower:  # Skip empty strings\n",
    "                G.add_edge(follower, username)  # Follower -> User\n",
    "        \n",
    "        for followee in following:\n",
    "            if followee:  # Skip empty strings\n",
    "                G.add_edge(username, followee)  # User -> Followee\n",
    "    \n",
    "    print(f\"Initial graph loaded: {G.number_of_nodes():,} nodes, {G.number_of_edges():,} edges.\")\n",
    "    return G\n",
    "\n",
    "def filter_by_reciprocity(G: nx.DiGraph) -> nx.DiGraph:\n",
    "    \"\"\"\n",
    "    Creates a new graph containing only reciprocal edges (mutual followers).\n",
    "    \n",
    "    Args:\n",
    "        G (nx.DiGraph): Input graph\n",
    "        \n",
    "    Returns:\n",
    "        nx.DiGraph: New graph with only mutual connections, or empty graph if none exist\n",
    "    \"\"\"\n",
    "    G_reciprocal = nx.DiGraph()\n",
    "    \n",
    "    # Keep only edges where the reverse edge also exists\n",
    "    reciprocal_edges = [edge for edge in G.edges() if G.has_edge(edge[1], edge[0])]\n",
    "    G_reciprocal.add_edges_from(reciprocal_edges)\n",
    "\n",
    "    # Remove nodes that now have 0 degree\n",
    "    G_reciprocal.remove_nodes_from(list(nx.isolates(G_reciprocal)))\n",
    "\n",
    "    print(f\"Filtered for mutuals: {G_reciprocal.number_of_nodes():,} nodes, {G_reciprocal.number_of_edges():,} edges.\")\n",
    "    return G_reciprocal\n",
    "\n",
    "def create_ego_alter_graph(G: nx.DiGraph, ego_username: str) -> nx.DiGraph:\n",
    "    \"\"\"\n",
    "    Creates an \"alter graph\" showing connections between the ego's L1 contacts.\n",
    "    \n",
    "    Args:\n",
    "        G (nx.DiGraph): Input graph\n",
    "        ego_username (str): The central node (ego)\n",
    "        \n",
    "    Returns:\n",
    "        nx.DiGraph: Graph of L1 contacts and their connections, or empty graph on error\n",
    "    \"\"\"\n",
    "    if ego_username not in G:\n",
    "        print(f\"ERROR: Ego node '{ego_username}' not in graph. Check CONFIG.\")\n",
    "        return nx.DiGraph()\n",
    "\n",
    "    # 1. Identify Alters (L1 nodes)\n",
    "    try:\n",
    "        followers = set(G.predecessors(ego_username))\n",
    "    except nx.NetworkXError:\n",
    "        followers = set()\n",
    "        \n",
    "    try:\n",
    "        following = set(G.successors(ego_username))\n",
    "    except nx.NetworkXError:\n",
    "        following = set()\n",
    "        \n",
    "    alters = followers.union(following)\n",
    "\n",
    "    if not alters:\n",
    "        print(\"WARNING: No alters (L1 connections) found for this ego.\")\n",
    "        return nx.DiGraph()\n",
    "\n",
    "    # 2. Create a new graph containing only connections between alters\n",
    "    alter_graph = G.subgraph(alters).copy()\n",
    "    \n",
    "    # Remove isolates (alters who don't connect to any other alters)\n",
    "    alter_graph.remove_nodes_from(list(nx.isolates(alter_graph)))\n",
    "\n",
    "    print(f\"Alter graph created: {alter_graph.number_of_nodes():,} alters, {alter_graph.number_of_edges():,} connections between them.\")\n",
    "    return alter_graph\n",
    "\n",
    "def prune_graph(G: nx.DiGraph, min_degree: int) -> nx.DiGraph:\n",
    "    \"\"\"\n",
    "    Uses nx.k_core function to find the maximal subgraph \n",
    "    where all nodes have degree >= min_degree.\n",
    "    \n",
    "    Args:\n",
    "        G (nx.DiGraph): Input graph\n",
    "        min_degree (int): Minimum degree threshold (k-value)\n",
    "        \n",
    "    Returns:\n",
    "        nx.DiGraph: Pruned graph (k-core subgraph)\n",
    "    \"\"\"\n",
    "    if min_degree <= 0:\n",
    "        print(\"Pruning skipped (min_degree <= 0).\")\n",
    "        return G\n",
    "\n",
    "    # nx.k_core finds the subgraph where all nodes have at least k-degree\n",
    "    # For DiGraphs, .degree() is in+out, which matches your original logic.\n",
    "    G_pruned = nx.k_core(G, k=min_degree)\n",
    "\n",
    "    nodes_removed = G.number_of_nodes() - G_pruned.number_of_nodes()\n",
    "    \n",
    "    print(f\"Pruning complete. Removed {nodes_removed:,} nodes.\")\n",
    "    print(f\"Final pruned graph: {G_pruned.number_of_nodes():,} nodes, {G_pruned.number_of_edges():,} edges.\")\n",
    "    return G_pruned\n",
    "\n",
    "def apply_strategy_filtering(G: nx.DiGraph, strategy: str, ego_username: Optional[str] = None) -> nx.DiGraph:\n",
    "    \"\"\"\n",
    "    Applies strategy-specific graph filtering for different analysis approaches.\n",
    "    \n",
    "    Args:\n",
    "        G (nx.DiGraph): Input graph\n",
    "        strategy (str): Analysis strategy (\"k-core\", \"reciprocal_k-core\", \"ego_alter_k-core\")\n",
    "        ego_username (Optional[str]): Required for \"ego_alter_k-core\" strategy\n",
    "        \n",
    "    Returns:\n",
    "        nx.DiGraph: Filtered graph based on strategy\n",
    "    \"\"\"\n",
    "    print(f\"Applying '{strategy}' strategy filtering...\")\n",
    "    \n",
    "    if strategy == \"k-core\":\n",
    "        # Use the full graph as-is\n",
    "        return G\n",
    "    elif strategy == \"reciprocal_k-core\":\n",
    "        # Filter for mutual connections only\n",
    "        return filter_by_reciprocity(G)\n",
    "    elif strategy == \"ego_alter_k-core\":\n",
    "        # Create ego-alter network\n",
    "        if not ego_username:\n",
    "            print(\"ERROR: ego_username required for 'ego_alter_k-core' strategy\")\n",
    "            return nx.DiGraph()\n",
    "        return create_ego_alter_graph(G, ego_username)\n",
    "    else:\n",
    "        print(f\"WARNING: Unknown strategy '{strategy}'. Using full graph.\")\n",
    "        return G\n",
    "\n",
    "# =================================== LOAD AND PROCESS GRAPH ===================================\n",
    "# Load graph from JSON data file with strategy-specific filtering\n",
    "\n",
    "print(\"=== LOADING GRAPH FROM DATA FILE ===\")\n",
    "DATA_FILE = CONFIG['input_file']\n",
    "\n",
    "# Load the initial graph from JSON\n",
    "G = load_graph_from_json(DATA_FILE)\n",
    "\n",
    "if G.number_of_nodes() == 0:\n",
    "    print(\"ERROR: No graph data loaded. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "# Apply strategy-specific filtering\n",
    "strategy = CONFIG['pipeline']['strategy']\n",
    "ego_username = CONFIG['pipeline'].get('ego_username')\n",
    "\n",
    "G_filtered = apply_strategy_filtering(G, strategy, ego_username)\n",
    "\n",
    "if G_filtered.number_of_nodes() == 0:\n",
    "    print(\"WARNING: Strategy filtering resulted in empty graph.\")\n",
    "    G_filtered = G  # Fall back to original graph\n",
    "\n",
    "# Apply k-core pruning based on strategy\n",
    "k_value = CONFIG['pruning']['k_values'].get(strategy, CONFIG['pruning']['default_k_value'])\n",
    "print(f\"Applying k-core pruning with k={k_value}...\")\n",
    "G_final = prune_graph(G_filtered, k_value)\n",
    "\n",
    "print(f\"Final processed graph: {G_final.number_of_nodes():,} nodes, {G_final.number_of_edges():,} edges.\")\n",
    "print(\"=====================================\\n\")\n",
    "\n",
    "# Update the main graph variable for downstream processing\n",
    "G = G_final\n",
    "\n",
    "# =================================== PROGRESS TRACKER CLASS ===================================\n",
    "# Comprehensive progress tracking for long-running operations\n",
    "\n",
    "class ProgressTracker:\n",
    "    \"\"\"\n",
    "    A utility class to print progress updates for long-running loops.\n",
    "    \n",
    "    Usage:\n",
    "        tracker = ProgressTracker(total=1000, title=\"Processing items\")\n",
    "        for i in range(1000):\n",
    "            tracker.update(i + 1)\n",
    "        tracker.complete()\n",
    "        \n",
    "    For chunked tasks, set num_updates to number of chunks:\n",
    "        tracker = ProgressTracker(total=50, title=\"Processing\", num_updates=50)\n",
    "        for i in range(0, 5000, 100):  # 50 chunks of 100 items\n",
    "            # ... do work ...\n",
    "            tracker.update((i // 100) + 1)  # Update with chunk number\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def _format_time(seconds: float) -> str:\n",
    "        \"\"\"Convert seconds to human-readable format.\"\"\"\n",
    "        if seconds < 60.0:\n",
    "            return f\"{seconds:.1f}s\"\n",
    "        minutes = int(seconds // 60)\n",
    "        remaining_sec = int(seconds % 60)\n",
    "        if minutes < 60:\n",
    "            return f\"{minutes}m {remaining_sec}s\"\n",
    "        hours = minutes // 60\n",
    "        remaining_min = minutes % 60\n",
    "        return f\"{hours}h {remaining_min}m\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        total: int,\n",
    "        title: str,\n",
    "        num_updates: int = 10,\n",
    "        threshold_sec: float = 3.0,\n",
    "        show_percentage: bool = True,\n",
    "        show_count: bool = True,\n",
    "        bar_width: int = 30\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the progress tracker.\n",
    "        \n",
    "        Args:\n",
    "            total: Total number of items/chunks to process\n",
    "            title: Title to display\n",
    "            num_updates: Target number of progress updates (usually = num chunks)\n",
    "            threshold_sec: Only show progress if total task estimated to exceed this (seconds)\n",
    "            show_percentage: Include percentage in output\n",
    "            show_count: Include item count in output\n",
    "            bar_width: Width of progress bar in characters (0 to disable)\n",
    "        \"\"\"\n",
    "        self.total = max(1, total)\n",
    "        self.title = title\n",
    "        self.num_updates = max(1, num_updates)\n",
    "        self.threshold_sec = max(0.1, threshold_sec)\n",
    "        self.show_percentage = show_percentage\n",
    "        self.show_count = show_count\n",
    "        self.bar_width = max(0, bar_width)\n",
    "        self.show_progress = False\n",
    "\n",
    "        self.title_printed = False \n",
    "        self.decision_made = False \n",
    "        self.update_every_n = max(1, self.total // self.num_updates)\n",
    "        \n",
    "        self.start_time = time.perf_counter()\n",
    "        self.last_printed_item = -1\n",
    "        \n",
    "        # For time estimation\n",
    "        self.first_chunk_time = None  # Time taken for first chunk\n",
    "        self.ema_rate = None  # Exponential moving average of processing rate\n",
    "        self.ema_alpha = 0.3  # Smoothing factor\n",
    "\n",
    "        # Print the title and an initial \"0%\" line immediately.\n",
    "        print(f\"{self.title}...\")\n",
    "        self.title_printed = True\n",
    "        \n",
    "        progress_str = self._format_progress_line(\n",
    "            current_item=0,\n",
    "            percent=0.0,\n",
    "            elapsed=0.0,\n",
    "            remaining_time=0.0  # 0.0 remaining_time triggers our fallback\n",
    "        )\n",
    "        print(progress_str, end=\"\\r\", flush=True)\n",
    "        self.last_printed_item = 0 # Mark item 0 as \"printed\"\n",
    "    \n",
    "    def __enter__(self):\n",
    "        \"\"\"Context manager entry.\"\"\"\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        \"\"\"Context manager exit.\"\"\"\n",
    "        self.complete()\n",
    "        return False\n",
    "    \n",
    "    def _make_decision(self, current_item: int) -> None:\n",
    "        \"\"\"\n",
    "        After first chunk, estimate total time and decide whether to show progress.\n",
    "        \"\"\"\n",
    "        if self.decision_made or current_item == 0:\n",
    "            return\n",
    "        \n",
    "        elapsed = time.perf_counter() - self.start_time\n",
    "        self.first_chunk_time = elapsed\n",
    "        \n",
    "        # Estimate total time based on first chunk\n",
    "        if current_item > 0:\n",
    "            chunks_remaining = self.total - current_item\n",
    "            estimated_total_time = elapsed + (chunks_remaining * (elapsed / current_item))\n",
    "        else:\n",
    "            estimated_total_time = 0\n",
    "        \n",
    "        # Decide: show progress only if estimated total exceeds threshold\n",
    "        self.show_progress = estimated_total_time > self.threshold_sec\n",
    "        self.decision_made = True\n",
    "        \n",
    "        if self.show_progress and not self.title_printed:\n",
    "            print(f\"{self.title}...\")\n",
    "            self.title_printed = True\n",
    "    \n",
    "    def _estimate_remaining_time(self, current_item: int) -> float:\n",
    "        \"\"\"\n",
    "        Estimate remaining time using exponential moving average of rate.\n",
    "        \"\"\"\n",
    "        if current_item <= 0:\n",
    "            return 0.0\n",
    "        \n",
    "        elapsed = time.perf_counter() - self.start_time\n",
    "        \n",
    "        if elapsed <= 0:\n",
    "            return 0.0\n",
    "        \n",
    "        # Calculate rate for this item\n",
    "        current_rate = current_item / elapsed\n",
    "        \n",
    "        # Update EMA\n",
    "        if self.ema_rate is None:\n",
    "            self.ema_rate = current_rate\n",
    "        else:\n",
    "            self.ema_rate = (self.ema_alpha * current_rate + \n",
    "                            (1 - self.ema_alpha) * self.ema_rate)\n",
    "        \n",
    "        # Estimate remaining time\n",
    "        remaining_items = self.total - current_item\n",
    "        if self.ema_rate > 0:\n",
    "            return remaining_items / self.ema_rate\n",
    "        return 0.0\n",
    "    \n",
    "    def _create_progress_bar(self, percent: float) -> str:\n",
    "        \"\"\"\n",
    "        Generate ASCII progress bar.\n",
    "        \"\"\"\n",
    "        if self.bar_width <= 0:\n",
    "            return \"\"\n",
    "        \n",
    "        # Calculate how many full '█' characters to show\n",
    "        filled_len = int(self.bar_width * percent / 100.0)\n",
    "        \n",
    "        # Create the bar string\n",
    "        bar_fill = '█' * filled_len\n",
    "        \n",
    "        # Pad the rest with spaces to match the total bar_width\n",
    "        bar = bar_fill.ljust(self.bar_width, ' ')\n",
    "        \n",
    "        return f\" [{bar}]\"\n",
    "    \n",
    "    def _format_progress_line(\n",
    "        self,\n",
    "        current_item: int,\n",
    "        percent: float,\n",
    "        elapsed: float,\n",
    "        remaining_time: float\n",
    "    ) -> str:\n",
    "        \"\"\"Format the progress line with all requested information.\"\"\"\n",
    "        parts = []\n",
    "        \n",
    "        # Count information\n",
    "        if self.show_count:\n",
    "            parts.append(f\"{current_item}/{self.total}\")\n",
    "        \n",
    "        # Percentage\n",
    "        if self.show_percentage:\n",
    "            parts.append(f\"({percent:.1f}%)\")\n",
    "        \n",
    "        # Progress bar\n",
    "        bar = self._create_progress_bar(percent)\n",
    "        if bar:\n",
    "            parts.append(bar)\n",
    "        \n",
    "        # Time information\n",
    "        if percent < 100.0 and percent > 1.0 and current_item < self.total:\n",
    "            if remaining_time > 0:\n",
    "                remaining_str = self._format_time(remaining_time)\n",
    "                parts.append(f\"- Est. {remaining_str} remaining\")\n",
    "            else:\n",
    "                parts.append(\"- Processing...\") \n",
    "        elif current_item >= self.total:\n",
    "            elapsed_str = self._format_time(elapsed)\n",
    "            parts.append(f\"- Completed in {elapsed_str}\")\n",
    "        \n",
    "        return \"    Progress: \" + \" \".join(parts)\n",
    "    \n",
    "    def update(self, current_item: int) -> None:\n",
    "        \"\"\"\n",
    "        Call this inside the loop with the current item count (1-based).\n",
    "        \n",
    "        Args:\n",
    "            current_item: Current iteration number (1-based) or chunk number\n",
    "        \"\"\"\n",
    "        is_last_item = (current_item >= self.total)\n",
    "        is_update_step = (current_item % self.update_every_n == 0)\n",
    "\n",
    "        # Decide whether to show progress\n",
    "        if not self.decision_made:\n",
    "            if (is_update_step and current_item > 0) or is_last_item:\n",
    "                self._make_decision(current_item)\n",
    "        \n",
    "        if not self.show_progress:\n",
    "            return  # Hides all output for short tasks\n",
    "\n",
    "        if (is_update_step or is_last_item) and current_item > self.last_printed_item:\n",
    "            \n",
    "            self.last_printed_item = current_item\n",
    "            current_item = min(current_item, self.total)\n",
    "            \n",
    "            elapsed = time.perf_counter() - self.start_time\n",
    "            percent = (current_item / self.total) * 100.0\n",
    "            remaining_time = self._estimate_remaining_time(current_item)\n",
    "            \n",
    "            # Format and print progress\n",
    "            progress_str = self._format_progress_line(\n",
    "                current_item,\n",
    "                percent,\n",
    "                elapsed,\n",
    "                remaining_time\n",
    "            )\n",
    "            print(progress_str, flush=True)\n",
    "    \n",
    "    def complete(self) -> None:\n",
    "        \"\"\"Call after the loop finishes. Prints completion info.\"\"\"\n",
    "        \n",
    "        if self.show_progress:\n",
    "            # Print final 100% line\n",
    "            self.update(self.total)\n",
    "    \n",
    "    def reset(self, total: Optional[int] = None) -> None:\n",
    "        \"\"\"Reset the tracker to start a new task (useful for reuse).\"\"\"\n",
    "        if total is not None:\n",
    "            self.total = max(1, total)\n",
    "        \n",
    "        self.start_time = time.perf_counter()\n",
    "        self.last_printed_item = -1 \n",
    "        self.ema_rate = None\n",
    "        self.show_progress = False\n",
    "        self.title_printed = False\n",
    "        self.decision_made = False\n",
    "        self.first_chunk_time = None\n",
    "\n",
    "# =================================== NETWORK ANALYSIS AND COMMUNITY DETECTION ===================================\n",
    "# Advanced network analysis with community detection and centrality calculations\n",
    "\n",
    "def analyze_network(G: nx.DiGraph) -> nx.DiGraph:\n",
    "    \"\"\"\n",
    "    Calculates network metrics and attaches them as node attributes.\n",
    "    Performs community detection and centrality calculations.\n",
    "    \n",
    "    Args:\n",
    "        G (nx.DiGraph): Input graph\n",
    "        \n",
    "    Returns:\n",
    "        nx.DiGraph: Graph with analysis results stored as node attributes\n",
    "    \"\"\"\n",
    "    MIN_NODES_FOR_ANALYSIS = 2\n",
    "    \n",
    "    if G.number_of_nodes() < MIN_NODES_FOR_ANALYSIS:\n",
    "        print(f\"WARNING: Graph has fewer than {MIN_NODES_FOR_ANALYSIS} nodes. Skipping analysis.\")\n",
    "        return G\n",
    "    \n",
    "    # This task has 3 main steps: 1. Louvain, 2. Betweenness, 3. Eigenvector\n",
    "    # We set total=3 and update after each step.\n",
    "    # We also give it a short threshold_sec (e.g., 1.0s) so it appears\n",
    "    # even if the first step (Louvain) is fast.\n",
    "    tracker_title = \"Analyzing structure (Communities, Centrality)\"\n",
    "    tracker = ProgressTracker(\n",
    "        total=3, \n",
    "        title=tracker_title, \n",
    "        num_updates=3, \n",
    "        threshold_sec=1.0, \n",
    "        bar_width=30\n",
    "    )\n",
    "    \n",
    "    # --- 1. Community Detection (Louvain) ---\n",
    "    G_undirected = G.to_undirected()\n",
    "    \n",
    "    if G_undirected.number_of_edges() == 0:\n",
    "        print(\"WARNING: Graph has no edges. Skipping Louvain community detection.\")\n",
    "        communities = []\n",
    "    else:\n",
    "        communities = community.louvain_communities(G_undirected, seed=123)\n",
    "    \n",
    "    print(f\"Detected {len(communities)} communities (Louvain).\")\n",
    "    partition = {node: i for i, comm in enumerate(communities) for node in comm}\n",
    "    nx.set_node_attributes(G, partition, 'community')\n",
    "    \n",
    "    tracker.update(1)  # Step 1 complete\n",
    "\n",
    "    # --- 2. Centrality Measures ---\n",
    "    degree_dict = dict(G.degree())\n",
    "    \n",
    "    try:\n",
    "        k_sample = int(math.sqrt(G.number_of_nodes()))\n",
    "        betweenness_dict = nx.betweenness_centrality(G, k=k_sample, seed=123)\n",
    "    except Exception as e:\n",
    "        print(f\"WARNING: Could not calculate betweenness centrality ({e}). Defaulting to 0.\")\n",
    "        betweenness_dict = {node: 0 for node in G.nodes()}\n",
    "\n",
    "    tracker.update(2)  # Step 2 complete\n",
    "\n",
    "    try:\n",
    "        eigenvector_dict = nx.eigenvector_centrality(G, max_iter=1000, nstart={n:1 for n in G.nodes()})\n",
    "    except Exception as e:\n",
    "        print(f\"WARNING: Could not calculate eigenvector centrality ({e}). Defaulting to 0.\")\n",
    "        eigenvector_dict = {node: 0 for node in G.nodes()}\n",
    "\n",
    "    tracker.update(3)  # Step 3 complete\n",
    "    tracker.complete()  # All steps done\n",
    "\n",
    "    # Store all metrics as node attributes\n",
    "    nx.set_node_attributes(G, degree_dict, 'degree')\n",
    "    nx.set_node_attributes(G, betweenness_dict, 'betweenness')\n",
    "    nx.set_node_attributes(G, eigenvector_dict, 'eigenvector')\n",
    "    \n",
    "    print(f\"Network analysis complete. Communities: {len(communities)}, Nodes analyzed: {G.number_of_nodes():,}\")\n",
    "    return G\n",
    "\n",
    "# =================================== PERFORM NETWORK ANALYSIS ===================================\n",
    "# Apply community detection and centrality analysis if not skipped\n",
    "\n",
    "if not CONFIG['pipeline']['skip_analysis']:\n",
    "    print(\"=== PERFORMING NETWORK ANALYSIS ===\")\n",
    "    G = analyze_network(G)\n",
    "    print(\"Network analysis completed.\")\n",
    "    print(\"====================================\\n\")\n",
    "else:\n",
    "    print(\"=== SKIPPING NETWORK ANALYSIS (as configured) ===\")\n",
    "    # Set default attributes for visualization compatibility\n",
    "    degree_dict = dict(G.degree())\n",
    "    nx.set_node_attributes(G, degree_dict, 'degree')\n",
    "    nx.set_node_attributes(G, {n: 0 for n in G.nodes()}, 'community')\n",
    "    nx.set_node_attributes(G, {n: 0.0 for n in G.nodes()}, 'betweenness')\n",
    "    nx.set_node_attributes(G, {n: 0.0 for n in G.nodes()}, 'eigenvector')\n",
    "    print(\"Default attributes set for visualization compatibility.\")\n",
    "    print(\"====================================================\\n\")\n",
    "\n",
    "# =================================== FAME ANALYSIS AND PATH FINDING ===================================\n",
    "# Advanced fame analysis with configurable thresholds and path finding capabilities\n",
    "\n",
    "def find_famous_accounts(G: nx.DiGraph, min_followers: int, min_ratio: float) -> Tuple[List[Dict], List[Dict]]:\n",
    "    \"\"\"\n",
    "    Analyzes the full graph to find accounts that match the \"famous\" heuristic:\n",
    "    - High followers (in-degree)\n",
    "    - Low following (out-degree)\n",
    "    - High ratio of followers-to-following\n",
    "    \n",
    "    Args:\n",
    "        G (nx.DiGraph): Input graph\n",
    "        min_followers (int): Minimum followers within network\n",
    "        min_ratio (float): Minimum followers/following ratio\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[List[Dict], List[Dict]]: (unreachable_famous, reachable_famous)\n",
    "                                     - unreachable_famous: List of famous accounts with following_in_network=0\n",
    "                                     - reachable_famous: List of famous accounts with following_in_network>=1\n",
    "    \"\"\"\n",
    "    unreachable_famous = []\n",
    "    reachable_famous = []\n",
    "    \n",
    "    if G.number_of_nodes() == 0:\n",
    "        return unreachable_famous, reachable_famous\n",
    "    \n",
    "    # Get all degrees at once in optimized bulk operations\n",
    "    print(\"       Caching all in-degrees and out-degrees...\")\n",
    "    in_degree_dict = dict(G.in_degree())\n",
    "    out_degree_dict = dict(G.out_degree())\n",
    "\n",
    "    # Use .items() to iterate over nodes and their pre-calculated in-degree\n",
    "    for node, in_deg in in_degree_dict.items():\n",
    "        # 1. Filter out nodes that aren't popular enough in your network\n",
    "        if in_deg < min_followers:\n",
    "            continue\n",
    "        \n",
    "        # Get the pre-calculated out-degree\n",
    "        out_deg = out_degree_dict.get(node, 0)  # .get() is safer\n",
    "\n",
    "        # 2. Calculate fame ratio (handle division by zero)\n",
    "        ratio = float('inf')  # Highest possible ratio\n",
    "        if out_deg > 0:\n",
    "            ratio = in_deg / out_deg\n",
    "        \n",
    "        # 3. Filter by ratio\n",
    "        if ratio >= min_ratio:\n",
    "            account_info = {\n",
    "                \"username\": node,\n",
    "                \"followers_in_network\": in_deg,\n",
    "                \"following_in_network\": out_deg,\n",
    "                \"ratio\": ratio\n",
    "            }\n",
    "            \n",
    "            # Separate into unreachable (follows nobody) and reachable\n",
    "            if out_deg == 0:\n",
    "                unreachable_famous.append(account_info)\n",
    "            else:\n",
    "                reachable_famous.append(account_info)\n",
    "    \n",
    "    # Sort by ratio (descending), then by followers (descending)\n",
    "    unreachable_famous.sort(key=lambda x: (x['ratio'], x['followers_in_network']), reverse=True)\n",
    "    reachable_famous.sort(key=lambda x: (x['ratio'], x['followers_in_network']), reverse=True)\n",
    "    \n",
    "    return unreachable_famous, reachable_famous\n",
    "\n",
    "def get_contact_path(G: nx.DiGraph, ego_username: str, target_username: str) -> Optional[List[str]]:\n",
    "    \"\"\"\n",
    "    Silently finds the shortest path from target to ego.\n",
    "    \n",
    "    Args:\n",
    "        G (nx.DiGraph): Input graph\n",
    "        ego_username (str): The ego node (destination)\n",
    "        target_username (str): The target node (source)\n",
    "    \n",
    "    Returns:\n",
    "        Optional[List[str]]: The path (a list of nodes) if found, None otherwise\n",
    "    \"\"\"\n",
    "    if ego_username not in G or target_username not in G:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        path = nx.shortest_path(G, source=target_username, target=ego_username)\n",
    "        return path\n",
    "    except (nx.NetworkXNoPath, nx.NodeNotFound):\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Unexpected error in get_contact_path ({target_username}): {e}\")\n",
    "        return None\n",
    "        \n",
    "def print_detailed_contact_path(G: nx.DiGraph, ego_username: str, target_username: str) -> bool:\n",
    "    \"\"\"\n",
    "    Finds and prints a detailed \"chain of influence\" from a target user back to the ego.\n",
    "    This is a verbose function for a single manual target.\n",
    "    \n",
    "    Args:\n",
    "        G (nx.DiGraph): Input graph\n",
    "        ego_username (str): The ego node (destination)\n",
    "        target_username (str): The target node (source)\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if a path was found, False otherwise\n",
    "    \"\"\"\n",
    "    print(f\"\\n---- Finding Path: {target_username} -> {ego_username} ----\")\n",
    "\n",
    "    if ego_username not in G:\n",
    "        print(f\"ERROR: Ego node '{ego_username}' not in the *full* graph.\")\n",
    "        return False\n",
    "    if target_username not in G:\n",
    "        print(f\"ERROR: Target node '{target_username}' not in the *full* graph.\")\n",
    "        print(\"       This person might not be in your L1/L2 network, or you may have a typo.\")\n",
    "        return False\n",
    "\n",
    "    path = get_contact_path(G, ego_username, target_username)\n",
    "\n",
    "    if path:\n",
    "        print(\"\\n*** SUCCESS: Contact path found! ***\")\n",
    "        \n",
    "        print(\"       Follows Chain (Target to Ego):\")\n",
    "        for i in range(len(path) - 1):\n",
    "            print(f\"         {path[i]:<20} -> follows -> {path[i+1]}\")\n",
    "            \n",
    "        print(f\"       Path length: {len(path) - 1} step(s).\")\n",
    "        \n",
    "        print(\"       Action Plan (Read from bottom up):\")\n",
    "        print(f\"         1. You contact:         {path[-2]}\")\n",
    "        \n",
    "        action_step = 2\n",
    "        for i in range(len(path) - 2, 0, -1):\n",
    "            print(f\"         {action_step}. {path[i]} contacts: {path[i-1]}\")\n",
    "            action_step += 1\n",
    "            \n",
    "        print(f\"         ...who can contact '{path[0]}'.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"\\n*** NO PATH found from '{target_username}' to '{ego_username}'. ***\")\n",
    "        print(\"       No 'chain of influence' exists in your network.\")\n",
    "        return False\n",
    "\n",
    "def perform_fame_analysis(G: nx.DiGraph, config: dict) -> None:\n",
    "    \"\"\"\n",
    "    Performs comprehensive fame analysis and path finding based on configuration.\n",
    "    \n",
    "    Args:\n",
    "        G (nx.DiGraph): Input graph\n",
    "        config (dict): Configuration dictionary\n",
    "    \"\"\"\n",
    "    print(\"=== PERFORMING FAME ANALYSIS ===\")\n",
    "    \n",
    "    # Get fame analysis parameters\n",
    "    min_followers = config['fame_analysis']['min_followers_in_network']\n",
    "    min_ratio = config['fame_analysis']['min_fame_ratio']\n",
    "    find_paths_to_all = config['fame_analysis']['find_paths_to_all_famous']\n",
    "    \n",
    "    print(f\"Fame criteria: min {min_followers} followers, min {min_ratio:.1f}x ratio\")\n",
    "    \n",
    "    # Find famous accounts\n",
    "    unreachable_famous, reachable_famous = find_famous_accounts(G, min_followers, min_ratio)\n",
    "    \n",
    "    total_famous = len(unreachable_famous) + len(reachable_famous)\n",
    "    print(f\"Found {total_famous} famous accounts ({len(reachable_famous)} reachable, {len(unreachable_famous)} unreachable)\")\n",
    "    \n",
    "    if total_famous == 0:\n",
    "        print(\"No famous accounts found with current criteria.\")\n",
    "        return\n",
    "    \n",
    "    # Display top famous accounts\n",
    "    print(\"\\n--- TOP FAMOUS ACCOUNTS (REACHABLE) ---\")\n",
    "    for i, account in enumerate(reachable_famous[:10]):  # Show top 10\n",
    "        ratio_str = \"∞\" if account['ratio'] == float('inf') else f\"{account['ratio']:.1f}\"\n",
    "        print(f\"  {i+1:2d}. {account['username']:<20} | {account['followers_in_network']:4d} followers | {account['following_in_network']:3d} following | {ratio_str}x ratio\")\n",
    "    \n",
    "    if unreachable_famous:\n",
    "        print(\"\\n--- TOP FAMOUS ACCOUNTS (UNREACHABLE) ---\")\n",
    "        for i, account in enumerate(unreachable_famous[:5]):  # Show top 5\n",
    "            print(f\"  {i+1:2d}. {account['username']:<20} | {account['followers_in_network']:4d} followers | 0 following | ∞x ratio\")\n",
    "    \n",
    "    # Manual path finding for specific target\n",
    "    manual_target = config['analysis'].get('contact_path_target')\n",
    "    if manual_target:\n",
    "        ego_username = config['pipeline'].get('ego_username', '_alexs.life')\n",
    "        print_detailed_contact_path(G, ego_username, manual_target)\n",
    "    \n",
    "    # Path finding for all famous accounts\n",
    "    if find_paths_to_all and reachable_famous:\n",
    "        ego_username = config['pipeline'].get('ego_username', '_alexs.life')\n",
    "        print(f\"\\n--- CONTACT PATHS TO FAMOUS ACCOUNTS ---\")\n",
    "        \n",
    "        paths_found = 0\n",
    "        for account in reachable_famous[:20]:  # Limit to top 20 for performance\n",
    "            path = get_contact_path(G, ego_username, account['username'])\n",
    "            if path:\n",
    "                paths_found += 1\n",
    "                path_length = len(path) - 1\n",
    "                print(f\"  {account['username']:<20} | {path_length} steps | Path: {' -> '.join(path[:3])}{'...' if len(path) > 3 else ''}\")\n",
    "        \n",
    "        print(f\"\\nFound paths to {paths_found}/{len(reachable_famous[:20])} famous accounts.\")\n",
    "    \n",
    "    print(\"Fame analysis completed.\")\n",
    "    print(\"=================================\\n\")\n",
    "\n",
    "# =================================== PERFORM FAME ANALYSIS ===================================\n",
    "# Execute fame analysis and path finding based on configuration\n",
    "\n",
    "if G.number_of_nodes() > 0:\n",
    "    perform_fame_analysis(G, CONFIG)\n",
    "else:\n",
    "    print(\"=== SKIPPING FAME ANALYSIS (empty graph) ===\")\n",
    "    print(\"============================================\\n\")\n",
    "\n",
    "# =================================== UTILITY FUNCTIONS FOR VISUALIZATION ===================================\n",
    "# Helper functions for filename generation, color mapping, and scaling algorithms\n",
    "\n",
    "def generate_output_filename(prefix: str, strategy: str, k_value: int, extension: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a descriptive and unique output filename based on config and time.\n",
    "    Format: {prefix}-{strategy}-k{k_value}-{config_hash}.{extension}\n",
    "    \n",
    "    Args:\n",
    "        prefix (str): Base filename prefix\n",
    "        strategy (str): Analysis strategy name\n",
    "        k_value (int): K-value used for pruning\n",
    "        extension (str): File extension (html, png, txt)\n",
    "        \n",
    "    Returns:\n",
    "        str: Unique filename with timestamp hash\n",
    "    \"\"\"\n",
    "    # Get current timestamp to ensure uniqueness\n",
    "    timestamp = datetime.datetime.now().isoformat()\n",
    "    \n",
    "    # Create a short hash of relevant config and time to avoid collisions\n",
    "    config_str = f\"{strategy}-{k_value}-{timestamp}\"\n",
    "    config_hash = hashlib.md5(config_str.encode()).hexdigest()[:6]\n",
    "    \n",
    "    return f\"{prefix}-{strategy}-k{k_value}-{config_hash}.{extension}\"\n",
    "\n",
    "def get_community_colors(num_communities: int) -> dict:\n",
    "    \"\"\"\n",
    "    Generates a color map for communities.\n",
    "    Returns a dict mapping community_id -> hex color string and RGBA tuple.\n",
    "    \n",
    "    Args:\n",
    "        num_communities (int): Number of communities to generate colors for\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with 'hex' and 'rgba' color mappings\n",
    "    \"\"\"\n",
    "    if num_communities > 0:\n",
    "        palette = plt.colormaps.get_cmap('viridis').resampled(num_communities)\n",
    "        colors = palette(range(num_communities))\n",
    "        return {\n",
    "            'hex': {i: f'#{int(c[0]*255):02x}{int(c[1]*255):02x}{int(c[2]*255):02x}' \n",
    "                    for i, c in enumerate(colors)},\n",
    "            'rgba': {i: c for i, c in enumerate(colors)}\n",
    "        }\n",
    "    return {'hex': {0: '#808080'}, 'rgba': {0: (0.5, 0.5, 0.5, 1.0)}}\n",
    "\n",
    "def _get_scaled_size(value: float, base_size: float, multiplier: float, algorithm: str) -> float:\n",
    "    \"\"\"\n",
    "    Helper to calculate node/edge size based on a metric's value.\n",
    "    \n",
    "    Args:\n",
    "        value (float): The metric value to scale\n",
    "        base_size (float): Base size before scaling\n",
    "        multiplier (float): Scaling multiplier\n",
    "        algorithm (str): Scaling algorithm ('logarithmic' or 'linear')\n",
    "        \n",
    "    Returns:\n",
    "        float: Scaled size value\n",
    "    \"\"\"\n",
    "    if algorithm == 'logarithmic':\n",
    "        # Use log1p (log(1+x)) to handle 0 values gracefully\n",
    "        return base_size + math.log1p(value) * multiplier\n",
    "    else:  # linear\n",
    "        return base_size + value * multiplier\n",
    "\n",
    "# =================================== NODE AND EDGE METRIC CALCULATIONS ===================================\n",
    "# Advanced metric calculations for both HTML and PNG visualizations\n",
    "\n",
    "def calculate_edge_metrics(G: nx.DiGraph, vis_config: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Calculates edge metrics for both HTML and PNG visualizations.\n",
    "    Returns a dict mapping (u, v) -> {'width': float, 'color': str, 'is_mutual': bool, 'is_bridge': bool, 'common_neighbors': int}\n",
    "    \n",
    "    This function iterates over the undirected graph's edges to avoid processing mutual pairs twice.\n",
    "    It calculates shared metrics (like common_neighbors) once per undirected pair\n",
    "    and then adds the correct directed edge entry/entries to the dictionary.\n",
    "    \n",
    "    Args:\n",
    "        G (nx.DiGraph): Input graph with community attributes\n",
    "        vis_config (dict): Visualization configuration\n",
    "        \n",
    "    Returns:\n",
    "        dict: Edge metrics mapping (u, v) -> metrics dict\n",
    "    \"\"\"\n",
    "    print(\"Calculating shared edge metrics (embeddedness, bridging, reciprocity)...\")\n",
    "    \n",
    "    G_undirected = G.to_undirected()\n",
    "    edge_metrics = {}\n",
    "    \n",
    "    base_edge_width = vis_config.get(\"base_edge_width\", 0.5)\n",
    "    edge_width_multiplier = vis_config.get(\"edge_width_multiplier\", 1.5)\n",
    "    edge_width_scaling = vis_config.get(\"edge_width_scaling\", \"logarithmic\")\n",
    "    bridge_color = vis_config.get(\"bridge_color\", \"#6e6e6e\")\n",
    "    \n",
    "    # Get community color map\n",
    "    num_communities = len(set(nx.get_node_attributes(G, 'community').values()))\n",
    "    color_maps = get_community_colors(num_communities)\n",
    "    color_map_hex = color_maps['hex']\n",
    "    \n",
    "    # Iterate over each UNDIRECTED edge pair once\n",
    "    for u, v in G_undirected.edges():\n",
    "        \n",
    "        # --- 1. Calculate metrics shared by the pair (u, v) ---\n",
    "        \n",
    "        # Calculate common neighbors (embeddedness)\n",
    "        try:\n",
    "            # Use sum(1 for...) to efficiently get iterator length without list()\n",
    "            common_neighbors_iter = nx.common_neighbors(G_undirected, u, v)\n",
    "            num_common = sum(1 for _ in common_neighbors_iter)\n",
    "        except nx.NetworkXError:\n",
    "            num_common = 0\n",
    "        \n",
    "        # Calculate edge width\n",
    "        edge_width = _get_scaled_size(\n",
    "            num_common,\n",
    "            base_edge_width,\n",
    "            edge_width_multiplier,\n",
    "            edge_width_scaling\n",
    "        )\n",
    "        \n",
    "        # Determine if bridge edge\n",
    "        u_comm = G.nodes[u].get('community', -1)\n",
    "        v_comm = G.nodes[v].get('community', -2)\n",
    "        is_bridge = (u_comm != v_comm)\n",
    "        \n",
    "        # Set edge color\n",
    "        if is_bridge:\n",
    "            edge_color = bridge_color\n",
    "        else:\n",
    "            edge_color = color_map_hex.get(u_comm, vis_config.get(\"intra_community_color\", \"#c0c0c0\"))\n",
    "\n",
    "        # --- 2. Check directionality in G and add to metrics dict ---\n",
    "            \n",
    "        has_u_v = G.has_edge(u, v)\n",
    "        has_v_u = G.has_edge(v, u)\n",
    "        is_mutual = has_u_v and has_v_u\n",
    "\n",
    "        # Define the base metrics for this pair\n",
    "        base_metrics = {\n",
    "            'width': edge_width,\n",
    "            'color': edge_color,\n",
    "            'is_bridge': is_bridge,\n",
    "            'common_neighbors': num_common,\n",
    "        }\n",
    "\n",
    "        if is_mutual:\n",
    "            # Add ONE entry for the mutual pair, key (u, v) by convention.\n",
    "            # Downstream functions know to render this as a mutual arrow.\n",
    "            edge_metrics[(u, v)] = {\n",
    "                **base_metrics,\n",
    "                'is_mutual': True,\n",
    "                'u_comm': u_comm, # 'from' node is u\n",
    "                'v_comm': v_comm  # 'to' node is v\n",
    "            }\n",
    "        \n",
    "        else:\n",
    "            # Add entries for each one-way edge that exists\n",
    "            if has_u_v:\n",
    "                edge_metrics[(u, v)] = {\n",
    "                    **base_metrics,\n",
    "                    'is_mutual': False,\n",
    "                    'u_comm': u_comm, # 'from' node is u\n",
    "                    'v_comm': v_comm  # 'to' node is v\n",
    "                }\n",
    "            \n",
    "            if has_v_u:\n",
    "                edge_metrics[(v, u)] = {\n",
    "                    **base_metrics,\n",
    "                    'is_mutual': False,\n",
    "                    'u_comm': v_comm, # 'from' node is v\n",
    "                    'v_comm': u_comm  # 'to' node is u\n",
    "                }\n",
    "                \n",
    "    return edge_metrics\n",
    "\n",
    "def calculate_node_metrics(G: nx.DiGraph, vis_config: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Calculates node size and color information for both HTML and PNG visualizations.\n",
    "    Returns a dict mapping node -> {'size': float, 'community': int, 'color_hex': str, 'color_rgba': tuple}\n",
    "    \n",
    "    Args:\n",
    "        G (nx.DiGraph): Input graph with analysis attributes\n",
    "        vis_config (dict): Visualization configuration\n",
    "        \n",
    "    Returns:\n",
    "        dict: Node metrics mapping node -> metrics dict\n",
    "    \"\"\"\n",
    "    \n",
    "    # Handle the case where analysis was skipped (attributes are missing)\n",
    "    if not nx.get_node_attributes(G, 'community'):\n",
    "        print(\"Using default metrics (degree, community 0) as analysis was skipped.\")\n",
    "        node_metrics = {}\n",
    "        # Set all community IDs to 0\n",
    "        nx.set_node_attributes(G, {n: 0 for n in G.nodes()}, 'community') \n",
    "        \n",
    "        # Set all centrality/degree to 0 or 1 for visualization fallback\n",
    "        for n in G.nodes():\n",
    "            G.nodes[n]['degree'] = G.degree(n) # Use actual degree for sizing fallback\n",
    "            G.nodes[n]['betweenness'] = 0.0\n",
    "            G.nodes[n]['eigenvector'] = 0.0\n",
    "            \n",
    "    # Regular metric calculation\n",
    "    node_metrics = {}\n",
    "    size_metric = vis_config['node_size_metric']\n",
    "    base_size = vis_config['base_node_size']\n",
    "    multiplier = vis_config['node_size_multiplier']\n",
    "    scaling_alg = vis_config['scaling_algorithm']\n",
    "    \n",
    "    # Get community colors\n",
    "    num_communities = len(set(nx.get_node_attributes(G, 'community').values()))\n",
    "    color_maps = get_community_colors(num_communities)\n",
    "    \n",
    "    for node, attrs in G.nodes(data=True):\n",
    "        # Use actual metric if available, otherwise fallback to degree\n",
    "        metric_value = attrs.get(size_metric, attrs.get('degree', 1))\n",
    "        community_id = attrs.get('community', 0)\n",
    "        \n",
    "        node_size = _get_scaled_size(\n",
    "            metric_value,\n",
    "            base_size,\n",
    "            multiplier,\n",
    "            scaling_alg\n",
    "        )\n",
    "        \n",
    "        node_metrics[node] = {\n",
    "            'size': node_size,\n",
    "            'community': community_id,\n",
    "            'color_hex': color_maps['hex'].get(community_id, '#808080'),\n",
    "            'color_rgba': color_maps['rgba'].get(community_id, (0.5, 0.5, 0.5, 1.0)),\n",
    "            'degree': attrs.get('degree', 0),\n",
    "            'betweenness': attrs.get('betweenness', 0),\n",
    "            'eigenvector': attrs.get('eigenvector', 0)\n",
    "        }\n",
    "    \n",
    "    return node_metrics\n",
    "\n",
    "# =================================== ENHANCED VISUALIZATION GENERATION ===================================\n",
    "# Generate both interactive HTML and static PNG visualizations with advanced styling\n",
    "\n",
    "def numEdges(node_id: str) -> int:\n",
    "    \"\"\"\n",
    "    Helper function to get the number of edges for a node (for legacy compatibility).\n",
    "    \n",
    "    Args:\n",
    "        node_id (str): Node identifier\n",
    "        \n",
    "    Returns:\n",
    "        int: Number of edges (degree) for the node\n",
    "    \"\"\"\n",
    "    if node_id in G:\n",
    "        return G.degree(node_id)\n",
    "    return 0\n",
    "\n",
    "# Calculate node and edge metrics for visualization\n",
    "print(\"=== CALCULATING VISUALIZATION METRICS ===\")\n",
    "node_metrics = calculate_node_metrics(G, CONFIG['visualization'])\n",
    "edge_metrics = calculate_edge_metrics(G, CONFIG['visualization'])\n",
    "print(f\"Calculated metrics for {len(node_metrics)} nodes and {len(edge_metrics)} edges.\")\n",
    "print(\"=============================================\\n\")\n",
    "\n",
    "# =================================== LEGACY COMPATIBILITY ===================================\n",
    "# Initialize Pyvis network for downstream compatibility\n",
    "net = Network(700, 700, directed=True, notebook=False)  # For jupyter notebook = True\n",
    "\n",
    "#======================================== NETWORKX TO PYVIS =============================================\n",
    "#Aesthetic Options\n",
    "sizeByConnections = 1 #Change a nodes size by number of connections \n",
    "\n",
    "# Add nodes and edges from the processed NetworkX graph to the Pyvis network\n",
    "net.from_nx(G)\n",
    "\n",
    "# Apply size scaling if enabled\n",
    "if sizeByConnections:\n",
    "    for node in net.nodes:\n",
    "        node['size'] = (numEdges(node['id'])/50)+9\n",
    "\n",
    "# Set physics options for the visualization\n",
    "net.force_atlas_2based(gravity=-50, central_gravity=0.01, spring_length=100, spring_strength=0.07, damping=0.8, overlap=1)\n",
    "net.show_buttons(filter_=['physics'])\n",
    "\n",
    "# Generate and show the HTML file\n",
    "net.save_graph(\"FollowWeb.html\")\n",
    "print(\"Visualization complete. Check 'FollowWeb.html'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}